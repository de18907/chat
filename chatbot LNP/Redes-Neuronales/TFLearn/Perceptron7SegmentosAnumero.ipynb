{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶ SUSCRÍBETE al canal XPIKUOS de YOUTUBE y DALE a la CAMPANITA para estar informado de las novedades: https://www.youtube.com/c/Xpikuos?sub_confirmation=1\n",
    "\n",
    "▶ MIRA el siguiente VÍDEO explicativo para entender mejor el código: https://www.youtube.com/watch?v=-7gsfbskvh4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn import DNN\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected \n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tendremos 7 entradas (una por cada LED activado en el display)\n",
    "#Significado de las posiciones de cada valor del array de entrada X\n",
    "# 0  Segmento superior del display\n",
    "#1 2 Segmentos laterales superiores del display\n",
    "# 3  Segmento central del display\n",
    "#4 5 Segmentos laterales inferiores del display\n",
    "# 6  Segmento inferior del display\n",
    "#Ejemplo: [0,0,1,0,0,1,0] -> display con los LEDs 2 y 5 encendidos, por lo cual está representando un 1\n",
    "X = [\n",
    "    [0,0,0,0,0,0,0], #0\n",
    "    [0,0,1,0,0,1,0], #1\n",
    "    [1,0,1,1,1,0,1], #2\n",
    "    [1,0,1,1,0,1,1], #3\n",
    "    [0,1,1,1,0,1,0], #4\n",
    "    [1,1,0,1,0,1,1], #5\n",
    "    [1,1,0,1,1,1,1], #6\n",
    "    [1,0,1,0,0,1,0], #7\n",
    "    [1,1,1,1,1,1,1], #8\n",
    "    [1,1,1,1,0,1,1], #9\n",
    "    ]\n",
    "\n",
    "# Tendremos una salida diferente para cada número a reconocer (por tanto, tendremos 10 salidas)\n",
    "# -la posición 0 activada de la salida indica que el display está mostrando un 0, \n",
    "# -la posición 9 activada de la salida indica que el display está mostrando un 9,\n",
    "\n",
    "Y = [\n",
    "    [1,0,0,0,0,0,0,0,0,0], #0\n",
    "    [0,1,0,0,0,0,0,0,0,0], #1\n",
    "    [0,0,1,0,0,0,0,0,0,0], #2\n",
    "    [0,0,0,1,0,0,0,0,0,0], #3\n",
    "    [0,0,0,0,1,0,0,0,0,0], #4\n",
    "    [0,0,0,0,0,1,0,0,0,0], #5\n",
    "    [0,0,0,0,0,0,1,0,0,0], #6\n",
    "    [0,0,0,0,0,0,0,1,0,0], #7\n",
    "    [0,0,0,0,0,0,0,0,1,0], #8\n",
    "    [0,0,0,0,0,0,0,0,0,1], #9\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflearn.init_graph(num_cores=8, gpu_memory_fraction=0.5)\n",
    "\n",
    "input_layer = input_data(shape=[None, 7])\n",
    "#[None, 7] significa que se puede entrenar la red con cualquier número de ejemplos, tales que cada ejemplo tenga 7 características. \n",
    "#Si sólo quisiéramos entrenar la red con 4 ejemplos escribiríamos shape=[4, 7]\n",
    "hidden_layer = fully_connected(input_layer , 7, activation='leaky_relu')#, activation='relu') \n",
    "hidden_layer = tflearn.dropout(hidden_layer, 1)\n",
    "output_layer = fully_connected(hidden_layer, 10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el tipo de regresor que realizará la \"back propagation\" y entrenará nuestro modelo \n",
    "#Nosotros usaremos el \"ADAM\" como método de optimización y la \"Categorical cross entropy\" como función de error (loss function)\n",
    "#La velocidad de aprendizaje del algoritmo (learning_rate) es 0.08\n",
    "regression = regression(output_layer , optimizer='adam', loss='categorical_crossentropy', learning_rate=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente defininimos nuestra \"deep neural network\" en tflearn usando DNN().\n",
    "model = DNN(regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: EM4C2O\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 10\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.069s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.66096\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 002 | loss: 0.66096 - acc: 0.1800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m0.72098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 003 | loss: 0.72098 - acc: 0.1145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m0.73051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 004 | loss: 0.73051 - acc: 0.2536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m0.73194\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 005 | loss: 0.73194 - acc: 0.3550 -- iter: 10/10\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m0.73123\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 006 | loss: 0.73123 - acc: 0.2553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m0.72957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 007 | loss: 0.72957 - acc: 0.2221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m0.72735\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 008 | loss: 0.72735 - acc: 0.2097 -- iter: 10/10\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m0.72474\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 009 | loss: 0.72474 - acc: 0.2046 -- iter: 10/10\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m0.72183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 010 | loss: 0.72183 - acc: 0.3023 -- iter: 10/10\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m0.71863\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 011 | loss: 0.71863 - acc: 0.4433 -- iter: 10/10\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.71515\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 012 | loss: 0.71515 - acc: 0.5138 -- iter: 10/10\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m0.71127\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 013 | loss: 0.71127 - acc: 0.5936 -- iter: 10/10\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m0.70689\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 014 | loss: 0.70689 - acc: 0.6780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m0.70220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 015 | loss: 0.70220 - acc: 0.7258 -- iter: 10/10\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.69757\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 016 | loss: 0.69757 - acc: 0.7536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m0.69329\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 017 | loss: 0.69329 - acc: 0.7703 -- iter: 10/10\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m0.68935\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 018 | loss: 0.68935 - acc: 0.7806 -- iter: 10/10\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m0.68570\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 0.68570 - acc: 0.7871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.68254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 020 | loss: 0.68254 - acc: 0.7912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m0.67980\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 021 | loss: 0.67980 - acc: 0.8250 -- iter: 10/10\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m0.67742\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 022 | loss: 0.67742 - acc: 0.8475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m0.67542\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 023 | loss: 0.67542 - acc: 0.8627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.67371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 024 | loss: 0.67371 - acc: 0.8732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m0.67222\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 025 | loss: 0.67222 - acc: 0.8805 -- iter: 10/10\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m0.67092\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 026 | loss: 0.67092 - acc: 0.8857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m0.66980\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 027 | loss: 0.66980 - acc: 0.8894 -- iter: 10/10\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.66884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 028 | loss: 0.66884 - acc: 0.8920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m0.66802\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 029 | loss: 0.66802 - acc: 0.8940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m0.66734\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 030 | loss: 0.66734 - acc: 0.8954 -- iter: 10/10\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m0.66678\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 031 | loss: 0.66678 - acc: 0.8965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.66632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 032 | loss: 0.66632 - acc: 0.8973 -- iter: 10/10\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m0.66595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 033 | loss: 0.66595 - acc: 0.8979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m0.66565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 034 | loss: 0.66565 - acc: 0.8983 -- iter: 10/10\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m0.66541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 035 | loss: 0.66541 - acc: 0.8987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.66522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 036 | loss: 0.66522 - acc: 0.8989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m0.66506\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 037 | loss: 0.66506 - acc: 0.8992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m0.66493\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 038 | loss: 0.66493 - acc: 0.8993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m0.66482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 039 | loss: 0.66482 - acc: 0.8994 -- iter: 10/10\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.66473\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 040 | loss: 0.66473 - acc: 0.8996 -- iter: 10/10\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m0.66466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 041 | loss: 0.66466 - acc: 0.8996 -- iter: 10/10\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m0.66460\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 042 | loss: 0.66460 - acc: 0.8997 -- iter: 10/10\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m0.66455\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 043 | loss: 0.66455 - acc: 0.8998 -- iter: 10/10\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.66451\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 044 | loss: 0.66451 - acc: 0.8998 -- iter: 10/10\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.66448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 045 | loss: 0.66448 - acc: 0.8998 -- iter: 10/10\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m0.66445\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 046 | loss: 0.66445 - acc: 0.8999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m0.66443\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 047 | loss: 0.66443 - acc: 0.8999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.66440\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 0.66440 - acc: 0.8999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m0.66439\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 0.66439 - acc: 0.8999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m0.67650\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 050 | loss: 0.67650 - acc: 0.7758 -- iter: 10/10\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m0.67475\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 051 | loss: 0.67475 - acc: 0.7947 -- iter: 10/10\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.67336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 052 | loss: 0.67336 - acc: 0.8105 -- iter: 10/10\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m0.67238\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 053 | loss: 0.67238 - acc: 0.8237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m0.68290\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 054 | loss: 0.68290 - acc: 0.7187 -- iter: 10/10\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m0.68047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 055 | loss: 0.68047 - acc: 0.7446 -- iter: 10/10\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.67834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 056 | loss: 0.67834 - acc: 0.7664 -- iter: 10/10\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m0.67652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 057 | loss: 0.67652 - acc: 0.7849 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m0.67498\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 058 | loss: 0.67498 - acc: 0.8006 -- iter: 10/10\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.67366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 059 | loss: 0.67366 - acc: 0.8140 -- iter: 10/10\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.67251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 060 | loss: 0.67251 - acc: 0.8254 -- iter: 10/10\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.67151\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 0.67151 - acc: 0.8351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.68086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 062 | loss: 0.68086 - acc: 0.7406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.67879\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 063 | loss: 0.67879 - acc: 0.7608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.67705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 064 | loss: 0.67705 - acc: 0.7782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.67554\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 065 | loss: 0.67554 - acc: 0.7932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.67421\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 066 | loss: 0.67421 - acc: 0.8062 -- iter: 10/10\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.67308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 067 | loss: 0.67308 - acc: 0.8175 -- iter: 10/10\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.67210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 068 | loss: 0.67210 - acc: 0.8272 -- iter: 10/10\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.67122\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 069 | loss: 0.67122 - acc: 0.8357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.67047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 070 | loss: 0.67047 - acc: 0.8431 -- iter: 10/10\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.66982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 071 | loss: 0.66982 - acc: 0.8496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.67823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 072 | loss: 0.67823 - acc: 0.7653 -- iter: 10/10\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.67673\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 073 | loss: 0.67673 - acc: 0.7803 -- iter: 10/10\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.67541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 074 | loss: 0.67541 - acc: 0.7934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.67424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 075 | loss: 0.67424 - acc: 0.8050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.67322\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 076 | loss: 0.67322 - acc: 0.8151 -- iter: 10/10\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.67232\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 077 | loss: 0.67232 - acc: 0.8241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.67151\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 078 | loss: 0.67151 - acc: 0.8321 -- iter: 10/10\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.67080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 079 | loss: 0.67080 - acc: 0.8391 -- iter: 10/10\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.67685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 080 | loss: 0.67685 - acc: 0.7737 -- iter: 10/10\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.67567\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 081 | loss: 0.67567 - acc: 0.7865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.68293\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 082 | loss: 0.68293 - acc: 0.7179 -- iter: 10/10\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.68240\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 083 | loss: 0.68240 - acc: 0.7261 -- iter: 10/10\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.68949\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 084 | loss: 0.68949 - acc: 0.6535 -- iter: 10/10\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.69037\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 085 | loss: 0.69037 - acc: 0.6481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.69573\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 086 | loss: 0.69573 - acc: 0.5933 -- iter: 10/10\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.69380\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 087 | loss: 0.69380 - acc: 0.6140 -- iter: 10/10\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.69893\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 088 | loss: 0.69893 - acc: 0.5626 -- iter: 10/10\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.69659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 089 | loss: 0.69659 - acc: 0.5863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.70149\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 090 | loss: 0.70149 - acc: 0.5377 -- iter: 10/10\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.69887\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 091 | loss: 0.69887 - acc: 0.5639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.69644\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 092 | loss: 0.69644 - acc: 0.5875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.69428\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 093 | loss: 0.69428 - acc: 0.6088 -- iter: 10/10\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.69231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 094 | loss: 0.69231 - acc: 0.6279 -- iter: 10/10\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.69056\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 0.69056 - acc: 0.6451 -- iter: 10/10\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.69598\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 096 | loss: 0.69598 - acc: 0.5906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.69391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 097 | loss: 0.69391 - acc: 0.6115 -- iter: 10/10\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.69200\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 098 | loss: 0.69200 - acc: 0.6304 -- iter: 10/10\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.69028\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 099 | loss: 0.69028 - acc: 0.6473 -- iter: 10/10\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.68874\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 100 | loss: 0.68874 - acc: 0.6626 -- iter: 10/10\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.68734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 101 | loss: 0.68734 - acc: 0.6763 -- iter: 10/10\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.69207\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 102 | loss: 0.69207 - acc: 0.6287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.69034\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 0.69034 - acc: 0.6458 -- iter: 10/10\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.68877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 104 | loss: 0.68877 - acc: 0.6613 -- iter: 10/10\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.68732\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 105 | loss: 0.68732 - acc: 0.6751 -- iter: 10/10\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.69334\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 106 | loss: 0.69334 - acc: 0.6176 -- iter: 10/10\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.69152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 107 | loss: 0.69152 - acc: 0.6359 -- iter: 10/10\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.68994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 108 | loss: 0.68994 - acc: 0.6523 -- iter: 10/10\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.68894\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 109 | loss: 0.68894 - acc: 0.6570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.69357\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 110 | loss: 0.69357 - acc: 0.6113 -- iter: 10/10\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.69174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 111 | loss: 0.69174 - acc: 0.6302 -- iter: 10/10\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.69010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 112 | loss: 0.69010 - acc: 0.6472 -- iter: 10/10\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.68863\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 113 | loss: 0.68863 - acc: 0.6625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.68731\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 114 | loss: 0.68731 - acc: 0.6762 -- iter: 10/10\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.68610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 115 | loss: 0.68610 - acc: 0.6886 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.68497\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 116 | loss: 0.68497 - acc: 0.6997 -- iter: 10/10\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.68397\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 117 | loss: 0.68397 - acc: 0.7098 -- iter: 10/10\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.68309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 118 | loss: 0.68309 - acc: 0.7188 -- iter: 10/10\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.68231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 119 | loss: 0.68231 - acc: 0.7269 -- iter: 10/10\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.68859\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 120 | loss: 0.68859 - acc: 0.6642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.68725\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 121 | loss: 0.68725 - acc: 0.6778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.68606\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 122 | loss: 0.68606 - acc: 0.6900 -- iter: 10/10\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.68498\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 123 | loss: 0.68498 - acc: 0.7010 -- iter: 10/10\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.68401\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 124 | loss: 0.68401 - acc: 0.7109 -- iter: 10/10\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.68313\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 125 | loss: 0.68313 - acc: 0.7198 -- iter: 10/10\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.68234\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 126 | loss: 0.68234 - acc: 0.7278 -- iter: 10/10\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.68163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 127 | loss: 0.68163 - acc: 0.7351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.68099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 128 | loss: 0.68099 - acc: 0.7416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.68042\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 129 | loss: 0.68042 - acc: 0.7474 -- iter: 10/10\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.68789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 130 | loss: 0.68789 - acc: 0.6727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.68662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 131 | loss: 0.68662 - acc: 0.6854 -- iter: 10/10\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.69347\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 132 | loss: 0.69347 - acc: 0.6169 -- iter: 10/10\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.69165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 133 | loss: 0.69165 - acc: 0.6352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.69000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 134 | loss: 0.69000 - acc: 0.6516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.68852\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 0.68852 - acc: 0.6665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.68719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 136 | loss: 0.68719 - acc: 0.6798 -- iter: 10/10\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.68599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 137 | loss: 0.68599 - acc: 0.6919 -- iter: 10/10\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.68491\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 138 | loss: 0.68491 - acc: 0.7027 -- iter: 10/10\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.68394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 139 | loss: 0.68394 - acc: 0.7124 -- iter: 10/10\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.69006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 140 | loss: 0.69006 - acc: 0.6512 -- iter: 10/10\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.68858\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 141 | loss: 0.68858 - acc: 0.6660 -- iter: 10/10\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.69423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 142 | loss: 0.69423 - acc: 0.6094 -- iter: 10/10\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.69233\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 143 | loss: 0.69233 - acc: 0.6285 -- iter: 10/10\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.69761\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 144 | loss: 0.69761 - acc: 0.5756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.69537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 145 | loss: 0.69537 - acc: 0.5981 -- iter: 10/10\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.69335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 146 | loss: 0.69335 - acc: 0.6183 -- iter: 10/10\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.69153\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 147 | loss: 0.69153 - acc: 0.6364 -- iter: 10/10\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.68990\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 148 | loss: 0.68990 - acc: 0.6528 -- iter: 10/10\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.68843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 149 | loss: 0.68843 - acc: 0.6675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.68710\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 150 | loss: 0.68710 - acc: 0.6808 -- iter: 10/10\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.68591\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 151 | loss: 0.68591 - acc: 0.6927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.69283\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 0.69283 - acc: 0.6234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.69107\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 153 | loss: 0.69107 - acc: 0.6411 -- iter: 10/10\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.68948\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 154 | loss: 0.68948 - acc: 0.6570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.68805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 155 | loss: 0.68805 - acc: 0.6713 -- iter: 10/10\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.69276\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 156 | loss: 0.69276 - acc: 0.6241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.69100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 157 | loss: 0.69100 - acc: 0.6417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.68942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 0.68942 - acc: 0.6576 -- iter: 10/10\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.68799\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 159 | loss: 0.68799 - acc: 0.6718 -- iter: 10/10\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.68671\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 160 | loss: 0.68671 - acc: 0.6846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.68556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 161 | loss: 0.68556 - acc: 0.6962 -- iter: 10/10\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.69052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 162 | loss: 0.69052 - acc: 0.6465 -- iter: 10/10\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.68898\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 163 | loss: 0.68898 - acc: 0.6619 -- iter: 10/10\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.68760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 164 | loss: 0.68760 - acc: 0.6757 -- iter: 10/10\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.68636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 165 | loss: 0.68636 - acc: 0.6881 -- iter: 10/10\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.68524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 166 | loss: 0.68524 - acc: 0.6993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.68423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 167 | loss: 0.68423 - acc: 0.7094 -- iter: 10/10\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.69132\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 168 | loss: 0.69132 - acc: 0.6384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.68971\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 169 | loss: 0.68971 - acc: 0.6546 -- iter: 10/10\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.69625\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 170 | loss: 0.69625 - acc: 0.5891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.69414\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 171 | loss: 0.69414 - acc: 0.6102 -- iter: 10/10\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.69225\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 172 | loss: 0.69225 - acc: 0.6292 -- iter: 10/10\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.69054\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 173 | loss: 0.69054 - acc: 0.6463 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.68900\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 174 | loss: 0.68900 - acc: 0.6617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.68762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 175 | loss: 0.68762 - acc: 0.6755 -- iter: 10/10\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.69337\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 176 | loss: 0.69337 - acc: 0.6179 -- iter: 10/10\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.69155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 177 | loss: 0.69155 - acc: 0.6361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.69791\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 178 | loss: 0.69791 - acc: 0.5725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.69563\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 179 | loss: 0.69563 - acc: 0.5953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.69358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 180 | loss: 0.69358 - acc: 0.6158 -- iter: 10/10\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.69173\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 181 | loss: 0.69173 - acc: 0.6342 -- iter: 10/10\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.69007\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 182 | loss: 0.69007 - acc: 0.6508 -- iter: 10/10\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.68857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 183 | loss: 0.68857 - acc: 0.6657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.69520\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 184 | loss: 0.69520 - acc: 0.5991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.69314\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 0.69314 - acc: 0.6192 -- iter: 10/10\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.69930\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 186 | loss: 0.69930 - acc: 0.5573 -- iter: 10/10\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.69686\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 187 | loss: 0.69686 - acc: 0.5816 -- iter: 10/10\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.69468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 188 | loss: 0.69468 - acc: 0.6034 -- iter: 10/10\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.69269\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 189 | loss: 0.69269 - acc: 0.6231 -- iter: 10/10\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.69887\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 190 | loss: 0.69887 - acc: 0.5608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.69646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 191 | loss: 0.69646 - acc: 0.5847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.70129\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 192 | loss: 0.70129 - acc: 0.5362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.69863\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 193 | loss: 0.69863 - acc: 0.5626 -- iter: 10/10\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.70222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 194 | loss: 0.70222 - acc: 0.5263 -- iter: 10/10\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.69945\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 195 | loss: 0.69945 - acc: 0.5537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.69694\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 196 | loss: 0.69694 - acc: 0.5783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.69465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 197 | loss: 0.69465 - acc: 0.6005 -- iter: 10/10\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.69259\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 198 | loss: 0.69259 - acc: 0.6204 -- iter: 10/10\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.69075\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 0.69075 - acc: 0.6384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.68909\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 200 | loss: 0.68909 - acc: 0.6546 -- iter: 10/10\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.68759\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 201 | loss: 0.68759 - acc: 0.6691 -- iter: 10/10\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.69361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 202 | loss: 0.69361 - acc: 0.6122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.69175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 203 | loss: 0.69175 - acc: 0.6310 -- iter: 10/10\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.69009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 204 | loss: 0.69009 - acc: 0.6479 -- iter: 10/10\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 205 | loss: 0.69523 - acc: 0.6631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 206 | loss: 0.69523 - acc: 0.5968 -- iter: 10/10\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.69371\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 207 | loss: 0.69371 - acc: 0.6071 -- iter: 10/10\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.69795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 208 | loss: 0.69795 - acc: 0.5664 -- iter: 10/10\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.69567\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 209 | loss: 0.69567 - acc: 0.5898 -- iter: 10/10\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.69362\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 210 | loss: 0.69362 - acc: 0.6108 -- iter: 10/10\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.69177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 211 | loss: 0.69177 - acc: 0.6297 -- iter: 10/10\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.69806\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 212 | loss: 0.69806 - acc: 0.5667 -- iter: 10/10\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.69571\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.69571 - acc: 0.5901 -- iter: 10/10\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.69361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 214 | loss: 0.69361 - acc: 0.6111 -- iter: 10/10\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.69174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 215 | loss: 0.69174 - acc: 0.6299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.69004\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 216 | loss: 0.69004 - acc: 0.6470 -- iter: 10/10\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.68850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 217 | loss: 0.68850 - acc: 0.6623 -- iter: 10/10\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.68712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 218 | loss: 0.68712 - acc: 0.6760 -- iter: 10/10\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.68588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 219 | loss: 0.68588 - acc: 0.6884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.68474\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 220 | loss: 0.68474 - acc: 0.6996 -- iter: 10/10\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.68369\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 221 | loss: 0.68369 - acc: 0.7096 -- iter: 10/10\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.68279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 222 | loss: 0.68279 - acc: 0.7187 -- iter: 10/10\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.68198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 223 | loss: 0.68198 - acc: 0.7268 -- iter: 10/10\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.68121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 224 | loss: 0.68121 - acc: 0.7341 -- iter: 10/10\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.68050\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 225 | loss: 0.68050 - acc: 0.7407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.68789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 226 | loss: 0.68789 - acc: 0.6666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.68655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 227 | loss: 0.68655 - acc: 0.6800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 228 | loss: 0.68531 - acc: 0.6920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.68419\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 229 | loss: 0.68419 - acc: 0.7028 -- iter: 10/10\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.68321\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 230 | loss: 0.68321 - acc: 0.7125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.68231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 231 | loss: 0.68231 - acc: 0.7212 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.68148\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 232 | loss: 0.68148 - acc: 0.7291 -- iter: 10/10\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.68076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 233 | loss: 0.68076 - acc: 0.7362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.68010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 234 | loss: 0.68010 - acc: 0.7426 -- iter: 10/10\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.67949\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 235 | loss: 0.67949 - acc: 0.7483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.67896\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 236 | loss: 0.67896 - acc: 0.7535 -- iter: 10/10\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.67848\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 237 | loss: 0.67848 - acc: 0.7581 -- iter: 10/10\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.67803\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 238 | loss: 0.67803 - acc: 0.7623 -- iter: 10/10\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.67764\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 239 | loss: 0.67764 - acc: 0.7661 -- iter: 10/10\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.67729\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 240 | loss: 0.67729 - acc: 0.7695 -- iter: 10/10\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.67696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 241 | loss: 0.67696 - acc: 0.7725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.67667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 242 | loss: 0.67667 - acc: 0.7753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.67641\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 243 | loss: 0.67641 - acc: 0.7778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.67617\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 244 | loss: 0.67617 - acc: 0.7800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.67596\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 245 | loss: 0.67596 - acc: 0.7820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 246 | loss: 0.68277 - acc: 0.7138 -- iter: 10/10\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.68189\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 247 | loss: 0.68189 - acc: 0.7224 -- iter: 10/10\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.68111\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 248 | loss: 0.68111 - acc: 0.7302 -- iter: 10/10\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.68040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 249 | loss: 0.68040 - acc: 0.7372 -- iter: 10/10\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.67976\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 250 | loss: 0.67976 - acc: 0.7434 -- iter: 10/10\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.67919\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 251 | loss: 0.67919 - acc: 0.7491 -- iter: 10/10\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.68323\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 252 | loss: 0.68323 - acc: 0.7042 -- iter: 10/10\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.68240\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 253 | loss: 0.68240 - acc: 0.7138 -- iter: 10/10\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.68168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 254 | loss: 0.68168 - acc: 0.7224 -- iter: 10/10\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.68102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 255 | loss: 0.68102 - acc: 0.7301 -- iter: 10/10\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.68643\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 256 | loss: 0.68643 - acc: 0.6771 -- iter: 10/10\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.68529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 257 | loss: 0.68529 - acc: 0.6894 -- iter: 10/10\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.68424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 258 | loss: 0.68424 - acc: 0.7005 -- iter: 10/10\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.68330\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 259 | loss: 0.68330 - acc: 0.7104 -- iter: 10/10\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.68945\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 260 | loss: 0.68945 - acc: 0.6494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.68819\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 261 | loss: 0.68819 - acc: 0.6644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.69354\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 262 | loss: 0.69354 - acc: 0.6080 -- iter: 10/10\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.69179\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 263 | loss: 0.69179 - acc: 0.6272 -- iter: 10/10\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.69020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 264 | loss: 0.69020 - acc: 0.6445 -- iter: 10/10\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.68904\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 265 | loss: 0.68904 - acc: 0.6600 -- iter: 10/10\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.68775\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 266 | loss: 0.68775 - acc: 0.6740 -- iter: 10/10\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.68650\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 267 | loss: 0.68650 - acc: 0.6866 -- iter: 10/10\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.68537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 268 | loss: 0.68537 - acc: 0.6980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.68435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 269 | loss: 0.68435 - acc: 0.7082 -- iter: 10/10\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.68343\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 270 | loss: 0.68343 - acc: 0.7174 -- iter: 10/10\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.68259\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 271 | loss: 0.68259 - acc: 0.7256 -- iter: 10/10\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.68882\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 272 | loss: 0.68882 - acc: 0.6631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.68740\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 273 | loss: 0.68740 - acc: 0.6767 -- iter: 10/10\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.69410\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 274 | loss: 0.69410 - acc: 0.6091 -- iter: 10/10\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.69227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 275 | loss: 0.69227 - acc: 0.6282 -- iter: 10/10\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.69057\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 276 | loss: 0.69057 - acc: 0.6454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.68897\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 277 | loss: 0.68897 - acc: 0.6608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.68754\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 278 | loss: 0.68754 - acc: 0.6747 -- iter: 10/10\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.68624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 279 | loss: 0.68624 - acc: 0.6873 -- iter: 10/10\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.69308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 280 | loss: 0.69308 - acc: 0.6185 -- iter: 10/10\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.69127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 281 | loss: 0.69127 - acc: 0.6367 -- iter: 10/10\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.69664\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 282 | loss: 0.69664 - acc: 0.5830 -- iter: 10/10\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.69447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 283 | loss: 0.69447 - acc: 0.6047 -- iter: 10/10\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.69869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 284 | loss: 0.69869 - acc: 0.5642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.69633\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 285 | loss: 0.69633 - acc: 0.5878 -- iter: 10/10\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.69422\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 286 | loss: 0.69422 - acc: 0.6090 -- iter: 10/10\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.69232\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 287 | loss: 0.69232 - acc: 0.6281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.69760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 288 | loss: 0.69760 - acc: 0.5753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.69536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 289 | loss: 0.69536 - acc: 0.5978 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.70133\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 290 | loss: 0.70133 - acc: 0.5380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.69874\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 291 | loss: 0.69874 - acc: 0.5642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.70430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 292 | loss: 0.70430 - acc: 0.5078 -- iter: 10/10\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.70165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 293 | loss: 0.70165 - acc: 0.5370 -- iter: 10/10\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.69905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 294 | loss: 0.69905 - acc: 0.5633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.69667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 295 | loss: 0.69667 - acc: 0.5870 -- iter: 10/10\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.69452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 296 | loss: 0.69452 - acc: 0.6083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.69259\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 297 | loss: 0.69259 - acc: 0.6275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.69085\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 298 | loss: 0.69085 - acc: 0.6447 -- iter: 10/10\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.68928\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 299 | loss: 0.68928 - acc: 0.6602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.68787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 300 | loss: 0.68787 - acc: 0.6742 -- iter: 10/10\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.68659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 301 | loss: 0.68659 - acc: 0.6868 -- iter: 10/10\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.68545\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 302 | loss: 0.68545 - acc: 0.6981 -- iter: 10/10\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.68441\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 303 | loss: 0.68441 - acc: 0.7083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.69044\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 304 | loss: 0.69044 - acc: 0.6475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.68884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 305 | loss: 0.68884 - acc: 0.6627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.68743\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 306 | loss: 0.68743 - acc: 0.6765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.68618\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 307 | loss: 0.68618 - acc: 0.6888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.69201\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 308 | loss: 0.69201 - acc: 0.6299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.69036\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 309 | loss: 0.69036 - acc: 0.6469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.68884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 310 | loss: 0.68884 - acc: 0.6622 -- iter: 10/10\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.68743\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 311 | loss: 0.68743 - acc: 0.6760 -- iter: 10/10\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.69414\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 312 | loss: 0.69414 - acc: 0.6084 -- iter: 10/10\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.69219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 313 | loss: 0.69219 - acc: 0.6276 -- iter: 10/10\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.69040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 314 | loss: 0.69040 - acc: 0.6448 -- iter: 10/10\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.68876\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 315 | loss: 0.68876 - acc: 0.6603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.69531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 316 | loss: 0.69531 - acc: 0.5943 -- iter: 10/10\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.69322\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 317 | loss: 0.69322 - acc: 0.6149 -- iter: 10/10\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.69835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 318 | loss: 0.69835 - acc: 0.5634 -- iter: 10/10\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.69596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 319 | loss: 0.69596 - acc: 0.5870 -- iter: 10/10\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.70179\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 320 | loss: 0.70179 - acc: 0.5283 -- iter: 10/10\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.69903\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 321 | loss: 0.69903 - acc: 0.5555 -- iter: 10/10\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.69653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 322 | loss: 0.69653 - acc: 0.5800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.69429\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 323 | loss: 0.69429 - acc: 0.6020 -- iter: 10/10\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.69229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 324 | loss: 0.69229 - acc: 0.6218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.69049\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 325 | loss: 0.69049 - acc: 0.6396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.68886\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 326 | loss: 0.68886 - acc: 0.6556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.68738\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 327 | loss: 0.68738 - acc: 0.6701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.68605\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 328 | loss: 0.68605 - acc: 0.6831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.68486\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 329 | loss: 0.68486 - acc: 0.6948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.68379\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 330 | loss: 0.68379 - acc: 0.7053 -- iter: 10/10\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.68282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 331 | loss: 0.68282 - acc: 0.7148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.68194\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 332 | loss: 0.68194 - acc: 0.7233 -- iter: 10/10\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.68115\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 333 | loss: 0.68115 - acc: 0.7309 -- iter: 10/10\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.68045\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 334 | loss: 0.68045 - acc: 0.7379 -- iter: 10/10\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.67981\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 335 | loss: 0.67981 - acc: 0.7441 -- iter: 10/10\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.67924\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 336 | loss: 0.67924 - acc: 0.7497 -- iter: 10/10\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.67871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 337 | loss: 0.67871 - acc: 0.7547 -- iter: 10/10\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.67825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 338 | loss: 0.67825 - acc: 0.7592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.67783\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 339 | loss: 0.67783 - acc: 0.7633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.67745\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 340 | loss: 0.67745 - acc: 0.7670 -- iter: 10/10\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.67711\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 341 | loss: 0.67711 - acc: 0.7703 -- iter: 10/10\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.67680\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 342 | loss: 0.67680 - acc: 0.7732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.67652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 343 | loss: 0.67652 - acc: 0.7759 -- iter: 10/10\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.68327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 344 | loss: 0.68327 - acc: 0.7083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.68235\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 345 | loss: 0.68235 - acc: 0.7175 -- iter: 10/10\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.68151\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 346 | loss: 0.68151 - acc: 0.7257 -- iter: 10/10\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.68077\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 347 | loss: 0.68077 - acc: 0.7332 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.68009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 348 | loss: 0.68009 - acc: 0.7399 -- iter: 10/10\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.67948\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 349 | loss: 0.67948 - acc: 0.7459 -- iter: 10/10\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.68448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 350 | loss: 0.68448 - acc: 0.6913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.68352\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 351 | loss: 0.68352 - acc: 0.7022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.68266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 352 | loss: 0.68266 - acc: 0.7119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.68190\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 353 | loss: 0.68190 - acc: 0.7207 -- iter: 10/10\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.68121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 354 | loss: 0.68121 - acc: 0.7287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.68061\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 355 | loss: 0.68061 - acc: 0.7358 -- iter: 10/10\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.68005\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 356 | loss: 0.68005 - acc: 0.7422 -- iter: 10/10\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.67956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 357 | loss: 0.67956 - acc: 0.7480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.67917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 358 | loss: 0.67917 - acc: 0.7532 -- iter: 10/10\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.67881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 359 | loss: 0.67881 - acc: 0.7579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.68643\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 360 | loss: 0.68643 - acc: 0.6821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 361 | loss: 0.68531 - acc: 0.6939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.68430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 362 | loss: 0.68430 - acc: 0.7045 -- iter: 10/10\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.68338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 363 | loss: 0.68338 - acc: 0.7140 -- iter: 10/10\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.68953\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 364 | loss: 0.68953 - acc: 0.6526 -- iter: 10/10\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.68808\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 365 | loss: 0.68808 - acc: 0.6674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.68674\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 366 | loss: 0.68674 - acc: 0.6806 -- iter: 10/10\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.68554\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 367 | loss: 0.68554 - acc: 0.6926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.68448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 368 | loss: 0.68448 - acc: 0.7033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.68353\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 369 | loss: 0.68353 - acc: 0.7130 -- iter: 10/10\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.68268\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 370 | loss: 0.68268 - acc: 0.7217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.68190\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 371 | loss: 0.68190 - acc: 0.7295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.68818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 372 | loss: 0.68818 - acc: 0.6666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.68683\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 373 | loss: 0.68683 - acc: 0.6799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.69344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 374 | loss: 0.69344 - acc: 0.6119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.69160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 375 | loss: 0.69160 - acc: 0.6307 -- iter: 10/10\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.69695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 376 | loss: 0.69695 - acc: 0.5777 -- iter: 10/10\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.69480\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 377 | loss: 0.69480 - acc: 0.5999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.69293\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 378 | loss: 0.69293 - acc: 0.6199 -- iter: 10/10\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.69121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 379 | loss: 0.69121 - acc: 0.6379 -- iter: 10/10\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.68962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 380 | loss: 0.68962 - acc: 0.6541 -- iter: 10/10\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.68818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 381 | loss: 0.68818 - acc: 0.6687 -- iter: 10/10\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.69487\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 382 | loss: 0.69487 - acc: 0.6018 -- iter: 10/10\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.69290\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 383 | loss: 0.69290 - acc: 0.6217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.69112\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 384 | loss: 0.69112 - acc: 0.6395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.68951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 385 | loss: 0.68951 - acc: 0.6555 -- iter: 10/10\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.68804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 386 | loss: 0.68804 - acc: 0.6700 -- iter: 10/10\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.68670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 387 | loss: 0.68670 - acc: 0.6830 -- iter: 10/10\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.68552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 388 | loss: 0.68552 - acc: 0.6947 -- iter: 10/10\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.68446\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 389 | loss: 0.68446 - acc: 0.7052 -- iter: 10/10\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.68352\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 390 | loss: 0.68352 - acc: 0.7147 -- iter: 10/10\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.68266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 391 | loss: 0.68266 - acc: 0.7232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.68961\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 392 | loss: 0.68961 - acc: 0.6509 -- iter: 10/10\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.68816\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 393 | loss: 0.68816 - acc: 0.6658 -- iter: 10/10\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.68687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 394 | loss: 0.68687 - acc: 0.6792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.68571\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 395 | loss: 0.68571 - acc: 0.6913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.69162\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 396 | loss: 0.69162 - acc: 0.6322 -- iter: 10/10\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.69094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 397 | loss: 0.69094 - acc: 0.6390 -- iter: 10/10\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.68987\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 398 | loss: 0.68987 - acc: 0.6451 -- iter: 10/10\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.68840\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 399 | loss: 0.68840 - acc: 0.6606 -- iter: 10/10\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.68588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 400 | loss: 0.68588 - acc: 0.6745 -- iter: 10/10\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.68588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 401 | loss: 0.68588 - acc: 0.6871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.68481\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 402 | loss: 0.68481 - acc: 0.6983 -- iter: 10/10\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.68381\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 403 | loss: 0.68381 - acc: 0.7085 -- iter: 10/10\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.68885\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 404 | loss: 0.68885 - acc: 0.6577 -- iter: 10/10\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.68768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 405 | loss: 0.68768 - acc: 0.6719 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.68653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 406 | loss: 0.68653 - acc: 0.6847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.68540\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 407 | loss: 0.68540 - acc: 0.6962 -- iter: 10/10\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.68437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 408 | loss: 0.68437 - acc: 0.7066 -- iter: 10/10\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.68341\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 409 | loss: 0.68341 - acc: 0.7160 -- iter: 10/10\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.68247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 410 | loss: 0.68247 - acc: 0.7244 -- iter: 10/10\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.68168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 411 | loss: 0.68168 - acc: 0.7319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.68098\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 412 | loss: 0.68098 - acc: 0.7387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.68035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 413 | loss: 0.68035 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.67977\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 414 | loss: 0.67977 - acc: 0.7504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.67926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 415 | loss: 0.67926 - acc: 0.7553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.67879\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 416 | loss: 0.67879 - acc: 0.7598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.67837\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 417 | loss: 0.67837 - acc: 0.7638 -- iter: 10/10\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.68399\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 418 | loss: 0.68399 - acc: 0.7074 -- iter: 10/10\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.68305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 419 | loss: 0.68305 - acc: 0.7167 -- iter: 10/10\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.68720\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 420 | loss: 0.68720 - acc: 0.6750 -- iter: 10/10\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.68594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 421 | loss: 0.68594 - acc: 0.6875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.68481\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 422 | loss: 0.68481 - acc: 0.6988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.68379\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 423 | loss: 0.68379 - acc: 0.7089 -- iter: 10/10\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.68287\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 424 | loss: 0.68287 - acc: 0.7180 -- iter: 10/10\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.68204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 425 | loss: 0.68204 - acc: 0.7262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.68129\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 426 | loss: 0.68129 - acc: 0.7336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.68062\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 427 | loss: 0.68062 - acc: 0.7402 -- iter: 10/10\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.68702\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 428 | loss: 0.68702 - acc: 0.6762 -- iter: 10/10\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.68577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 429 | loss: 0.68577 - acc: 0.6886 -- iter: 10/10\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.68465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 430 | loss: 0.68465 - acc: 0.6997 -- iter: 10/10\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.68365\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 431 | loss: 0.68365 - acc: 0.7098 -- iter: 10/10\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.68274\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 432 | loss: 0.68274 - acc: 0.7188 -- iter: 10/10\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.68192\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 433 | loss: 0.68192 - acc: 0.7269 -- iter: 10/10\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.68919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 434 | loss: 0.68919 - acc: 0.6542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.68773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 435 | loss: 0.68773 - acc: 0.6688 -- iter: 10/10\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.69441\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 436 | loss: 0.69441 - acc: 0.6019 -- iter: 10/10\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.69243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 437 | loss: 0.69243 - acc: 0.6217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.69664\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 438 | loss: 0.69664 - acc: 0.5795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.69443\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 439 | loss: 0.69443 - acc: 0.6016 -- iter: 10/10\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.69245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 440 | loss: 0.69245 - acc: 0.6214 -- iter: 10/10\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.69066\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 441 | loss: 0.69066 - acc: 0.6393 -- iter: 10/10\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.68905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 442 | loss: 0.68905 - acc: 0.6554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.68760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 443 | loss: 0.68760 - acc: 0.6698 -- iter: 10/10\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.68629\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 444 | loss: 0.68629 - acc: 0.6828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.68511\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 445 | loss: 0.68511 - acc: 0.6946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.68404\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 446 | loss: 0.68404 - acc: 0.7051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.68308\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 447 | loss: 0.68308 - acc: 0.7146 -- iter: 10/10\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.69022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 448 | loss: 0.69022 - acc: 0.6431 -- iter: 10/10\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.68864\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 449 | loss: 0.68864 - acc: 0.6588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.69270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 450 | loss: 0.69270 - acc: 0.6229 -- iter: 10/10\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 451 | loss: 0.69095 - acc: 0.6406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.69437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 452 | loss: 0.69437 - acc: 0.6066 -- iter: 10/10\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.69245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 453 | loss: 0.69245 - acc: 0.6259 -- iter: 10/10\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.69109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 454 | loss: 0.69109 - acc: 0.6433 -- iter: 10/10\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.68951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 455 | loss: 0.68951 - acc: 0.6590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.68809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 456 | loss: 0.68809 - acc: 0.6731 -- iter: 10/10\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.68681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 457 | loss: 0.68681 - acc: 0.6858 -- iter: 10/10\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.68565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 458 | loss: 0.68565 - acc: 0.6972 -- iter: 10/10\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.68461\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 459 | loss: 0.68461 - acc: 0.7075 -- iter: 10/10\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.68366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 460 | loss: 0.68366 - acc: 0.7167 -- iter: 10/10\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.68275\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 461 | loss: 0.68275 - acc: 0.7251 -- iter: 10/10\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.68198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 462 | loss: 0.68198 - acc: 0.7326 -- iter: 10/10\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.68130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 463 | loss: 0.68130 - acc: 0.7393 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.68068\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 464 | loss: 0.68068 - acc: 0.7454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.68008\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 465 | loss: 0.68008 - acc: 0.7508 -- iter: 10/10\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.67957\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 466 | loss: 0.67957 - acc: 0.7558 -- iter: 10/10\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.67912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 467 | loss: 0.67912 - acc: 0.7602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.67873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 468 | loss: 0.67873 - acc: 0.7642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.67837\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 469 | loss: 0.67837 - acc: 0.7677 -- iter: 10/10\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.68405\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 470 | loss: 0.68405 - acc: 0.7110 -- iter: 10/10\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.68316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 471 | loss: 0.68316 - acc: 0.7199 -- iter: 10/10\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 472 | loss: 0.68236 - acc: 0.7279 -- iter: 10/10\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 473 | loss: 0.68164 - acc: 0.7351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.68099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 474 | loss: 0.68099 - acc: 0.7416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.68041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 475 | loss: 0.68041 - acc: 0.7474 -- iter: 10/10\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.67989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 476 | loss: 0.67989 - acc: 0.7527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.67941\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 477 | loss: 0.67941 - acc: 0.7574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.67899\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 478 | loss: 0.67899 - acc: 0.7617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.67861\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 479 | loss: 0.67861 - acc: 0.7655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.67826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 480 | loss: 0.67826 - acc: 0.7690 -- iter: 10/10\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.67795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 481 | loss: 0.67795 - acc: 0.7721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.67767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 482 | loss: 0.67767 - acc: 0.7749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.67742\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 483 | loss: 0.67742 - acc: 0.7774 -- iter: 10/10\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.67720\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 484 | loss: 0.67720 - acc: 0.7796 -- iter: 10/10\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.67699\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 485 | loss: 0.67699 - acc: 0.7817 -- iter: 10/10\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.68381\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 486 | loss: 0.68381 - acc: 0.7135 -- iter: 10/10\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.68295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 487 | loss: 0.68295 - acc: 0.7222 -- iter: 10/10\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.68217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 488 | loss: 0.68217 - acc: 0.7299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.68147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 489 | loss: 0.68147 - acc: 0.7369 -- iter: 10/10\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.68084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 490 | loss: 0.68084 - acc: 0.7432 -- iter: 10/10\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.68027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 491 | loss: 0.68027 - acc: 0.7489 -- iter: 10/10\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.67976\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 492 | loss: 0.67976 - acc: 0.7540 -- iter: 10/10\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.67930\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 493 | loss: 0.67930 - acc: 0.7586 -- iter: 10/10\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.68689\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 494 | loss: 0.68689 - acc: 0.6828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.68571\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 495 | loss: 0.68571 - acc: 0.6945 -- iter: 10/10\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.68466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 496 | loss: 0.68466 - acc: 0.7050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.68371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 497 | loss: 0.68371 - acc: 0.7145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.68286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 498 | loss: 0.68286 - acc: 0.7231 -- iter: 10/10\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.68209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 499 | loss: 0.68209 - acc: 0.7308 -- iter: 10/10\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.68139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 500 | loss: 0.68139 - acc: 0.7377 -- iter: 10/10\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.68077\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 501 | loss: 0.68077 - acc: 0.7439 -- iter: 10/10\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.68721\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 502 | loss: 0.68721 - acc: 0.6795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.68601\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 503 | loss: 0.68601 - acc: 0.6916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.69292\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 504 | loss: 0.69292 - acc: 0.6224 -- iter: 10/10\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.69115\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 505 | loss: 0.69115 - acc: 0.6402 -- iter: 10/10\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.68955\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 506 | loss: 0.68955 - acc: 0.6562 -- iter: 10/10\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.68811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 507 | loss: 0.68811 - acc: 0.6705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.68681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 508 | loss: 0.68681 - acc: 0.6835 -- iter: 10/10\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.68565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 509 | loss: 0.68565 - acc: 0.6951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.68460\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 510 | loss: 0.68460 - acc: 0.7056 -- iter: 10/10\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.68366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 511 | loss: 0.68366 - acc: 0.7151 -- iter: 10/10\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.68281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 512 | loss: 0.68281 - acc: 0.7236 -- iter: 10/10\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.68204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 513 | loss: 0.68204 - acc: 0.7312 -- iter: 10/10\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.68136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 514 | loss: 0.68136 - acc: 0.7381 -- iter: 10/10\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.68074\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 515 | loss: 0.68074 - acc: 0.7443 -- iter: 10/10\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.68018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 516 | loss: 0.68018 - acc: 0.7498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.67968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 517 | loss: 0.67968 - acc: 0.7549 -- iter: 10/10\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.67923\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 518 | loss: 0.67923 - acc: 0.7594 -- iter: 10/10\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.67882\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 519 | loss: 0.67882 - acc: 0.7634 -- iter: 10/10\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.67845\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 520 | loss: 0.67845 - acc: 0.7671 -- iter: 10/10\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.67813\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 521 | loss: 0.67813 - acc: 0.7704 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.68383\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 522 | loss: 0.68383 - acc: 0.7133 -- iter: 10/10\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.68296\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 523 | loss: 0.68296 - acc: 0.7220 -- iter: 10/10\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.68218\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 524 | loss: 0.68218 - acc: 0.7298 -- iter: 10/10\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.68148\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 525 | loss: 0.68148 - acc: 0.7368 -- iter: 10/10\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.68085\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 526 | loss: 0.68085 - acc: 0.7431 -- iter: 10/10\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.68028\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 527 | loss: 0.68028 - acc: 0.7488 -- iter: 10/10\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.67977\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 528 | loss: 0.67977 - acc: 0.7539 -- iter: 10/10\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.67931\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 529 | loss: 0.67931 - acc: 0.7586 -- iter: 10/10\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.67889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 530 | loss: 0.67889 - acc: 0.7627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.67852\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 531 | loss: 0.67852 - acc: 0.7664 -- iter: 10/10\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.67818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 532 | loss: 0.67818 - acc: 0.7698 -- iter: 10/10\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.67788\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 533 | loss: 0.67788 - acc: 0.7728 -- iter: 10/10\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.68561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 534 | loss: 0.68561 - acc: 0.6955 -- iter: 10/10\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.68457\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 535 | loss: 0.68457 - acc: 0.7060 -- iter: 10/10\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.69062\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 536 | loss: 0.69062 - acc: 0.6454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.68908\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 537 | loss: 0.68908 - acc: 0.6608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.68769\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 538 | loss: 0.68769 - acc: 0.6748 -- iter: 10/10\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.68643\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 539 | loss: 0.68643 - acc: 0.6873 -- iter: 10/10\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 540 | loss: 0.68531 - acc: 0.6986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.68429\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 541 | loss: 0.68429 - acc: 0.7087 -- iter: 10/10\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.68338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 542 | loss: 0.68338 - acc: 0.7178 -- iter: 10/10\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.68256\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 543 | loss: 0.68256 - acc: 0.7260 -- iter: 10/10\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.68782\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 544 | loss: 0.68782 - acc: 0.6734 -- iter: 10/10\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.68655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 545 | loss: 0.68655 - acc: 0.6861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.69241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 546 | loss: 0.69241 - acc: 0.6275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.69069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 547 | loss: 0.69069 - acc: 0.6447 -- iter: 10/10\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.69514\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 548 | loss: 0.69514 - acc: 0.6003 -- iter: 10/10\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.69314\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 549 | loss: 0.69314 - acc: 0.6202 -- iter: 10/10\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.69134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 550 | loss: 0.69134 - acc: 0.6382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.68972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 551 | loss: 0.68972 - acc: 0.6544 -- iter: 10/10\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.68827\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 552 | loss: 0.68827 - acc: 0.6690 -- iter: 10/10\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.68696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 553 | loss: 0.68696 - acc: 0.6821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.69378\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 554 | loss: 0.69378 - acc: 0.6139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.69191\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 555 | loss: 0.69191 - acc: 0.6325 -- iter: 10/10\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.69024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 556 | loss: 0.69024 - acc: 0.6492 -- iter: 10/10\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.68873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 557 | loss: 0.68873 - acc: 0.6643 -- iter: 10/10\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.69437\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 558 | loss: 0.69437 - acc: 0.6079 -- iter: 10/10\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.69245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 559 | loss: 0.69245 - acc: 0.6271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.69072\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 560 | loss: 0.69072 - acc: 0.6444 -- iter: 10/10\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.68917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 561 | loss: 0.68917 - acc: 0.6599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.68777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 562 | loss: 0.68777 - acc: 0.6739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.68651\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 563 | loss: 0.68651 - acc: 0.6865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.68537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 564 | loss: 0.68537 - acc: 0.6979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.68435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 565 | loss: 0.68435 - acc: 0.7081 -- iter: 10/10\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.69143\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 566 | loss: 0.69143 - acc: 0.6373 -- iter: 10/10\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.68980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 567 | loss: 0.68980 - acc: 0.6536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.68834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 568 | loss: 0.68834 - acc: 0.6682 -- iter: 10/10\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.68702\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 569 | loss: 0.68702 - acc: 0.6814 -- iter: 10/10\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.69384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 570 | loss: 0.69384 - acc: 0.6132 -- iter: 10/10\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.69197\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 571 | loss: 0.69197 - acc: 0.6319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.69529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 572 | loss: 0.69529 - acc: 0.5987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.69328\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 573 | loss: 0.69328 - acc: 0.6189 -- iter: 10/10\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.69146\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 574 | loss: 0.69146 - acc: 0.6370 -- iter: 10/10\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.68983\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 575 | loss: 0.68983 - acc: 0.6533 -- iter: 10/10\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.68837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 576 | loss: 0.68837 - acc: 0.6679 -- iter: 10/10\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.68705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 577 | loss: 0.68705 - acc: 0.6812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.68586\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 578 | loss: 0.68586 - acc: 0.6930 -- iter: 10/10\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.68479\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 579 | loss: 0.68479 - acc: 0.7037 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.69182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 580 | loss: 0.69182 - acc: 0.6334 -- iter: 10/10\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.69016\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 581 | loss: 0.69016 - acc: 0.6500 -- iter: 10/10\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.69566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 582 | loss: 0.69566 - acc: 0.5950 -- iter: 10/10\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.69361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 583 | loss: 0.69361 - acc: 0.6155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.69176\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 584 | loss: 0.69176 - acc: 0.6340 -- iter: 10/10\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.69010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 585 | loss: 0.69010 - acc: 0.6506 -- iter: 10/10\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.68861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 586 | loss: 0.68861 - acc: 0.6655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.68726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 587 | loss: 0.68726 - acc: 0.6790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.69205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 588 | loss: 0.69205 - acc: 0.6311 -- iter: 10/10\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.69036\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 589 | loss: 0.69036 - acc: 0.6480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.68884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 590 | loss: 0.68884 - acc: 0.6632 -- iter: 10/10\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.68748\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 591 | loss: 0.68748 - acc: 0.6768 -- iter: 10/10\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.68624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 592 | loss: 0.68624 - acc: 0.6892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.68514\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 593 | loss: 0.68514 - acc: 0.7002 -- iter: 10/10\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.68414\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 594 | loss: 0.68414 - acc: 0.7102 -- iter: 10/10\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.68324\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 595 | loss: 0.68324 - acc: 0.7192 -- iter: 10/10\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.68243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 596 | loss: 0.68243 - acc: 0.7273 -- iter: 10/10\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.68171\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 597 | loss: 0.68171 - acc: 0.7346 -- iter: 10/10\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.68105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 598 | loss: 0.68105 - acc: 0.7411 -- iter: 10/10\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.68046\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 599 | loss: 0.68046 - acc: 0.7470 -- iter: 10/10\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.67993\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 600 | loss: 0.67993 - acc: 0.7523 -- iter: 10/10\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.67946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 601 | loss: 0.67946 - acc: 0.7571 -- iter: 10/10\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.67903\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 602 | loss: 0.67903 - acc: 0.7614 -- iter: 10/10\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.67864\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 603 | loss: 0.67864 - acc: 0.7652 -- iter: 10/10\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.67829\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 604 | loss: 0.67829 - acc: 0.7687 -- iter: 10/10\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.67798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 605 | loss: 0.67798 - acc: 0.7718 -- iter: 10/10\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.67770\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 606 | loss: 0.67770 - acc: 0.7746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.67744\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 607 | loss: 0.67744 - acc: 0.7772 -- iter: 10/10\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.67721\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 608 | loss: 0.67721 - acc: 0.7795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.67701\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 609 | loss: 0.67701 - acc: 0.7815 -- iter: 10/10\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.68482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 610 | loss: 0.68482 - acc: 0.7034 -- iter: 10/10\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.68386\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 611 | loss: 0.68386 - acc: 0.7130 -- iter: 10/10\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.68299\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 612 | loss: 0.68299 - acc: 0.7217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.68221\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 613 | loss: 0.68221 - acc: 0.7296 -- iter: 10/10\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.68750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 614 | loss: 0.68750 - acc: 0.6766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.68627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 615 | loss: 0.68627 - acc: 0.6889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.68516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 616 | loss: 0.68516 - acc: 0.7000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.68416\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 617 | loss: 0.68416 - acc: 0.7100 -- iter: 10/10\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.68326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 618 | loss: 0.68326 - acc: 0.7190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.68245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 619 | loss: 0.68245 - acc: 0.7271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.68872\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 620 | loss: 0.68872 - acc: 0.6644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.68736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 621 | loss: 0.68736 - acc: 0.6780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.68614\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 622 | loss: 0.68614 - acc: 0.6902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.68504\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 623 | loss: 0.68504 - acc: 0.7012 -- iter: 10/10\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.69006\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 624 | loss: 0.69006 - acc: 0.6510 -- iter: 10/10\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.68857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 625 | loss: 0.68857 - acc: 0.6659 -- iter: 10/10\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.68723\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 626 | loss: 0.68723 - acc: 0.6793 -- iter: 10/10\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.68602\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 627 | loss: 0.68602 - acc: 0.6914 -- iter: 10/10\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.69093\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 628 | loss: 0.69093 - acc: 0.6423 -- iter: 10/10\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.68936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 629 | loss: 0.68936 - acc: 0.6580 -- iter: 10/10\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.69394\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 630 | loss: 0.69394 - acc: 0.6122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.69206\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 631 | loss: 0.69206 - acc: 0.6310 -- iter: 10/10\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.69737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 632 | loss: 0.69737 - acc: 0.5779 -- iter: 10/10\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.69515\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 633 | loss: 0.69515 - acc: 0.6001 -- iter: 10/10\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.69315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 634 | loss: 0.69315 - acc: 0.6201 -- iter: 10/10\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.69135\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 635 | loss: 0.69135 - acc: 0.6381 -- iter: 10/10\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.68973\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 636 | loss: 0.68973 - acc: 0.6543 -- iter: 10/10\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.68827\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 637 | loss: 0.68827 - acc: 0.6689 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.68696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 638 | loss: 0.68696 - acc: 0.6820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.68578\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 639 | loss: 0.68578 - acc: 0.6938 -- iter: 10/10\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.68472\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 640 | loss: 0.68472 - acc: 0.7044 -- iter: 10/10\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.68376\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 641 | loss: 0.68376 - acc: 0.7140 -- iter: 10/10\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.68590\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 642 | loss: 0.68590 - acc: 0.6926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.68483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 643 | loss: 0.68483 - acc: 0.7033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.69186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 644 | loss: 0.69186 - acc: 0.6330 -- iter: 10/10\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.69019\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 645 | loss: 0.69019 - acc: 0.6497 -- iter: 10/10\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.68869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 646 | loss: 0.68869 - acc: 0.6647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.68734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 647 | loss: 0.68734 - acc: 0.6782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.68612\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 648 | loss: 0.68612 - acc: 0.6904 -- iter: 10/10\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.68502\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 649 | loss: 0.68502 - acc: 0.7014 -- iter: 10/10\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.68404\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 650 | loss: 0.68404 - acc: 0.7112 -- iter: 10/10\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.68315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 651 | loss: 0.68315 - acc: 0.7201 -- iter: 10/10\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.68235\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 652 | loss: 0.68235 - acc: 0.7281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.68163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 653 | loss: 0.68163 - acc: 0.7353 -- iter: 10/10\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.68898\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 654 | loss: 0.68898 - acc: 0.6618 -- iter: 10/10\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.68760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 655 | loss: 0.68760 - acc: 0.6756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.69036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 656 | loss: 0.69036 - acc: 0.6480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.68884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 657 | loss: 0.68884 - acc: 0.6632 -- iter: 10/10\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.68747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 658 | loss: 0.68747 - acc: 0.6769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.68624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 659 | loss: 0.68624 - acc: 0.6892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.68513\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 660 | loss: 0.68513 - acc: 0.7003 -- iter: 10/10\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.68413\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 661 | loss: 0.68413 - acc: 0.7103 -- iter: 10/10\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.69024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 662 | loss: 0.69024 - acc: 0.6492 -- iter: 10/10\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.68873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 663 | loss: 0.68873 - acc: 0.6643 -- iter: 10/10\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.69537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 664 | loss: 0.69537 - acc: 0.5979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.69335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 665 | loss: 0.69335 - acc: 0.6181 -- iter: 10/10\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.69153\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 666 | loss: 0.69153 - acc: 0.6363 -- iter: 10/10\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.68989\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 667 | loss: 0.68989 - acc: 0.6527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.69542\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 668 | loss: 0.69542 - acc: 0.5974 -- iter: 10/10\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.69339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 669 | loss: 0.69339 - acc: 0.6177 -- iter: 10/10\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.69157\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 670 | loss: 0.69157 - acc: 0.6359 -- iter: 10/10\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.68993\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 671 | loss: 0.68993 - acc: 0.6523 -- iter: 10/10\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.68845\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 672 | loss: 0.68845 - acc: 0.6671 -- iter: 10/10\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.68712\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 673 | loss: 0.68712 - acc: 0.6804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.68593\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 674 | loss: 0.68593 - acc: 0.6923 -- iter: 10/10\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.68485\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 675 | loss: 0.68485 - acc: 0.7031 -- iter: 10/10\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.69188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 676 | loss: 0.69188 - acc: 0.6328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.69021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 677 | loss: 0.69021 - acc: 0.6495 -- iter: 10/10\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.69670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 678 | loss: 0.69670 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.69455\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 679 | loss: 0.69455 - acc: 0.6061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.69261\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 680 | loss: 0.69261 - acc: 0.6255 -- iter: 10/10\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.69087\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 681 | loss: 0.69087 - acc: 0.6429 -- iter: 10/10\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.69530\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 682 | loss: 0.69530 - acc: 0.5986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.69328\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 683 | loss: 0.69328 - acc: 0.6188 -- iter: 10/10\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.69847\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 684 | loss: 0.69847 - acc: 0.5669 -- iter: 10/10\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.69614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 685 | loss: 0.69614 - acc: 0.5902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.69404\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 686 | loss: 0.69404 - acc: 0.6112 -- iter: 10/10\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.69215\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 687 | loss: 0.69215 - acc: 0.6301 -- iter: 10/10\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.69045\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 688 | loss: 0.69045 - acc: 0.6471 -- iter: 10/10\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.68892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 689 | loss: 0.68892 - acc: 0.6624 -- iter: 10/10\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.68755\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 690 | loss: 0.68755 - acc: 0.6761 -- iter: 10/10\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.68631\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 691 | loss: 0.68631 - acc: 0.6885 -- iter: 10/10\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.68519\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 692 | loss: 0.68519 - acc: 0.6997 -- iter: 10/10\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.68419\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 693 | loss: 0.68419 - acc: 0.7097 -- iter: 10/10\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.68329\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 694 | loss: 0.68329 - acc: 0.7187 -- iter: 10/10\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.68247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 695 | loss: 0.68247 - acc: 0.7269 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.68174\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 696 | loss: 0.68174 - acc: 0.7342 -- iter: 10/10\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.68109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 697 | loss: 0.68109 - acc: 0.7407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.68749\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 698 | loss: 0.68749 - acc: 0.6767 -- iter: 10/10\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.68626\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 699 | loss: 0.68626 - acc: 0.6890 -- iter: 10/10\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.68515\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 700 | loss: 0.68515 - acc: 0.7001 -- iter: 10/10\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.68415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 701 | loss: 0.68415 - acc: 0.7101 -- iter: 10/10\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.69125\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 702 | loss: 0.69125 - acc: 0.6391 -- iter: 10/10\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.68964\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 703 | loss: 0.68964 - acc: 0.6552 -- iter: 10/10\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.68819\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 704 | loss: 0.68819 - acc: 0.6697 -- iter: 10/10\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.68689\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 705 | loss: 0.68689 - acc: 0.6827 -- iter: 10/10\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.68572\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 706 | loss: 0.68572 - acc: 0.6944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.68466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 707 | loss: 0.68466 - acc: 0.7050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.68371\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 708 | loss: 0.68371 - acc: 0.7145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.68286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 709 | loss: 0.68286 - acc: 0.7230 -- iter: 10/10\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.68809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 710 | loss: 0.68809 - acc: 0.6707 -- iter: 10/10\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.68679\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 711 | loss: 0.68679 - acc: 0.6837 -- iter: 10/10\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.69263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 712 | loss: 0.69263 - acc: 0.6253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.69088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 713 | loss: 0.69088 - acc: 0.6428 -- iter: 10/10\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.68931\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 714 | loss: 0.68931 - acc: 0.6585 -- iter: 10/10\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.68790\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 715 | loss: 0.68790 - acc: 0.6726 -- iter: 10/10\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.68662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 716 | loss: 0.68662 - acc: 0.6854 -- iter: 10/10\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.68548\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 717 | loss: 0.68548 - acc: 0.6968 -- iter: 10/10\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.69144\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 718 | loss: 0.69144 - acc: 0.6372 -- iter: 10/10\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.68982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 719 | loss: 0.68982 - acc: 0.6534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.68835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 720 | loss: 0.68835 - acc: 0.6681 -- iter: 10/10\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.68703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 721 | loss: 0.68703 - acc: 0.6813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.69284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 722 | loss: 0.69284 - acc: 0.6232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.69108\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 723 | loss: 0.69108 - acc: 0.6408 -- iter: 10/10\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.68948\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 724 | loss: 0.68948 - acc: 0.6568 -- iter: 10/10\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.68805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 725 | loss: 0.68805 - acc: 0.6711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.69276\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 726 | loss: 0.69276 - acc: 0.6240 -- iter: 10/10\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.69100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 727 | loss: 0.69100 - acc: 0.6416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.68942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 728 | loss: 0.68942 - acc: 0.6574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.68799\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 729 | loss: 0.68799 - acc: 0.6717 -- iter: 10/10\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.68671\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 730 | loss: 0.68671 - acc: 0.6845 -- iter: 10/10\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.68555\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 731 | loss: 0.68555 - acc: 0.6961 -- iter: 10/10\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.69151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 732 | loss: 0.69151 - acc: 0.6365 -- iter: 10/10\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.68988\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 733 | loss: 0.68988 - acc: 0.6528 -- iter: 10/10\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.68841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 734 | loss: 0.68841 - acc: 0.6675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.68708\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 735 | loss: 0.68708 - acc: 0.6808 -- iter: 10/10\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.68589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 736 | loss: 0.68589 - acc: 0.6927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.68482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 737 | loss: 0.68482 - acc: 0.7034 -- iter: 10/10\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.69185\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 738 | loss: 0.69185 - acc: 0.6331 -- iter: 10/10\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.69018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 739 | loss: 0.69018 - acc: 0.6498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.68868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 740 | loss: 0.68868 - acc: 0.6648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.68733\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 741 | loss: 0.68733 - acc: 0.6783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.68611\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 742 | loss: 0.68611 - acc: 0.6905 -- iter: 10/10\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.68502\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 743 | loss: 0.68502 - acc: 0.7014 -- iter: 10/10\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.69103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 744 | loss: 0.69103 - acc: 0.6413 -- iter: 10/10\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.68944\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 745 | loss: 0.68944 - acc: 0.6572 -- iter: 10/10\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.68802\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 746 | loss: 0.68802 - acc: 0.6714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.68673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 747 | loss: 0.68673 - acc: 0.6843 -- iter: 10/10\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.68557\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 748 | loss: 0.68557 - acc: 0.6959 -- iter: 10/10\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.68453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 749 | loss: 0.68453 - acc: 0.7063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.69059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 750 | loss: 0.69059 - acc: 0.6457 -- iter: 10/10\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.68905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 751 | loss: 0.68905 - acc: 0.6611 -- iter: 10/10\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.68766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 752 | loss: 0.68766 - acc: 0.6750 -- iter: 10/10\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.68641\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 753 | loss: 0.68641 - acc: 0.6875 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.68529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 754 | loss: 0.68529 - acc: 0.6987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.68427\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 755 | loss: 0.68427 - acc: 0.7089 -- iter: 10/10\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.68336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 756 | loss: 0.68336 - acc: 0.7180 -- iter: 10/10\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.68254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 757 | loss: 0.68254 - acc: 0.7262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.68180\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 758 | loss: 0.68180 - acc: 0.7336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.68114\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 759 | loss: 0.68114 - acc: 0.7402 -- iter: 10/10\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.68854\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 760 | loss: 0.68854 - acc: 0.6662 -- iter: 10/10\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.68720\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 761 | loss: 0.68720 - acc: 0.6796 -- iter: 10/10\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.68600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 762 | loss: 0.68600 - acc: 0.6916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.68492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 763 | loss: 0.68492 - acc: 0.7024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.69094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 764 | loss: 0.69094 - acc: 0.6422 -- iter: 10/10\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.68936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 765 | loss: 0.68936 - acc: 0.6580 -- iter: 10/10\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.69594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 766 | loss: 0.69594 - acc: 0.5922 -- iter: 10/10\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.69386\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 767 | loss: 0.69386 - acc: 0.6130 -- iter: 10/10\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.69999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 768 | loss: 0.69999 - acc: 0.5517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.69751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 769 | loss: 0.69751 - acc: 0.5765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.69527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 770 | loss: 0.69527 - acc: 0.5989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.69326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 771 | loss: 0.69326 - acc: 0.6190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.69145\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 772 | loss: 0.69145 - acc: 0.6371 -- iter: 10/10\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.68982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 773 | loss: 0.68982 - acc: 0.6534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.68836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 774 | loss: 0.68836 - acc: 0.6680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.68704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 775 | loss: 0.68704 - acc: 0.6812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.68585\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 776 | loss: 0.68585 - acc: 0.6931 -- iter: 10/10\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.68478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 777 | loss: 0.68478 - acc: 0.7038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.68382\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 778 | loss: 0.68382 - acc: 0.7134 -- iter: 10/10\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.68295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 779 | loss: 0.68295 - acc: 0.7221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.68217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 780 | loss: 0.68217 - acc: 0.7299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.68147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 781 | loss: 0.68147 - acc: 0.7369 -- iter: 10/10\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.68784\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 782 | loss: 0.68784 - acc: 0.6732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.68657\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 783 | loss: 0.68657 - acc: 0.6859 -- iter: 10/10\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.69343\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 784 | loss: 0.69343 - acc: 0.6173 -- iter: 10/10\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.69160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 785 | loss: 0.69160 - acc: 0.6356 -- iter: 10/10\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.69696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 786 | loss: 0.69696 - acc: 0.5820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.69478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 787 | loss: 0.69478 - acc: 0.6038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.69282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 788 | loss: 0.69282 - acc: 0.6234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.69105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 789 | loss: 0.69105 - acc: 0.6411 -- iter: 10/10\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.68946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 790 | loss: 0.68946 - acc: 0.6570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.68803\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 791 | loss: 0.68803 - acc: 0.6713 -- iter: 10/10\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.68675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 792 | loss: 0.68675 - acc: 0.6841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.68559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 793 | loss: 0.68559 - acc: 0.6957 -- iter: 10/10\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.69154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 794 | loss: 0.69154 - acc: 0.6362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.68991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 795 | loss: 0.68991 - acc: 0.6525 -- iter: 10/10\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.69443\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 796 | loss: 0.69443 - acc: 0.6073 -- iter: 10/10\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.69250\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 797 | loss: 0.69250 - acc: 0.6266 -- iter: 10/10\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.69077\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 798 | loss: 0.69077 - acc: 0.6439 -- iter: 10/10\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.68921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 799 | loss: 0.68921 - acc: 0.6595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.69580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 800 | loss: 0.69580 - acc: 0.5936 -- iter: 10/10\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.69374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 801 | loss: 0.69374 - acc: 0.6142 -- iter: 10/10\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.69188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 802 | loss: 0.69188 - acc: 0.6328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.69021\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 803 | loss: 0.69021 - acc: 0.6495 -- iter: 10/10\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.68870\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 804 | loss: 0.68870 - acc: 0.6646 -- iter: 10/10\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.68735\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 805 | loss: 0.68735 - acc: 0.6781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.68613\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 806 | loss: 0.68613 - acc: 0.6903 -- iter: 10/10\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.68503\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 807 | loss: 0.68503 - acc: 0.7013 -- iter: 10/10\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.68405\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 808 | loss: 0.68405 - acc: 0.7111 -- iter: 10/10\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.68316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 809 | loss: 0.68316 - acc: 0.7200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 810 | loss: 0.68236 - acc: 0.7280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 811 | loss: 0.68164 - acc: 0.7352 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.68799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 812 | loss: 0.68799 - acc: 0.6717 -- iter: 10/10\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.68671\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 813 | loss: 0.68671 - acc: 0.6845 -- iter: 10/10\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.68555\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 814 | loss: 0.68555 - acc: 0.6961 -- iter: 10/10\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.68451\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 815 | loss: 0.68451 - acc: 0.7065 -- iter: 10/10\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.68358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 816 | loss: 0.68358 - acc: 0.7158 -- iter: 10/10\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.68274\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 817 | loss: 0.68274 - acc: 0.7242 -- iter: 10/10\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.68198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 818 | loss: 0.68198 - acc: 0.7318 -- iter: 10/10\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.68130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 819 | loss: 0.68130 - acc: 0.7386 -- iter: 10/10\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.68868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 820 | loss: 0.68868 - acc: 0.6648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.68733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 821 | loss: 0.68733 - acc: 0.6783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.69411\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 822 | loss: 0.69411 - acc: 0.6105 -- iter: 10/10\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.69222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 823 | loss: 0.69222 - acc: 0.6294 -- iter: 10/10\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.69751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 824 | loss: 0.69751 - acc: 0.5765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.69528\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 825 | loss: 0.69528 - acc: 0.5988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.69326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 826 | loss: 0.69326 - acc: 0.6189 -- iter: 10/10\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.69145\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 827 | loss: 0.69145 - acc: 0.6371 -- iter: 10/10\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.69682\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 828 | loss: 0.69682 - acc: 0.5833 -- iter: 10/10\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.69466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 829 | loss: 0.69466 - acc: 0.6050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.69271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 830 | loss: 0.69271 - acc: 0.6245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 831 | loss: 0.69095 - acc: 0.6421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.68937\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 832 | loss: 0.68937 - acc: 0.6579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.68795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 833 | loss: 0.68795 - acc: 0.6721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.68667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 834 | loss: 0.68667 - acc: 0.6849 -- iter: 10/10\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.68552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 835 | loss: 0.68552 - acc: 0.6964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.68449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 836 | loss: 0.68449 - acc: 0.7067 -- iter: 10/10\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.68355\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 837 | loss: 0.68355 - acc: 0.7161 -- iter: 10/10\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.68271\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 838 | loss: 0.68271 - acc: 0.7245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 839 | loss: 0.68196 - acc: 0.7320 -- iter: 10/10\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 840 | loss: 0.68128 - acc: 0.7388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.68067\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 841 | loss: 0.68067 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.68712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 842 | loss: 0.68712 - acc: 0.6804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.68592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 843 | loss: 0.68592 - acc: 0.6924 -- iter: 10/10\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.68484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 844 | loss: 0.68484 - acc: 0.7032 -- iter: 10/10\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.68388\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 845 | loss: 0.68388 - acc: 0.7128 -- iter: 10/10\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.68300\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 846 | loss: 0.68300 - acc: 0.7216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.68222\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 847 | loss: 0.68222 - acc: 0.7294 -- iter: 10/10\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.68151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 848 | loss: 0.68151 - acc: 0.7365 -- iter: 10/10\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.68088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 849 | loss: 0.68088 - acc: 0.7428 -- iter: 10/10\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.68031\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 850 | loss: 0.68031 - acc: 0.7485 -- iter: 10/10\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.67979\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 851 | loss: 0.67979 - acc: 0.7537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.67933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 852 | loss: 0.67933 - acc: 0.7583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.67891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 853 | loss: 0.67891 - acc: 0.7625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.68354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 854 | loss: 0.68354 - acc: 0.7162 -- iter: 10/10\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.68270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 855 | loss: 0.68270 - acc: 0.7246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.68994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 856 | loss: 0.68994 - acc: 0.6521 -- iter: 10/10\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.68847\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 857 | loss: 0.68847 - acc: 0.6669 -- iter: 10/10\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.68714\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 858 | loss: 0.68714 - acc: 0.6802 -- iter: 10/10\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.68594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 859 | loss: 0.68594 - acc: 0.6922 -- iter: 10/10\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.68486\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 860 | loss: 0.68486 - acc: 0.7030 -- iter: 10/10\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.68389\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 861 | loss: 0.68389 - acc: 0.7127 -- iter: 10/10\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.68302\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 862 | loss: 0.68302 - acc: 0.7214 -- iter: 10/10\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.68223\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 863 | loss: 0.68223 - acc: 0.7293 -- iter: 10/10\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.68852\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 864 | loss: 0.68852 - acc: 0.6664 -- iter: 10/10\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.68719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 865 | loss: 0.68719 - acc: 0.6797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.68598\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 866 | loss: 0.68598 - acc: 0.6917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.68490\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 867 | loss: 0.68490 - acc: 0.7026 -- iter: 10/10\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.68393\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 868 | loss: 0.68393 - acc: 0.7123 -- iter: 10/10\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.68305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 869 | loss: 0.68305 - acc: 0.7211 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.68226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 870 | loss: 0.68226 - acc: 0.7290 -- iter: 10/10\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.68155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 871 | loss: 0.68155 - acc: 0.7361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.68091\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 872 | loss: 0.68091 - acc: 0.7425 -- iter: 10/10\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.68034\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 873 | loss: 0.68034 - acc: 0.7482 -- iter: 10/10\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.67982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 874 | loss: 0.67982 - acc: 0.7534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.67935\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 875 | loss: 0.67935 - acc: 0.7581 -- iter: 10/10\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.67893\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 876 | loss: 0.67893 - acc: 0.7623 -- iter: 10/10\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.67856\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 877 | loss: 0.67856 - acc: 0.7660 -- iter: 10/10\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.67822\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 878 | loss: 0.67822 - acc: 0.7694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.67791\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 879 | loss: 0.67791 - acc: 0.7725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.67764\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 880 | loss: 0.67764 - acc: 0.7752 -- iter: 10/10\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.67739\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 881 | loss: 0.67739 - acc: 0.7777 -- iter: 10/10\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.67717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 882 | loss: 0.67717 - acc: 0.7799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.67696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 883 | loss: 0.67696 - acc: 0.7819 -- iter: 10/10\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.67678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 884 | loss: 0.67678 - acc: 0.7838 -- iter: 10/10\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.67662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 885 | loss: 0.67662 - acc: 0.7854 -- iter: 10/10\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.68248\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 886 | loss: 0.68248 - acc: 0.7268 -- iter: 10/10\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.68174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 887 | loss: 0.68174 - acc: 0.7342 -- iter: 10/10\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.68709\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 888 | loss: 0.68709 - acc: 0.6807 -- iter: 10/10\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.68589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 889 | loss: 0.68589 - acc: 0.6927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.68482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 890 | loss: 0.68482 - acc: 0.7034 -- iter: 10/10\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.68385\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 891 | loss: 0.68385 - acc: 0.7131 -- iter: 10/10\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.68298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 892 | loss: 0.68298 - acc: 0.7218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.68220\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 893 | loss: 0.68220 - acc: 0.7296 -- iter: 10/10\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.68150\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 894 | loss: 0.68150 - acc: 0.7366 -- iter: 10/10\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.68086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 895 | loss: 0.68086 - acc: 0.7430 -- iter: 10/10\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.68029\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 896 | loss: 0.68029 - acc: 0.7487 -- iter: 10/10\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.67978\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 897 | loss: 0.67978 - acc: 0.7538 -- iter: 10/10\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.67932\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 898 | loss: 0.67932 - acc: 0.7584 -- iter: 10/10\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.67890\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 899 | loss: 0.67890 - acc: 0.7626 -- iter: 10/10\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.68653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 900 | loss: 0.68653 - acc: 0.6863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.68539\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 901 | loss: 0.68539 - acc: 0.6977 -- iter: 10/10\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.69237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 902 | loss: 0.69237 - acc: 0.6279 -- iter: 10/10\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.69065\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 903 | loss: 0.69065 - acc: 0.6451 -- iter: 10/10\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.69610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 904 | loss: 0.69610 - acc: 0.5906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.69400\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 905 | loss: 0.69400 - acc: 0.6116 -- iter: 10/10\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 906 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.69672\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 907 | loss: 0.69672 - acc: 0.5844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.69457\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 908 | loss: 0.69457 - acc: 0.6059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.69263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 909 | loss: 0.69263 - acc: 0.6253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.69088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 910 | loss: 0.69088 - acc: 0.6428 -- iter: 10/10\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.68931\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 911 | loss: 0.68931 - acc: 0.6585 -- iter: 10/10\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.68789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 912 | loss: 0.68789 - acc: 0.6727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.68547\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 913 | loss: 0.68547 - acc: 0.6854 -- iter: 10/10\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.68547\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 914 | loss: 0.68547 - acc: 0.6969 -- iter: 10/10\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.68444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 915 | loss: 0.68444 - acc: 0.7072 -- iter: 10/10\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.68351\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 916 | loss: 0.68351 - acc: 0.7165 -- iter: 10/10\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.68268\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 917 | loss: 0.68268 - acc: 0.7248 -- iter: 10/10\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.68193\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 918 | loss: 0.68193 - acc: 0.7323 -- iter: 10/10\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.68125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 919 | loss: 0.68125 - acc: 0.7391 -- iter: 10/10\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.68064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 920 | loss: 0.68064 - acc: 0.7452 -- iter: 10/10\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.68009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 921 | loss: 0.68009 - acc: 0.7507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.67960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 922 | loss: 0.67960 - acc: 0.7556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.67916\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 923 | loss: 0.67916 - acc: 0.7600 -- iter: 10/10\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.68676\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 924 | loss: 0.68676 - acc: 0.6840 -- iter: 10/10\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.68560\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 925 | loss: 0.68560 - acc: 0.6956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.68455\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 926 | loss: 0.68455 - acc: 0.7061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.68361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 927 | loss: 0.68361 - acc: 0.7155 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 928 | loss: 0.68277 - acc: 0.7239 -- iter: 10/10\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.68201\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 929 | loss: 0.68201 - acc: 0.7315 -- iter: 10/10\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.68132\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 930 | loss: 0.68132 - acc: 0.7384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.68071\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 931 | loss: 0.68071 - acc: 0.7445 -- iter: 10/10\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.68015\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 932 | loss: 0.68015 - acc: 0.7501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.67965\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 933 | loss: 0.67965 - acc: 0.7551 -- iter: 10/10\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.67920\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 934 | loss: 0.67920 - acc: 0.7596 -- iter: 10/10\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.67880\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 935 | loss: 0.67880 - acc: 0.7636 -- iter: 10/10\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.67843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 936 | loss: 0.67843 - acc: 0.7672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.67811\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 937 | loss: 0.67811 - acc: 0.7705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.68581\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 938 | loss: 0.68581 - acc: 0.6935 -- iter: 10/10\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.68475\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 939 | loss: 0.68475 - acc: 0.7041 -- iter: 10/10\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.68379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 940 | loss: 0.68379 - acc: 0.7137 -- iter: 10/10\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.68293\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 941 | loss: 0.68293 - acc: 0.7223 -- iter: 10/10\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.68815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 942 | loss: 0.68815 - acc: 0.6701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.68685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 943 | loss: 0.68685 - acc: 0.6831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.68568\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 944 | loss: 0.68568 - acc: 0.6948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.68463\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 945 | loss: 0.68463 - acc: 0.7053 -- iter: 10/10\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.68968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 946 | loss: 0.68968 - acc: 0.6548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 947 | loss: 0.68823 - acc: 0.6693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.69492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 948 | loss: 0.69492 - acc: 0.6024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.69295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 949 | loss: 0.69295 - acc: 0.6221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.69717\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 950 | loss: 0.69717 - acc: 0.5799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.69497\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 951 | loss: 0.69497 - acc: 0.6019 -- iter: 10/10\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.69999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 952 | loss: 0.69999 - acc: 0.5517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.69750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 953 | loss: 0.69750 - acc: 0.5766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.70027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 954 | loss: 0.70027 - acc: 0.5489 -- iter: 10/10\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.69776\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 955 | loss: 0.69776 - acc: 0.5740 -- iter: 10/10\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.69550\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 956 | loss: 0.69550 - acc: 0.5966 -- iter: 10/10\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.69346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 957 | loss: 0.69346 - acc: 0.6170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.69163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 958 | loss: 0.69163 - acc: 0.6353 -- iter: 10/10\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.68999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 959 | loss: 0.68999 - acc: 0.6517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.68850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 960 | loss: 0.68850 - acc: 0.6666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.68717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 961 | loss: 0.68717 - acc: 0.6799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.68597\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 962 | loss: 0.68597 - acc: 0.6919 -- iter: 10/10\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.68489\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 963 | loss: 0.68489 - acc: 0.7027 -- iter: 10/10\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.68391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 964 | loss: 0.68391 - acc: 0.7124 -- iter: 10/10\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.68304\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 965 | loss: 0.68304 - acc: 0.7212 -- iter: 10/10\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.68225\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 966 | loss: 0.68225 - acc: 0.7291 -- iter: 10/10\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.68154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 967 | loss: 0.68154 - acc: 0.7362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.68090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 968 | loss: 0.68090 - acc: 0.7426 -- iter: 10/10\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.68033\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 969 | loss: 0.68033 - acc: 0.7483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.67981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 970 | loss: 0.67981 - acc: 0.7535 -- iter: 10/10\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.67935\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 971 | loss: 0.67935 - acc: 0.7581 -- iter: 10/10\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.68693\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 972 | loss: 0.68693 - acc: 0.6823 -- iter: 10/10\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.68575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 973 | loss: 0.68575 - acc: 0.6941 -- iter: 10/10\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.69269\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 974 | loss: 0.69269 - acc: 0.6247 -- iter: 10/10\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.69094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 975 | loss: 0.69094 - acc: 0.6422 -- iter: 10/10\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.68936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 976 | loss: 0.68936 - acc: 0.6580 -- iter: 10/10\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.68794\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 977 | loss: 0.68794 - acc: 0.6722 -- iter: 10/10\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.69366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 978 | loss: 0.69366 - acc: 0.6150 -- iter: 10/10\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.69181\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 979 | loss: 0.69181 - acc: 0.6335 -- iter: 10/10\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.69015\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 980 | loss: 0.69015 - acc: 0.6501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.68865\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 981 | loss: 0.68865 - acc: 0.6651 -- iter: 10/10\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.68730\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 982 | loss: 0.68730 - acc: 0.6786 -- iter: 10/10\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.68609\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 983 | loss: 0.68609 - acc: 0.6907 -- iter: 10/10\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.69099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 984 | loss: 0.69099 - acc: 0.6417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.68941\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 985 | loss: 0.68941 - acc: 0.6575 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.68798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 986 | loss: 0.68798 - acc: 0.6717 -- iter: 10/10\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.68670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 987 | loss: 0.68670 - acc: 0.6846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.68555\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 988 | loss: 0.68555 - acc: 0.6961 -- iter: 10/10\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.68451\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 989 | loss: 0.68451 - acc: 0.7065 -- iter: 10/10\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.69157\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 990 | loss: 0.69157 - acc: 0.6359 -- iter: 10/10\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.68993\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 991 | loss: 0.68993 - acc: 0.6523 -- iter: 10/10\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.69645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 992 | loss: 0.69645 - acc: 0.5870 -- iter: 10/10\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.69433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 993 | loss: 0.69433 - acc: 0.6083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.70041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 994 | loss: 0.70041 - acc: 0.5475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.69788\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 995 | loss: 0.69788 - acc: 0.5728 -- iter: 10/10\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.69561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 996 | loss: 0.69561 - acc: 0.5955 -- iter: 10/10\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.69357\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 997 | loss: 0.69357 - acc: 0.6159 -- iter: 10/10\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.69973\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 998 | loss: 0.69973 - acc: 0.5543 -- iter: 10/10\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.69727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 999 | loss: 0.69727 - acc: 0.5789 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.69506\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1000 | loss: 0.69506 - acc: 0.6010 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.69307\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1001 | loss: 0.69307 - acc: 0.6209 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.69128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1002 | loss: 0.69128 - acc: 0.6388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.68967\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1003 | loss: 0.68967 - acc: 0.6549 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.68821\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1004 | loss: 0.68821 - acc: 0.6694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.68691\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1005 | loss: 0.68691 - acc: 0.6825 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.68573\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1006 | loss: 0.68573 - acc: 0.6943 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.68468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1007 | loss: 0.68468 - acc: 0.7048 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.68373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1008 | loss: 0.68373 - acc: 0.7143 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.68287\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1009 | loss: 0.68287 - acc: 0.7229 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.68210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1010 | loss: 0.68210 - acc: 0.7306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.68140\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1011 | loss: 0.68140 - acc: 0.7376 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.68078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1012 | loss: 0.68078 - acc: 0.7438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.68022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1013 | loss: 0.68022 - acc: 0.7494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.67971\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1014 | loss: 0.67971 - acc: 0.7545 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.67926\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1015 | loss: 0.67926 - acc: 0.7590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.67885\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1016 | loss: 0.67885 - acc: 0.7631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.67848\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1017 | loss: 0.67848 - acc: 0.7668 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.67815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1018 | loss: 0.67815 - acc: 0.7701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.67785\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1019 | loss: 0.67785 - acc: 0.7731 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.67758\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1020 | loss: 0.67758 - acc: 0.7758 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.67734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1021 | loss: 0.67734 - acc: 0.7782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.67712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1022 | loss: 0.67712 - acc: 0.7804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.67692\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1023 | loss: 0.67692 - acc: 0.7824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.67675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1024 | loss: 0.67675 - acc: 0.7841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.67659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1025 | loss: 0.67659 - acc: 0.7857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.67645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1026 | loss: 0.67645 - acc: 0.7871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.67632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1027 | loss: 0.67632 - acc: 0.7884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.67620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1028 | loss: 0.67620 - acc: 0.7896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.67610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1029 | loss: 0.67610 - acc: 0.7906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.67600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1030 | loss: 0.67600 - acc: 0.7916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.67592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1031 | loss: 0.67592 - acc: 0.7924 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.67584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1032 | loss: 0.67584 - acc: 0.7932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.67577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1033 | loss: 0.67577 - acc: 0.7939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.68271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1034 | loss: 0.68271 - acc: 0.7245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1035 | loss: 0.68196 - acc: 0.7320 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.68928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1036 | loss: 0.68928 - acc: 0.6588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.68787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1037 | loss: 0.68787 - acc: 0.6729 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.69360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1038 | loss: 0.69360 - acc: 0.6156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.69175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1039 | loss: 0.69175 - acc: 0.6341 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.69009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1040 | loss: 0.69009 - acc: 0.6507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.68860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1041 | loss: 0.68860 - acc: 0.6656 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.68726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1042 | loss: 0.68726 - acc: 0.6790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.68605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1043 | loss: 0.68605 - acc: 0.6911 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.68496\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1044 | loss: 0.68496 - acc: 0.7020 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.68398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1045 | loss: 0.68398 - acc: 0.7118 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.68310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1046 | loss: 0.68310 - acc: 0.7206 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.68230\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1047 | loss: 0.68230 - acc: 0.7286 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.68159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1048 | loss: 0.68159 - acc: 0.7357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.68094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1049 | loss: 0.68094 - acc: 0.7421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.68737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1050 | loss: 0.68737 - acc: 0.6779 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.68615\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1051 | loss: 0.68615 - acc: 0.6901 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.68505\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1052 | loss: 0.68505 - acc: 0.7011 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.68406\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1053 | loss: 0.68406 - acc: 0.7110 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.68317\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1054 | loss: 0.68317 - acc: 0.7199 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.68237\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1055 | loss: 0.68237 - acc: 0.7279 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.68165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1056 | loss: 0.68165 - acc: 0.7351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.68100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1057 | loss: 0.68100 - acc: 0.7416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.68041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1058 | loss: 0.68041 - acc: 0.7475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.67989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1059 | loss: 0.67989 - acc: 0.7527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.68542\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1060 | loss: 0.68542 - acc: 0.6974 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.68439\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1061 | loss: 0.68439 - acc: 0.7077 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.68347\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1062 | loss: 0.68347 - acc: 0.7169 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.68264\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1063 | loss: 0.68264 - acc: 0.7252 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.68889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1064 | loss: 0.68889 - acc: 0.6627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.68752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1065 | loss: 0.68752 - acc: 0.6764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.69128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1066 | loss: 0.69128 - acc: 0.6388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.68967\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1067 | loss: 0.68967 - acc: 0.6549 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.69522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1068 | loss: 0.69522 - acc: 0.5994 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.69321\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1069 | loss: 0.69321 - acc: 0.6195 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.69841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1070 | loss: 0.69841 - acc: 0.5675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.69608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1071 | loss: 0.69608 - acc: 0.5908 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.69399\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1072 | loss: 0.69399 - acc: 0.6117 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.69211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1073 | loss: 0.69211 - acc: 0.6305 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.69841\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1074 | loss: 0.69841 - acc: 0.5675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.69609\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1075 | loss: 0.69609 - acc: 0.5907 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.70099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1076 | loss: 0.70099 - acc: 0.5417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.69841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1077 | loss: 0.69841 - acc: 0.5675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.70308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1078 | loss: 0.70308 - acc: 0.5207 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.70029\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1079 | loss: 0.70029 - acc: 0.5487 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.69778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1080 | loss: 0.69778 - acc: 0.5738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.69552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1081 | loss: 0.69552 - acc: 0.5964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.69348\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1082 | loss: 0.69348 - acc: 0.6168 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.69165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1083 | loss: 0.69165 - acc: 0.6351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.69600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1084 | loss: 0.69600 - acc: 0.5916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.69392\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1085 | loss: 0.69392 - acc: 0.6124 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.69204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1086 | loss: 0.69204 - acc: 0.6312 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.69035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1087 | loss: 0.69035 - acc: 0.6481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.68883\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1088 | loss: 0.68883 - acc: 0.6633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.68747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1089 | loss: 0.68747 - acc: 0.6769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.68623\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1090 | loss: 0.68623 - acc: 0.6892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.68513\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1091 | loss: 0.68513 - acc: 0.7003 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.68413\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1092 | loss: 0.68413 - acc: 0.7103 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.68323\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1093 | loss: 0.68323 - acc: 0.7193 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.68243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1094 | loss: 0.68243 - acc: 0.7273 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.68170\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1095 | loss: 0.68170 - acc: 0.7346 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.68905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1096 | loss: 0.68905 - acc: 0.6611 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.68766\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1097 | loss: 0.68766 - acc: 0.6750 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.68641\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1098 | loss: 0.68641 - acc: 0.6875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.68528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1099 | loss: 0.68528 - acc: 0.6988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.68427\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1100 | loss: 0.68427 - acc: 0.7089 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.68336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1101 | loss: 0.68336 - acc: 0.7180 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.68254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1102 | loss: 0.68254 - acc: 0.7262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.68180\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1103 | loss: 0.68180 - acc: 0.7336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.68914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1104 | loss: 0.68914 - acc: 0.6602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.68774\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1105 | loss: 0.68774 - acc: 0.6742 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.69448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1106 | loss: 0.69448 - acc: 0.6068 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.69255\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1107 | loss: 0.69255 - acc: 0.6261 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.69081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1108 | loss: 0.69081 - acc: 0.6435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.68924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1109 | loss: 0.68924 - acc: 0.6591 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.69584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1110 | loss: 0.69584 - acc: 0.5932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.69377\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1111 | loss: 0.69377 - acc: 0.6139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.69191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1112 | loss: 0.69191 - acc: 0.6325 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.69023\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1113 | loss: 0.69023 - acc: 0.6493 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.69573\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1114 | loss: 0.69573 - acc: 0.5943 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.69367\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1115 | loss: 0.69367 - acc: 0.6149 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.69682\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1116 | loss: 0.69682 - acc: 0.5834 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.69465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1117 | loss: 0.69465 - acc: 0.6051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.69270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1118 | loss: 0.69270 - acc: 0.6246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1119 | loss: 0.69095 - acc: 0.6421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.69637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1120 | loss: 0.69637 - acc: 0.5879 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.69425\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1121 | loss: 0.69425 - acc: 0.6091 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.69234\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1122 | loss: 0.69234 - acc: 0.6282 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.69062\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1123 | loss: 0.69062 - acc: 0.6454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.69608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1124 | loss: 0.69608 - acc: 0.5908 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.69398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1125 | loss: 0.69398 - acc: 0.6118 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.69210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1126 | loss: 0.69210 - acc: 0.6306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.69041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1127 | loss: 0.69041 - acc: 0.6475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.68888\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1128 | loss: 0.68888 - acc: 0.6628 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.68751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1129 | loss: 0.68751 - acc: 0.6765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.68627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1130 | loss: 0.68627 - acc: 0.6888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.68516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1131 | loss: 0.68516 - acc: 0.7000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.69116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1132 | loss: 0.69116 - acc: 0.6400 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.68956\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1133 | loss: 0.68956 - acc: 0.6560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.68812\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1134 | loss: 0.68812 - acc: 0.6704 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.68683\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1135 | loss: 0.68683 - acc: 0.6833 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.68566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1136 | loss: 0.68566 - acc: 0.6950 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.68461\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1137 | loss: 0.68461 - acc: 0.7055 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.68366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1138 | loss: 0.68366 - acc: 0.7149 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.68281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1139 | loss: 0.68281 - acc: 0.7235 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.68205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1140 | loss: 0.68205 - acc: 0.7311 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.68136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1141 | loss: 0.68136 - acc: 0.7380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.68074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1142 | loss: 0.68074 - acc: 0.7442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.68018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1143 | loss: 0.68018 - acc: 0.7498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.67968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1144 | loss: 0.67968 - acc: 0.7548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.67923\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1145 | loss: 0.67923 - acc: 0.7593 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.67882\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1146 | loss: 0.67882 - acc: 0.7634 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.67845\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1147 | loss: 0.67845 - acc: 0.7670 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.67812\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1148 | loss: 0.67812 - acc: 0.7703 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.67783\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1149 | loss: 0.67783 - acc: 0.7733 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.68356\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1150 | loss: 0.68356 - acc: 0.7160 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.68272\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1151 | loss: 0.68272 - acc: 0.7244 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1152 | loss: 0.68196 - acc: 0.7319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1153 | loss: 0.68128 - acc: 0.7387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.68067\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1154 | loss: 0.68067 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.68012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1155 | loss: 0.68012 - acc: 0.7504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.67962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1156 | loss: 0.67962 - acc: 0.7553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.67918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1157 | loss: 0.67918 - acc: 0.7598 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.67878\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1158 | loss: 0.67878 - acc: 0.7638 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.67841\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1159 | loss: 0.67841 - acc: 0.7674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.68609\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1160 | loss: 0.68609 - acc: 0.6907 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.68500\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1161 | loss: 0.68500 - acc: 0.7016 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.69001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1162 | loss: 0.69001 - acc: 0.6515 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.68853\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1163 | loss: 0.68853 - acc: 0.6663 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.69519\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1164 | loss: 0.69519 - acc: 0.5997 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.69319\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1165 | loss: 0.69319 - acc: 0.6197 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.69938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1166 | loss: 0.69938 - acc: 0.5577 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.69696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1167 | loss: 0.69696 - acc: 0.5820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.69478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1168 | loss: 0.69478 - acc: 0.6038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.69282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1169 | loss: 0.69282 - acc: 0.6234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.69805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1170 | loss: 0.69805 - acc: 0.5711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.69576\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1171 | loss: 0.69576 - acc: 0.5940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.69370\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1172 | loss: 0.69370 - acc: 0.6146 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.69185\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1173 | loss: 0.69185 - acc: 0.6331 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.69018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1174 | loss: 0.69018 - acc: 0.6498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.68868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1175 | loss: 0.68868 - acc: 0.6648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.68733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1176 | loss: 0.68733 - acc: 0.6783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.68611\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1177 | loss: 0.68611 - acc: 0.6905 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.68501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1178 | loss: 0.68501 - acc: 0.7014 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.68403\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1179 | loss: 0.68403 - acc: 0.7113 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.68314\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1180 | loss: 0.68314 - acc: 0.7202 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.68234\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1181 | loss: 0.68234 - acc: 0.7282 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.68163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1182 | loss: 0.68163 - acc: 0.7353 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.68098\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1183 | loss: 0.68098 - acc: 0.7418 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.68040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1184 | loss: 0.68040 - acc: 0.7476 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.67987\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1185 | loss: 0.67987 - acc: 0.7529 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.67940\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1186 | loss: 0.67940 - acc: 0.7576 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.67898\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1187 | loss: 0.67898 - acc: 0.7618 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.68560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1188 | loss: 0.68560 - acc: 0.6956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.68455\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1189 | loss: 0.68455 - acc: 0.7061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.68361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1190 | loss: 0.68361 - acc: 0.7155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1191 | loss: 0.68277 - acc: 0.7239 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.68801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1192 | loss: 0.68801 - acc: 0.6715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.68672\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1193 | loss: 0.68672 - acc: 0.6844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.68557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1194 | loss: 0.68557 - acc: 0.6959 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.68452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1195 | loss: 0.68452 - acc: 0.7063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.68359\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1196 | loss: 0.68359 - acc: 0.7157 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.68275\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1197 | loss: 0.68275 - acc: 0.7241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.68199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1198 | loss: 0.68199 - acc: 0.7317 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.68130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1199 | loss: 0.68130 - acc: 0.7386 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m0.68669\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1200 | loss: 0.68669 - acc: 0.6847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m0.68554\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1201 | loss: 0.68554 - acc: 0.6962 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.69150\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1202 | loss: 0.69150 - acc: 0.6366 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.68986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1203 | loss: 0.68986 - acc: 0.6529 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.68839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1204 | loss: 0.68839 - acc: 0.6676 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.68707\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1205 | loss: 0.68707 - acc: 0.6809 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.68588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1206 | loss: 0.68588 - acc: 0.6928 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.68481\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1207 | loss: 0.68481 - acc: 0.7035 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.68384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1208 | loss: 0.68384 - acc: 0.7132 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.68297\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1209 | loss: 0.68297 - acc: 0.7218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.68219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1210 | loss: 0.68219 - acc: 0.7297 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.68149\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1211 | loss: 0.68149 - acc: 0.7367 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.68086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1212 | loss: 0.68086 - acc: 0.7430 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.68029\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1213 | loss: 0.68029 - acc: 0.7487 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.67977\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1214 | loss: 0.67977 - acc: 0.7539 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.67931\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1215 | loss: 0.67931 - acc: 0.7585 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.68590\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1216 | loss: 0.68590 - acc: 0.6926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.68482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1217 | loss: 0.68482 - acc: 0.7034 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.68386\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1218 | loss: 0.68386 - acc: 0.7130 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.68299\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1219 | loss: 0.68299 - acc: 0.7217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.68220\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1220 | loss: 0.68220 - acc: 0.7295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.68150\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1221 | loss: 0.68150 - acc: 0.7366 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.68087\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1222 | loss: 0.68087 - acc: 0.7429 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.68030\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1223 | loss: 0.68030 - acc: 0.7486 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.68778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1224 | loss: 0.68778 - acc: 0.6738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.68652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1225 | loss: 0.68652 - acc: 0.6864 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.68538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1226 | loss: 0.68538 - acc: 0.6978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.68436\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1227 | loss: 0.68436 - acc: 0.7080 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.68344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1228 | loss: 0.68344 - acc: 0.7172 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.68261\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1229 | loss: 0.68261 - acc: 0.7255 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.68187\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1230 | loss: 0.68187 - acc: 0.7329 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.68120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1231 | loss: 0.68120 - acc: 0.7396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.68859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1232 | loss: 0.68859 - acc: 0.6657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.68725\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1233 | loss: 0.68725 - acc: 0.6791 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.68604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1234 | loss: 0.68604 - acc: 0.6912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.68495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1235 | loss: 0.68495 - acc: 0.7021 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.68397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1236 | loss: 0.68397 - acc: 0.7119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.68309\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1237 | loss: 0.68309 - acc: 0.7207 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.68230\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1238 | loss: 0.68230 - acc: 0.7286 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.68158\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1239 | loss: 0.68158 - acc: 0.7357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.68894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1240 | loss: 0.68894 - acc: 0.6622 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.68756\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1241 | loss: 0.68756 - acc: 0.6760 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.68632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1242 | loss: 0.68632 - acc: 0.6884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.68521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1243 | loss: 0.68521 - acc: 0.6995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.68420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1244 | loss: 0.68420 - acc: 0.7096 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.68330\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1245 | loss: 0.68330 - acc: 0.7186 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.68248\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1246 | loss: 0.68248 - acc: 0.7268 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.68175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1247 | loss: 0.68175 - acc: 0.7341 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.68109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1248 | loss: 0.68109 - acc: 0.7407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.68050\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1249 | loss: 0.68050 - acc: 0.7466 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.68496\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1250 | loss: 0.68496 - acc: 0.7019 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.68398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1251 | loss: 0.68398 - acc: 0.7117 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.68310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1252 | loss: 0.68310 - acc: 0.7206 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.68231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1253 | loss: 0.68231 - acc: 0.7285 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.68159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1254 | loss: 0.68159 - acc: 0.7357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.68095\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1255 | loss: 0.68095 - acc: 0.7421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.68037\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1256 | loss: 0.68037 - acc: 0.7479 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.67985\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1257 | loss: 0.67985 - acc: 0.7531 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.67938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1258 | loss: 0.67938 - acc: 0.7578 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.67896\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1259 | loss: 0.67896 - acc: 0.7620 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.68558\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1260 | loss: 0.68558 - acc: 0.6958 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.68454\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1261 | loss: 0.68454 - acc: 0.7062 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.68360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1262 | loss: 0.68360 - acc: 0.7156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.68275\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1263 | loss: 0.68275 - acc: 0.7240 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.68200\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1264 | loss: 0.68200 - acc: 0.7316 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.68131\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1265 | loss: 0.68131 - acc: 0.7385 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.68770\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1266 | loss: 0.68770 - acc: 0.6746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.68644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1267 | loss: 0.68644 - acc: 0.6872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1268 | loss: 0.68531 - acc: 0.6984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.68430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1269 | loss: 0.68430 - acc: 0.7086 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.69038\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1270 | loss: 0.69038 - acc: 0.6477 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.68886\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1271 | loss: 0.68886 - acc: 0.6630 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.69549\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1272 | loss: 0.69549 - acc: 0.5967 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.69346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1273 | loss: 0.69346 - acc: 0.6170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.69163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1274 | loss: 0.69163 - acc: 0.6353 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.68998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1275 | loss: 0.68998 - acc: 0.6518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.68850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1276 | loss: 0.68850 - acc: 0.6666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.68717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1277 | loss: 0.68717 - acc: 0.6799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.69196\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1278 | loss: 0.69196 - acc: 0.6319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.69028\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1279 | loss: 0.69028 - acc: 0.6487 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.69577\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1280 | loss: 0.69577 - acc: 0.5939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.69371\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1281 | loss: 0.69371 - acc: 0.6145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.69886\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1282 | loss: 0.69886 - acc: 0.5630 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.69649\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1283 | loss: 0.69649 - acc: 0.5867 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.70235\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1284 | loss: 0.70235 - acc: 0.5281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.69963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1285 | loss: 0.69963 - acc: 0.5553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.69719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1286 | loss: 0.69719 - acc: 0.5797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.69498\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1287 | loss: 0.69498 - acc: 0.6018 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.69300\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1288 | loss: 0.69300 - acc: 0.6216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.69122\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1289 | loss: 0.69122 - acc: 0.6394 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.68961\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1290 | loss: 0.68961 - acc: 0.6555 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.68817\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1291 | loss: 0.68817 - acc: 0.6699 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.68687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1292 | loss: 0.68687 - acc: 0.6829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.68569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1293 | loss: 0.68569 - acc: 0.6946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.68464\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1294 | loss: 0.68464 - acc: 0.7052 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.68369\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1295 | loss: 0.68369 - acc: 0.7147 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.68284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1296 | loss: 0.68284 - acc: 0.7232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.68207\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1297 | loss: 0.68207 - acc: 0.7309 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.68138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1298 | loss: 0.68138 - acc: 0.7378 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.68076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1299 | loss: 0.68076 - acc: 0.7440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.68820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1300 | loss: 0.68820 - acc: 0.6696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.68689\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1301 | loss: 0.68689 - acc: 0.6826 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.69272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1302 | loss: 0.69272 - acc: 0.6244 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.69096\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1303 | loss: 0.69096 - acc: 0.6419 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.69738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1304 | loss: 0.69738 - acc: 0.5778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.69516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1305 | loss: 0.69516 - acc: 0.6000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.69316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1306 | loss: 0.69316 - acc: 0.6200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.69136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1307 | loss: 0.69136 - acc: 0.6380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.68974\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1308 | loss: 0.68974 - acc: 0.6542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.68828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1309 | loss: 0.68828 - acc: 0.6688 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.68697\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1310 | loss: 0.68697 - acc: 0.6819 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.68579\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1311 | loss: 0.68579 - acc: 0.6937 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.68473\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1312 | loss: 0.68473 - acc: 0.7043 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.68377\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1313 | loss: 0.68377 - acc: 0.7139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.68991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1314 | loss: 0.68991 - acc: 0.6525 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.68843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1315 | loss: 0.68843 - acc: 0.6673 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.69511\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1316 | loss: 0.69511 - acc: 0.6005 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.69311\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1317 | loss: 0.69311 - acc: 0.6384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.69132\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1318 | loss: 0.69132 - acc: 0.6384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.68970\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1319 | loss: 0.68970 - acc: 0.6546 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.69525\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1320 | loss: 0.69525 - acc: 0.5991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.69324\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1321 | loss: 0.69324 - acc: 0.6192 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.69143\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1322 | loss: 0.69143 - acc: 0.6373 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.68980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1323 | loss: 0.68980 - acc: 0.6536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.68834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1324 | loss: 0.68834 - acc: 0.6682 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.68702\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1325 | loss: 0.68702 - acc: 0.6814 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.69383\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1326 | loss: 0.69383 - acc: 0.6132 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.69197\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1327 | loss: 0.69197 - acc: 0.6319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.69029\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1328 | loss: 0.69029 - acc: 0.6487 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.68877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1329 | loss: 0.68877 - acc: 0.6639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.68741\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1330 | loss: 0.68741 - acc: 0.6775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.68619\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1331 | loss: 0.68619 - acc: 0.6897 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.69308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1332 | loss: 0.69308 - acc: 0.6208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.69129\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1333 | loss: 0.69129 - acc: 0.6387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.68968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1334 | loss: 0.68968 - acc: 0.6548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1335 | loss: 0.68823 - acc: 0.6693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.69492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1336 | loss: 0.69492 - acc: 0.6024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.69294\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1337 | loss: 0.69294 - acc: 0.6222 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.69116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1338 | loss: 0.69116 - acc: 0.6399 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.68956\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1339 | loss: 0.68956 - acc: 0.6559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.69612\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1340 | loss: 0.69612 - acc: 0.5904 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.69403\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1341 | loss: 0.69403 - acc: 0.6113 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.70014\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1342 | loss: 0.70014 - acc: 0.5502 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.69764\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1343 | loss: 0.69764 - acc: 0.5752 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.70339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1344 | loss: 0.70339 - acc: 0.5176 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.70057\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1345 | loss: 0.70057 - acc: 0.5459 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.69803\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1346 | loss: 0.69803 - acc: 0.5713 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.69574\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1347 | loss: 0.69574 - acc: 0.5942 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.69368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1348 | loss: 0.69368 - acc: 0.6148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.69183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1349 | loss: 0.69183 - acc: 0.6333 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.69716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1350 | loss: 0.69716 - acc: 0.5799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.69496\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1351 | loss: 0.69496 - acc: 0.6020 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.69298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1352 | loss: 0.69298 - acc: 0.6218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.69120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1353 | loss: 0.69120 - acc: 0.6396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.68960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1354 | loss: 0.68960 - acc: 0.6556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.68815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1355 | loss: 0.68815 - acc: 0.6701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.69385\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1356 | loss: 0.69385 - acc: 0.6131 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.69198\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1357 | loss: 0.69198 - acc: 0.6317 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.69830\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1358 | loss: 0.69830 - acc: 0.5686 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.69599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1359 | loss: 0.69599 - acc: 0.5917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.69390\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1360 | loss: 0.69390 - acc: 0.6125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.69203\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1361 | loss: 0.69203 - acc: 0.6313 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.69034\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1362 | loss: 0.69034 - acc: 0.6482 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.68882\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1363 | loss: 0.68882 - acc: 0.6633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.68746\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1364 | loss: 0.68746 - acc: 0.6770 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.68623\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1365 | loss: 0.68623 - acc: 0.6893 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.68512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1366 | loss: 0.68512 - acc: 0.7004 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.68412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1367 | loss: 0.68412 - acc: 0.7103 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.69123\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1368 | loss: 0.69123 - acc: 0.6393 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.68962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1369 | loss: 0.68962 - acc: 0.6554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.68818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1370 | loss: 0.68818 - acc: 0.6698 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.68687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1371 | loss: 0.68687 - acc: 0.6829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.68570\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1372 | loss: 0.68570 - acc: 0.6946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.68465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1373 | loss: 0.68465 - acc: 0.7051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.68370\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1374 | loss: 0.68370 - acc: 0.7146 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.68284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1375 | loss: 0.68284 - acc: 0.7231 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.68208\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1376 | loss: 0.68208 - acc: 0.7308 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.68138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1377 | loss: 0.68138 - acc: 0.7377 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.68076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1378 | loss: 0.68076 - acc: 0.7440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.68020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1379 | loss: 0.68020 - acc: 0.7496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.67970\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1380 | loss: 0.67970 - acc: 0.7546 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.67924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1381 | loss: 0.67924 - acc: 0.7592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.68484\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1382 | loss: 0.68484 - acc: 0.7032 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.68387\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1383 | loss: 0.68387 - acc: 0.7129 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.68300\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1384 | loss: 0.68300 - acc: 0.7216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.68221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1385 | loss: 0.68221 - acc: 0.7295 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.68851\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1386 | loss: 0.68851 - acc: 0.6665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.68717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1387 | loss: 0.68717 - acc: 0.6799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.68597\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1388 | loss: 0.68597 - acc: 0.6919 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.68489\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1389 | loss: 0.68489 - acc: 0.7027 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.68392\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1390 | loss: 0.68392 - acc: 0.7124 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.68304\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1391 | loss: 0.68304 - acc: 0.7212 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.68225\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1392 | loss: 0.68225 - acc: 0.7291 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.68154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1393 | loss: 0.68154 - acc: 0.7362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.68091\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1394 | loss: 0.68091 - acc: 0.7425 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.68033\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1395 | loss: 0.68033 - acc: 0.7483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.67981\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1396 | loss: 0.67981 - acc: 0.7535 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.67935\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1397 | loss: 0.67935 - acc: 0.7581 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.67893\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1398 | loss: 0.67893 - acc: 0.7623 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.67855\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1399 | loss: 0.67855 - acc: 0.7661 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.68521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1400 | loss: 0.68521 - acc: 0.6995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.68421\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1401 | loss: 0.68421 - acc: 0.7095 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.68330\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1402 | loss: 0.68330 - acc: 0.7186 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.68249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1403 | loss: 0.68249 - acc: 0.7267 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.68176\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1404 | loss: 0.68176 - acc: 0.7340 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.68110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1405 | loss: 0.68110 - acc: 0.7406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.68850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1406 | loss: 0.68850 - acc: 0.6666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.68717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1407 | loss: 0.68717 - acc: 0.6799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.68997\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1408 | loss: 0.68997 - acc: 0.6519 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.68849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1409 | loss: 0.68849 - acc: 0.6667 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.68715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1410 | loss: 0.68715 - acc: 0.6801 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.68595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1411 | loss: 0.68595 - acc: 0.6921 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.69187\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1412 | loss: 0.69187 - acc: 0.6328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.69020\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1413 | loss: 0.69020 - acc: 0.6496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.69670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1414 | loss: 0.69670 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.69454\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1415 | loss: 0.69454 - acc: 0.6061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.69861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1416 | loss: 0.69861 - acc: 0.5655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.69626\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1417 | loss: 0.69626 - acc: 0.5890 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.70015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1418 | loss: 0.70015 - acc: 0.5501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.69765\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1419 | loss: 0.69765 - acc: 0.5751 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.69540\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1420 | loss: 0.69540 - acc: 0.5976 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.69338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1421 | loss: 0.69338 - acc: 0.6178 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.69156\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1422 | loss: 0.69156 - acc: 0.6360 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.68992\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1423 | loss: 0.68992 - acc: 0.6524 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.68844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1424 | loss: 0.68844 - acc: 0.6672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.68711\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1425 | loss: 0.68711 - acc: 0.6805 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.68592\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1426 | loss: 0.68592 - acc: 0.6924 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.68484\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1427 | loss: 0.68484 - acc: 0.7032 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.68387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1428 | loss: 0.68387 - acc: 0.7129 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.68300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1429 | loss: 0.68300 - acc: 0.7216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.69022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1430 | loss: 0.69022 - acc: 0.6494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.68871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1431 | loss: 0.68871 - acc: 0.6645 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.69536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1432 | loss: 0.69536 - acc: 0.5980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.69334\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1433 | loss: 0.69334 - acc: 0.6182 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.69152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1434 | loss: 0.69152 - acc: 0.6364 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.68988\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1435 | loss: 0.68988 - acc: 0.6528 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.68841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1436 | loss: 0.68841 - acc: 0.6675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.68709\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1437 | loss: 0.68709 - acc: 0.6807 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.68589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1438 | loss: 0.68589 - acc: 0.6927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.68482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1439 | loss: 0.68482 - acc: 0.7034 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.69185\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1440 | loss: 0.69185 - acc: 0.6331 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.69018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1441 | loss: 0.69018 - acc: 0.6498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.68868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1442 | loss: 0.68868 - acc: 0.6648 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.68733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1443 | loss: 0.68733 - acc: 0.6783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.69411\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1444 | loss: 0.69411 - acc: 0.6105 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.69222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1445 | loss: 0.69222 - acc: 0.6294 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.69051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1446 | loss: 0.69051 - acc: 0.6465 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.68898\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1447 | loss: 0.68898 - acc: 0.6618 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.69459\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1448 | loss: 0.69459 - acc: 0.6056 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.69265\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1449 | loss: 0.69265 - acc: 0.6251 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.69090\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1450 | loss: 0.69090 - acc: 0.6583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.68933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1451 | loss: 0.68933 - acc: 0.6583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.68791\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1452 | loss: 0.68791 - acc: 0.6725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.68664\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1453 | loss: 0.68664 - acc: 0.6852 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.68549\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1454 | loss: 0.68549 - acc: 0.6967 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.68445\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1455 | loss: 0.68445 - acc: 0.7070 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.68953\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1456 | loss: 0.68953 - acc: 0.6563 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.68809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1457 | loss: 0.68809 - acc: 0.6707 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.69480\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1458 | loss: 0.69480 - acc: 0.6036 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.69283\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1459 | loss: 0.69283 - acc: 0.6233 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.69906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1460 | loss: 0.69906 - acc: 0.5609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.69667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1461 | loss: 0.69667 - acc: 0.5848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.69452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1462 | loss: 0.69452 - acc: 0.6064 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.69259\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1463 | loss: 0.69259 - acc: 0.6257 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.69084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1464 | loss: 0.69084 - acc: 0.6432 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.68928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1465 | loss: 0.68928 - acc: 0.6588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.68786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1466 | loss: 0.68786 - acc: 0.6730 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.68659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1467 | loss: 0.68659 - acc: 0.6857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.69245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1468 | loss: 0.69245 - acc: 0.6271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.69072\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1469 | loss: 0.69072 - acc: 0.6444 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.69516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1470 | loss: 0.69516 - acc: 0.5999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.69316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1471 | loss: 0.69316 - acc: 0.6200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.69836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1472 | loss: 0.69836 - acc: 0.5680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.69604\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1473 | loss: 0.69604 - acc: 0.5912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.69395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1474 | loss: 0.69395 - acc: 0.6120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.69208\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1475 | loss: 0.69208 - acc: 0.6308 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.69738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1476 | loss: 0.69738 - acc: 0.5778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.69516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1477 | loss: 0.69516 - acc: 0.6000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.69316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1478 | loss: 0.69316 - acc: 0.6200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.69136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1479 | loss: 0.69136 - acc: 0.6380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.68974\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1480 | loss: 0.68974 - acc: 0.6542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.68828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1481 | loss: 0.68828 - acc: 0.6688 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.68697\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1482 | loss: 0.68697 - acc: 0.6819 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.68579\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1483 | loss: 0.68579 - acc: 0.6937 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.68473\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1484 | loss: 0.68473 - acc: 0.7043 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.68377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1485 | loss: 0.68377 - acc: 0.7139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.68291\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1486 | loss: 0.68291 - acc: 0.7225 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.68213\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1487 | loss: 0.68213 - acc: 0.7303 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.68144\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1488 | loss: 0.68144 - acc: 0.7372 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.68081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1489 | loss: 0.68081 - acc: 0.7435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.68024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1490 | loss: 0.68024 - acc: 0.7492 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.67973\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1491 | loss: 0.67973 - acc: 0.7542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.67928\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1492 | loss: 0.67928 - acc: 0.7588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.67887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1493 | loss: 0.67887 - acc: 0.7629 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.68649\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1494 | loss: 0.68649 - acc: 0.6866 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.68536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1495 | loss: 0.68536 - acc: 0.6980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.68434\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1496 | loss: 0.68434 - acc: 0.7082 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.68342\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1497 | loss: 0.68342 - acc: 0.7174 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.68960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1498 | loss: 0.68960 - acc: 0.6556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.68815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1499 | loss: 0.68815 - acc: 0.6701 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.69485\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1500 | loss: 0.69485 - acc: 0.6031 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.69288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1501 | loss: 0.69288 - acc: 0.6228 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.69111\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1502 | loss: 0.69111 - acc: 0.6405 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.68952\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1503 | loss: 0.68952 - acc: 0.6564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.68808\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1504 | loss: 0.68808 - acc: 0.6708 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.68679\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1505 | loss: 0.68679 - acc: 0.6837 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.68563\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1506 | loss: 0.68563 - acc: 0.6953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.68458\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1507 | loss: 0.68458 - acc: 0.7058 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.68364\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1508 | loss: 0.68364 - acc: 0.7152 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.68279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1509 | loss: 0.68279 - acc: 0.7237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.68203\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1510 | loss: 0.68203 - acc: 0.7313 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.68134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1511 | loss: 0.68134 - acc: 0.7382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.68572\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1512 | loss: 0.68572 - acc: 0.6944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.68467\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1513 | loss: 0.68467 - acc: 0.7049 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.68371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1514 | loss: 0.68371 - acc: 0.7144 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.68286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1515 | loss: 0.68286 - acc: 0.7230 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.68209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1516 | loss: 0.68209 - acc: 0.7307 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.68140\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1517 | loss: 0.68140 - acc: 0.7376 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.68777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1518 | loss: 0.68777 - acc: 0.6739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.68651\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1519 | loss: 0.68651 - acc: 0.6865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.68538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1520 | loss: 0.68538 - acc: 0.6978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.68435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1521 | loss: 0.68435 - acc: 0.7080 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.68843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1522 | loss: 0.68843 - acc: 0.6672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.68711\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1523 | loss: 0.68711 - acc: 0.6805 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.68591\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1524 | loss: 0.68591 - acc: 0.6925 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.68484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1525 | loss: 0.68484 - acc: 0.7032 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.68987\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1526 | loss: 0.68987 - acc: 0.6529 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.68840\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1527 | loss: 0.68840 - acc: 0.6676 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.68707\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1528 | loss: 0.68707 - acc: 0.6808 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.68588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1529 | loss: 0.68588 - acc: 0.6928 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.68481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1530 | loss: 0.68481 - acc: 0.7035 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.68385\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1531 | loss: 0.68385 - acc: 0.7131 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.68998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1532 | loss: 0.68998 - acc: 0.6518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.68849\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1533 | loss: 0.68849 - acc: 0.6666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.69416\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1534 | loss: 0.69416 - acc: 0.6100 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.69226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1535 | loss: 0.69226 - acc: 0.6290 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.69055\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1536 | loss: 0.69055 - acc: 0.6461 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.68901\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1537 | loss: 0.68901 - acc: 0.6615 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.69363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1538 | loss: 0.69363 - acc: 0.6153 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.69178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1539 | loss: 0.69178 - acc: 0.6338 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.69812\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1540 | loss: 0.69812 - acc: 0.5704 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.69582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1541 | loss: 0.69582 - acc: 0.5934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.69376\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1542 | loss: 0.69376 - acc: 0.6140 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.69190\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1543 | loss: 0.69190 - acc: 0.6326 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.69822\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1544 | loss: 0.69822 - acc: 0.5694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.69592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1545 | loss: 0.69592 - acc: 0.5924 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.69384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1546 | loss: 0.69384 - acc: 0.6132 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.69197\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1547 | loss: 0.69197 - acc: 0.6319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.69729\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1548 | loss: 0.69729 - acc: 0.5787 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.69508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1549 | loss: 0.69508 - acc: 0.6008 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.69309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1550 | loss: 0.69309 - acc: 0.6207 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.69129\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1551 | loss: 0.69129 - acc: 0.6387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.68968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1552 | loss: 0.68968 - acc: 0.6548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1553 | loss: 0.68823 - acc: 0.6693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.69492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1554 | loss: 0.69492 - acc: 0.6024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.69294\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1555 | loss: 0.69294 - acc: 0.6221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.69117\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1556 | loss: 0.69117 - acc: 0.6399 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.68957\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1557 | loss: 0.68957 - acc: 0.6559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.68812\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1558 | loss: 0.68812 - acc: 0.6703 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.68683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1559 | loss: 0.68683 - acc: 0.6833 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.68566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1560 | loss: 0.68566 - acc: 0.6950 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.68461\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1561 | loss: 0.68461 - acc: 0.7055 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.68367\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1562 | loss: 0.68367 - acc: 0.7149 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.68282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1563 | loss: 0.68282 - acc: 0.7234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.68205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1564 | loss: 0.68205 - acc: 0.7311 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.68136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1565 | loss: 0.68136 - acc: 0.7380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.68074\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1566 | loss: 0.68074 - acc: 0.7442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.68018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1567 | loss: 0.68018 - acc: 0.7498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.67968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1568 | loss: 0.67968 - acc: 0.7548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.67923\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1569 | loss: 0.67923 - acc: 0.7593 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.68582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1570 | loss: 0.68582 - acc: 0.6934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.68475\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1571 | loss: 0.68475 - acc: 0.7040 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.68380\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1572 | loss: 0.68380 - acc: 0.7136 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.68293\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1573 | loss: 0.68293 - acc: 0.7223 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.68215\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1574 | loss: 0.68215 - acc: 0.7300 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.68145\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1575 | loss: 0.68145 - acc: 0.7370 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.68083\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1576 | loss: 0.68083 - acc: 0.7433 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.68026\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1577 | loss: 0.68026 - acc: 0.7490 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.68675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1578 | loss: 0.68675 - acc: 0.6841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.68559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1579 | loss: 0.68559 - acc: 0.6957 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.69255\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1580 | loss: 0.69255 - acc: 0.6261 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.69081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1581 | loss: 0.69081 - acc: 0.6435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.68924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1582 | loss: 0.68924 - acc: 0.6592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.68783\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1583 | loss: 0.68783 - acc: 0.6732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.69357\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1584 | loss: 0.69357 - acc: 0.6159 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.69173\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1585 | loss: 0.69173 - acc: 0.6343 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.69007\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1586 | loss: 0.69007 - acc: 0.6509 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.68858\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1587 | loss: 0.68858 - acc: 0.6658 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.68724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1588 | loss: 0.68724 - acc: 0.6792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.68603\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1589 | loss: 0.68603 - acc: 0.6913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.68494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1590 | loss: 0.68494 - acc: 0.7022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.68396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1591 | loss: 0.68396 - acc: 0.7120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.68308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1592 | loss: 0.68308 - acc: 0.7208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.68229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1593 | loss: 0.68229 - acc: 0.7287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.68158\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1594 | loss: 0.68158 - acc: 0.7358 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.68094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1595 | loss: 0.68094 - acc: 0.7422 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.68736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1596 | loss: 0.68736 - acc: 0.6780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.68614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1597 | loss: 0.68614 - acc: 0.6902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.68504\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1598 | loss: 0.68504 - acc: 0.7012 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.68405\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1599 | loss: 0.68405 - acc: 0.7111 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.68316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1600 | loss: 0.68316 - acc: 0.7200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1601 | loss: 0.68236 - acc: 0.7280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1602 | loss: 0.68164 - acc: 0.7352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.68099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1603 | loss: 0.68099 - acc: 0.7417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.68741\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1604 | loss: 0.68741 - acc: 0.6775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.68619\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1605 | loss: 0.68619 - acc: 0.6897 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.68508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1606 | loss: 0.68508 - acc: 0.7008 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.68409\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1607 | loss: 0.68409 - acc: 0.7107 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.68320\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1608 | loss: 0.68320 - acc: 0.7196 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.68239\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1609 | loss: 0.68239 - acc: 0.7277 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.68167\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1610 | loss: 0.68167 - acc: 0.7349 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.68102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1611 | loss: 0.68102 - acc: 0.7414 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.68443\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1612 | loss: 0.68443 - acc: 0.7073 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.68351\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1613 | loss: 0.68351 - acc: 0.7165 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.68267\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1614 | loss: 0.68267 - acc: 0.7249 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.68192\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1615 | loss: 0.68192 - acc: 0.7324 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.68124\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1616 | loss: 0.68124 - acc: 0.7392 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.68064\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1617 | loss: 0.68064 - acc: 0.7452 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.68009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1618 | loss: 0.68009 - acc: 0.7507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.67959\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1619 | loss: 0.67959 - acc: 0.7556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.67915\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1620 | loss: 0.67915 - acc: 0.7601 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.67875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1621 | loss: 0.67875 - acc: 0.7641 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.67839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1622 | loss: 0.67839 - acc: 0.7677 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.67807\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1623 | loss: 0.67807 - acc: 0.7709 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.68478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1624 | loss: 0.68478 - acc: 0.7038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.68382\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1625 | loss: 0.68382 - acc: 0.7134 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.68295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1626 | loss: 0.68295 - acc: 0.7221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.68217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1627 | loss: 0.68217 - acc: 0.7299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.68947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1628 | loss: 0.68947 - acc: 0.6569 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.68804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1629 | loss: 0.68804 - acc: 0.6712 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.68675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1630 | loss: 0.68675 - acc: 0.6841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.68559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1631 | loss: 0.68559 - acc: 0.6957 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.68455\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1632 | loss: 0.68455 - acc: 0.7061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.68361\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1633 | loss: 0.68361 - acc: 0.7155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.69076\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1634 | loss: 0.69076 - acc: 0.6439 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.68920\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1635 | loss: 0.68920 - acc: 0.6596 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.68780\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1636 | loss: 0.68780 - acc: 0.6736 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.68654\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1637 | loss: 0.68654 - acc: 0.6862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.68540\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1638 | loss: 0.68540 - acc: 0.6976 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.68437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1639 | loss: 0.68437 - acc: 0.7079 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.68345\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1640 | loss: 0.68345 - acc: 0.7171 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.68262\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1641 | loss: 0.68262 - acc: 0.7254 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.68188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1642 | loss: 0.68188 - acc: 0.7328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.68120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1643 | loss: 0.68120 - acc: 0.7395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.68760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1644 | loss: 0.68760 - acc: 0.6756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.68636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1645 | loss: 0.68636 - acc: 0.6880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.68524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1646 | loss: 0.68524 - acc: 0.6992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.68423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1647 | loss: 0.68423 - acc: 0.7093 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.68332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1648 | loss: 0.68332 - acc: 0.7184 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.68251\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1649 | loss: 0.68251 - acc: 0.7265 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.68177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1650 | loss: 0.68177 - acc: 0.7339 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.68111\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1651 | loss: 0.68111 - acc: 0.7405 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.68051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1652 | loss: 0.68051 - acc: 0.7464 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.67998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1653 | loss: 0.67998 - acc: 0.7518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.67950\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1654 | loss: 0.67950 - acc: 0.7566 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.67906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1655 | loss: 0.67906 - acc: 0.7610 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.68567\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1656 | loss: 0.68567 - acc: 0.6949 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.68462\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1657 | loss: 0.68462 - acc: 0.7054 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.68368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1658 | loss: 0.68368 - acc: 0.7148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.68282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1659 | loss: 0.68282 - acc: 0.7234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.68906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1660 | loss: 0.68906 - acc: 0.6610 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.68767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1661 | loss: 0.68767 - acc: 0.6749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.69442\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1662 | loss: 0.69442 - acc: 0.6074 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.69249\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1663 | loss: 0.69249 - acc: 0.6267 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.69076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1664 | loss: 0.69076 - acc: 0.6440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.68920\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1665 | loss: 0.68920 - acc: 0.6596 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.68779\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1666 | loss: 0.68779 - acc: 0.6737 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.68653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1667 | loss: 0.68653 - acc: 0.6863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.68539\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1668 | loss: 0.68539 - acc: 0.6977 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.68437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1669 | loss: 0.68437 - acc: 0.7079 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.69145\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1670 | loss: 0.69145 - acc: 0.6371 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.68982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1671 | loss: 0.68982 - acc: 0.6534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.68835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1672 | loss: 0.68835 - acc: 0.6681 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.68703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1673 | loss: 0.68703 - acc: 0.6812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.68585\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1674 | loss: 0.68585 - acc: 0.6931 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.68478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1675 | loss: 0.68478 - acc: 0.7038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.68382\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1676 | loss: 0.68382 - acc: 0.7134 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.68295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1677 | loss: 0.68295 - acc: 0.7221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.68217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1678 | loss: 0.68217 - acc: 0.7299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.68147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1679 | loss: 0.68147 - acc: 0.7369 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.68484\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1680 | loss: 0.68484 - acc: 0.7032 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.68387\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1681 | loss: 0.68387 - acc: 0.7129 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.68300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1682 | loss: 0.68300 - acc: 0.7216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.68222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1683 | loss: 0.68222 - acc: 0.7294 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.68151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1684 | loss: 0.68151 - acc: 0.7365 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.68087\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1685 | loss: 0.68087 - acc: 0.7428 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.68030\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1686 | loss: 0.68030 - acc: 0.7486 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.67979\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1687 | loss: 0.67979 - acc: 0.7537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.67933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1688 | loss: 0.67933 - acc: 0.7583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.67891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1689 | loss: 0.67891 - acc: 0.7625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.67853\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1690 | loss: 0.67853 - acc: 0.7662 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.67820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1691 | loss: 0.67820 - acc: 0.7696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.67789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1692 | loss: 0.67789 - acc: 0.7727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.67762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1693 | loss: 0.67762 - acc: 0.7754 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.67737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1694 | loss: 0.67737 - acc: 0.7779 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.67715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1695 | loss: 0.67715 - acc: 0.7801 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.67695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1696 | loss: 0.67695 - acc: 0.7821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.67677\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1697 | loss: 0.67677 - acc: 0.7839 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.68461\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1698 | loss: 0.68461 - acc: 0.7055 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.68367\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1699 | loss: 0.68367 - acc: 0.7149 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.68282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1700 | loss: 0.68282 - acc: 0.7234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.68205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1701 | loss: 0.68205 - acc: 0.7311 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.68136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1702 | loss: 0.68136 - acc: 0.7380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.68074\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1703 | loss: 0.68074 - acc: 0.7442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.68018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1704 | loss: 0.68018 - acc: 0.7498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.67968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1705 | loss: 0.67968 - acc: 0.7548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.68723\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1706 | loss: 0.68723 - acc: 0.6793 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.68602\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1707 | loss: 0.68602 - acc: 0.6914 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.68494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1708 | loss: 0.68494 - acc: 0.7022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.68396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1709 | loss: 0.68396 - acc: 0.7120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.68308\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1710 | loss: 0.68308 - acc: 0.7208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.68229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1711 | loss: 0.68229 - acc: 0.7287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.68857\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1712 | loss: 0.68857 - acc: 0.6659 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.68723\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1713 | loss: 0.68723 - acc: 0.6793 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.68602\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1714 | loss: 0.68602 - acc: 0.6913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.68494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1715 | loss: 0.68494 - acc: 0.7022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.68396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1716 | loss: 0.68396 - acc: 0.7120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.68308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1717 | loss: 0.68308 - acc: 0.7208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.68929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1718 | loss: 0.68929 - acc: 0.6587 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.68787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1719 | loss: 0.68787 - acc: 0.6728 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.69360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1720 | loss: 0.69360 - acc: 0.6156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.69176\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1721 | loss: 0.69176 - acc: 0.6340 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.69010\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1722 | loss: 0.69010 - acc: 0.6506 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.68860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1723 | loss: 0.68860 - acc: 0.6655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.68726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1724 | loss: 0.68726 - acc: 0.6790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.68605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1725 | loss: 0.68605 - acc: 0.6911 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.69096\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1726 | loss: 0.69096 - acc: 0.6420 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.68938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1727 | loss: 0.68938 - acc: 0.6578 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.69496\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1728 | loss: 0.69496 - acc: 0.6020 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.69298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1729 | loss: 0.69298 - acc: 0.6218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.69120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1730 | loss: 0.69120 - acc: 0.6396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.68959\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1731 | loss: 0.68959 - acc: 0.6557 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.68815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1732 | loss: 0.68815 - acc: 0.6701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.68685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1733 | loss: 0.68685 - acc: 0.6831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.68568\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1734 | loss: 0.68568 - acc: 0.6948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.68463\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1735 | loss: 0.68463 - acc: 0.7053 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.68368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1736 | loss: 0.68368 - acc: 0.7148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.68283\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1737 | loss: 0.68283 - acc: 0.7233 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.68206\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1738 | loss: 0.68206 - acc: 0.7310 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.68137\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1739 | loss: 0.68137 - acc: 0.7379 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.68075\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1740 | loss: 0.68075 - acc: 0.7441 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.68019\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1741 | loss: 0.68019 - acc: 0.7497 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.68769\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1742 | loss: 0.68769 - acc: 0.6747 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.68644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1743 | loss: 0.68644 - acc: 0.6872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1744 | loss: 0.68531 - acc: 0.6985 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.68429\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1745 | loss: 0.68429 - acc: 0.7087 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.68338\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1746 | loss: 0.68338 - acc: 0.7178 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.68256\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1747 | loss: 0.68256 - acc: 0.7260 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.68182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1748 | loss: 0.68182 - acc: 0.7334 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.68115\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1749 | loss: 0.68115 - acc: 0.7401 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.68055\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1750 | loss: 0.68055 - acc: 0.7461 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.68001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1751 | loss: 0.68001 - acc: 0.7515 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.68553\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1752 | loss: 0.68553 - acc: 0.6963 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.68449\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1753 | loss: 0.68449 - acc: 0.7067 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.68356\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1754 | loss: 0.68356 - acc: 0.7160 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.68272\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1755 | loss: 0.68272 - acc: 0.7244 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1756 | loss: 0.68196 - acc: 0.7320 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1757 | loss: 0.68128 - acc: 0.7388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.68767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1758 | loss: 0.68767 - acc: 0.6749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.68642\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1759 | loss: 0.68642 - acc: 0.6874 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.69229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1760 | loss: 0.69229 - acc: 0.6287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.69058\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1761 | loss: 0.69058 - acc: 0.6458 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.68904\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1762 | loss: 0.68904 - acc: 0.6612 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.68765\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1763 | loss: 0.68765 - acc: 0.6751 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.68640\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1764 | loss: 0.68640 - acc: 0.6876 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.68528\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1765 | loss: 0.68528 - acc: 0.6988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.69226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1766 | loss: 0.69226 - acc: 0.6289 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.69055\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1767 | loss: 0.69055 - acc: 0.6461 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.68901\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1768 | loss: 0.68901 - acc: 0.6614 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.68763\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1769 | loss: 0.68763 - acc: 0.6753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.68638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1770 | loss: 0.68638 - acc: 0.6878 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.68526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1771 | loss: 0.68526 - acc: 0.6990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.68425\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1772 | loss: 0.68425 - acc: 0.7091 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.68334\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1773 | loss: 0.68334 - acc: 0.7182 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.69052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1774 | loss: 0.69052 - acc: 0.6464 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.68899\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1775 | loss: 0.68899 - acc: 0.6617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.68760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1776 | loss: 0.68760 - acc: 0.6756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.68636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1777 | loss: 0.68636 - acc: 0.6880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.69224\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1778 | loss: 0.69224 - acc: 0.6292 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.69053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1779 | loss: 0.69053 - acc: 0.6463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.68899\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1780 | loss: 0.68899 - acc: 0.6617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.68761\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1781 | loss: 0.68761 - acc: 0.6755 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.68637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1782 | loss: 0.68637 - acc: 0.6879 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.68524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1783 | loss: 0.68524 - acc: 0.6991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.68424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1784 | loss: 0.68424 - acc: 0.7092 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.68333\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1785 | loss: 0.68333 - acc: 0.7183 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.68251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1786 | loss: 0.68251 - acc: 0.7265 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.68178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1787 | loss: 0.68178 - acc: 0.7338 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.68111\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1788 | loss: 0.68111 - acc: 0.7404 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.68052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1789 | loss: 0.68052 - acc: 0.7464 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.67998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1790 | loss: 0.67998 - acc: 0.7518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.67950\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1791 | loss: 0.67950 - acc: 0.7566 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.67907\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1792 | loss: 0.67907 - acc: 0.7609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.67868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1793 | loss: 0.67868 - acc: 0.7648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.67832\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1794 | loss: 0.67832 - acc: 0.7684 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.67801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1795 | loss: 0.67801 - acc: 0.7715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.68572\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1796 | loss: 0.68572 - acc: 0.6944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.68467\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1797 | loss: 0.68467 - acc: 0.7049 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.68372\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1798 | loss: 0.68372 - acc: 0.7144 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.68286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1799 | loss: 0.68286 - acc: 0.7230 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.68909\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1800 | loss: 0.68909 - acc: 0.6607 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.68770\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1801 | loss: 0.68770 - acc: 0.6746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.68644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1802 | loss: 0.68644 - acc: 0.6872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1803 | loss: 0.68531 - acc: 0.6984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.69130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1804 | loss: 0.69130 - acc: 0.6386 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.68968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1805 | loss: 0.68968 - acc: 0.6547 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1806 | loss: 0.68823 - acc: 0.6693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.68693\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1807 | loss: 0.68693 - acc: 0.6823 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.69175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1808 | loss: 0.69175 - acc: 0.6341 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.69009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1809 | loss: 0.69009 - acc: 0.6507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.68860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1810 | loss: 0.68860 - acc: 0.6656 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.68725\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1811 | loss: 0.68725 - acc: 0.6791 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.68604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1812 | loss: 0.68604 - acc: 0.6912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.68495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1813 | loss: 0.68495 - acc: 0.7020 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.68998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1814 | loss: 0.68998 - acc: 0.6518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.68849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1815 | loss: 0.68849 - acc: 0.6667 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.68716\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1816 | loss: 0.68716 - acc: 0.6800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.68596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1817 | loss: 0.68596 - acc: 0.6920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.68488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1818 | loss: 0.68488 - acc: 0.7028 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.68391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1819 | loss: 0.68391 - acc: 0.7125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.68303\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1820 | loss: 0.68303 - acc: 0.7213 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.68225\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1821 | loss: 0.68225 - acc: 0.7291 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.68154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1822 | loss: 0.68154 - acc: 0.7362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.68090\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1823 | loss: 0.68090 - acc: 0.7426 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.68033\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1824 | loss: 0.68033 - acc: 0.7483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.67981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1825 | loss: 0.67981 - acc: 0.7535 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.67934\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1826 | loss: 0.67934 - acc: 0.7582 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.67893\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1827 | loss: 0.67893 - acc: 0.7623 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.67855\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1828 | loss: 0.67855 - acc: 0.7661 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.67821\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1829 | loss: 0.67821 - acc: 0.7695 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.67790\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1830 | loss: 0.67790 - acc: 0.7725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.67763\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1831 | loss: 0.67763 - acc: 0.7753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.67738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1832 | loss: 0.67738 - acc: 0.7778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.67716\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1833 | loss: 0.67716 - acc: 0.7800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.67696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1834 | loss: 0.67696 - acc: 0.7820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.67678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1835 | loss: 0.67678 - acc: 0.7838 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.67662\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1836 | loss: 0.67662 - acc: 0.7854 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.67647\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1837 | loss: 0.67647 - acc: 0.7869 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.68434\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1838 | loss: 0.68434 - acc: 0.7082 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.68342\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1839 | loss: 0.68342 - acc: 0.7174 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.69060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1840 | loss: 0.69060 - acc: 0.6456 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.68905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1841 | loss: 0.68905 - acc: 0.6611 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.68766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1842 | loss: 0.68766 - acc: 0.6750 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.68641\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1843 | loss: 0.68641 - acc: 0.6875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.68529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1844 | loss: 0.68529 - acc: 0.6987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.68427\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1845 | loss: 0.68427 - acc: 0.7088 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.68836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1846 | loss: 0.68836 - acc: 0.6680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.68704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1847 | loss: 0.68704 - acc: 0.6812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.68585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1848 | loss: 0.68585 - acc: 0.6930 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.68478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1849 | loss: 0.68478 - acc: 0.7037 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.68382\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1850 | loss: 0.68382 - acc: 0.7134 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.68296\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1851 | loss: 0.68296 - acc: 0.7220 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.68218\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1852 | loss: 0.68218 - acc: 0.7298 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.68147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1853 | loss: 0.68147 - acc: 0.7368 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.68084\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1854 | loss: 0.68084 - acc: 0.7432 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.68027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1855 | loss: 0.68027 - acc: 0.7488 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.68676\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1856 | loss: 0.68676 - acc: 0.6840 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.68560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1857 | loss: 0.68560 - acc: 0.6956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.69256\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1858 | loss: 0.69256 - acc: 0.6260 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.69082\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1859 | loss: 0.69082 - acc: 0.6434 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.68925\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1860 | loss: 0.68925 - acc: 0.6591 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.68784\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1861 | loss: 0.68784 - acc: 0.6732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.68657\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1862 | loss: 0.68657 - acc: 0.6858 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.68543\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1863 | loss: 0.68543 - acc: 0.6973 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.68441\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1864 | loss: 0.68441 - acc: 0.7075 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.68348\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1865 | loss: 0.68348 - acc: 0.7168 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.68265\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1866 | loss: 0.68265 - acc: 0.7251 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.68190\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1867 | loss: 0.68190 - acc: 0.7326 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.68823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1868 | loss: 0.68823 - acc: 0.6693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.68692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1869 | loss: 0.68692 - acc: 0.6824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.68574\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1870 | loss: 0.68574 - acc: 0.6942 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.68468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1871 | loss: 0.68468 - acc: 0.7047 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.68373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1872 | loss: 0.68373 - acc: 0.7143 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.68287\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1873 | loss: 0.68287 - acc: 0.7228 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.68210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1874 | loss: 0.68210 - acc: 0.7306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.68141\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1875 | loss: 0.68141 - acc: 0.7375 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.68078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1876 | loss: 0.68078 - acc: 0.7438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.68022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1877 | loss: 0.68022 - acc: 0.7494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.67972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1878 | loss: 0.67972 - acc: 0.7544 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.67926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1879 | loss: 0.67926 - acc: 0.7590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.68485\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1880 | loss: 0.68485 - acc: 0.7031 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.68388\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1881 | loss: 0.68388 - acc: 0.7128 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.69101\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1882 | loss: 0.69101 - acc: 0.6415 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.68942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1883 | loss: 0.68942 - acc: 0.6574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.68800\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1884 | loss: 0.68800 - acc: 0.6716 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.68671\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1885 | loss: 0.68671 - acc: 0.6845 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.69056\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1886 | loss: 0.69056 - acc: 0.6460 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.68902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1887 | loss: 0.68902 - acc: 0.6614 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.68763\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1888 | loss: 0.68763 - acc: 0.6753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.68638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1889 | loss: 0.68638 - acc: 0.6877 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.68526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1890 | loss: 0.68526 - acc: 0.6990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.68425\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1891 | loss: 0.68425 - acc: 0.7091 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.69134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1892 | loss: 0.69134 - acc: 0.6382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.68972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1893 | loss: 0.68972 - acc: 0.6543 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.68827\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1894 | loss: 0.68827 - acc: 0.6689 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.68696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1895 | loss: 0.68696 - acc: 0.6820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.69078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1896 | loss: 0.69078 - acc: 0.6438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.68922\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1897 | loss: 0.68922 - acc: 0.6594 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.68781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1898 | loss: 0.68781 - acc: 0.6735 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.68654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1899 | loss: 0.68654 - acc: 0.6861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.68541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1900 | loss: 0.68541 - acc: 0.6975 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.68438\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1901 | loss: 0.68438 - acc: 0.7078 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.68346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1902 | loss: 0.68346 - acc: 0.7170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.68263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1903 | loss: 0.68263 - acc: 0.7253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.68188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1904 | loss: 0.68188 - acc: 0.7328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.68121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1905 | loss: 0.68121 - acc: 0.7395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.68760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1906 | loss: 0.68760 - acc: 0.6755 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.68636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1907 | loss: 0.68636 - acc: 0.6880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.69224\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1908 | loss: 0.69224 - acc: 0.6292 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.69053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1909 | loss: 0.69053 - acc: 0.6463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.68899\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1910 | loss: 0.68899 - acc: 0.6616 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.68761\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1911 | loss: 0.68761 - acc: 0.6755 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.69337\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1912 | loss: 0.69337 - acc: 0.6179 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.69155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1913 | loss: 0.69155 - acc: 0.6361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.68991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1914 | loss: 0.68991 - acc: 0.6525 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.68843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1915 | loss: 0.68843 - acc: 0.6673 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.68710\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1916 | loss: 0.68710 - acc: 0.6805 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.68591\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1917 | loss: 0.68591 - acc: 0.6925 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.68483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1918 | loss: 0.68483 - acc: 0.7032 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.68387\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1919 | loss: 0.68387 - acc: 0.7129 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.68300\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1920 | loss: 0.68300 - acc: 0.7216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.68221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1921 | loss: 0.68221 - acc: 0.7295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.68151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1922 | loss: 0.68151 - acc: 0.7365 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.68087\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1923 | loss: 0.68087 - acc: 0.7429 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.68030\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1924 | loss: 0.68030 - acc: 0.7486 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.67979\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1925 | loss: 0.67979 - acc: 0.7537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.67932\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1926 | loss: 0.67932 - acc: 0.7583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.67891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1927 | loss: 0.67891 - acc: 0.7625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.67853\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1928 | loss: 0.67853 - acc: 0.7663 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.67820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1929 | loss: 0.67820 - acc: 0.7696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.67789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1930 | loss: 0.67789 - acc: 0.7727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.67762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1931 | loss: 0.67762 - acc: 0.7754 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.68537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1932 | loss: 0.68537 - acc: 0.6979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.68435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1933 | loss: 0.68435 - acc: 0.7081 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.69043\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1934 | loss: 0.69043 - acc: 0.6473 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.68890\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1935 | loss: 0.68890 - acc: 0.6625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.68753\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1936 | loss: 0.68753 - acc: 0.6763 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.68629\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1937 | loss: 0.68629 - acc: 0.6887 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.69318\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1938 | loss: 0.69318 - acc: 0.6198 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.69138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1939 | loss: 0.69138 - acc: 0.6378 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.68976\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1940 | loss: 0.68976 - acc: 0.6540 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.68830\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1941 | loss: 0.68830 - acc: 0.6686 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.69298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1942 | loss: 0.69298 - acc: 0.6218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.69120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1943 | loss: 0.69120 - acc: 0.6396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.68960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1944 | loss: 0.68960 - acc: 0.6556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.68815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1945 | loss: 0.68815 - acc: 0.6701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.68685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1946 | loss: 0.68685 - acc: 0.6831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.68568\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1947 | loss: 0.68568 - acc: 0.6948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.68463\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1948 | loss: 0.68463 - acc: 0.7053 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.68368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1949 | loss: 0.68368 - acc: 0.7148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.68983\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1950 | loss: 0.68983 - acc: 0.6533 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.68836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1951 | loss: 0.68836 - acc: 0.6679 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.68704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1952 | loss: 0.68704 - acc: 0.6812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.68586\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1953 | loss: 0.68586 - acc: 0.6930 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.69279\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1954 | loss: 0.69279 - acc: 0.6237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.69102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1955 | loss: 0.69102 - acc: 0.6414 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.68944\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1956 | loss: 0.68944 - acc: 0.6572 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.68801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1957 | loss: 0.68801 - acc: 0.6715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.68672\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1958 | loss: 0.68672 - acc: 0.6844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.68557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1959 | loss: 0.68557 - acc: 0.6959 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.68453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1960 | loss: 0.68453 - acc: 0.7063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.68359\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1961 | loss: 0.68359 - acc: 0.7157 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.68275\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1962 | loss: 0.68275 - acc: 0.7241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.68199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1963 | loss: 0.68199 - acc: 0.7317 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.68130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1964 | loss: 0.68130 - acc: 0.7385 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.68069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1965 | loss: 0.68069 - acc: 0.7447 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.68014\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1966 | loss: 0.68014 - acc: 0.7502 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.67964\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1967 | loss: 0.67964 - acc: 0.7552 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.67919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1968 | loss: 0.67919 - acc: 0.7597 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.67879\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1969 | loss: 0.67879 - acc: 0.7637 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.68643\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1970 | loss: 0.68643 - acc: 0.6873 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.68530\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1971 | loss: 0.68530 - acc: 0.6986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.69128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1972 | loss: 0.69128 - acc: 0.6387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.68967\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1973 | loss: 0.68967 - acc: 0.6549 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.68822\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1974 | loss: 0.68822 - acc: 0.6694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.68691\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1975 | loss: 0.68691 - acc: 0.6824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.69374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1976 | loss: 0.69374 - acc: 0.6142 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.69188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1977 | loss: 0.69188 - acc: 0.6328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.69500\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1978 | loss: 0.69500 - acc: 0.5795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.69500\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1979 | loss: 0.69500 - acc: 0.6016 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.69302\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1980 | loss: 0.69302 - acc: 0.6214 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.69123\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1981 | loss: 0.69123 - acc: 0.6393 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.69763\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1982 | loss: 0.69763 - acc: 0.5753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.69538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1983 | loss: 0.69538 - acc: 0.5978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.69336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1984 | loss: 0.69336 - acc: 0.6180 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.69154\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 1985 | loss: 0.69154 - acc: 0.6362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.68990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1986 | loss: 0.68990 - acc: 0.6526 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.68843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1987 | loss: 0.68843 - acc: 0.6673 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.69510\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1988 | loss: 0.69510 - acc: 0.6006 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.69310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1989 | loss: 0.69310 - acc: 0.6205 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.69831\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1990 | loss: 0.69831 - acc: 0.5685 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.69600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1991 | loss: 0.69600 - acc: 0.5916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.69391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1992 | loss: 0.69391 - acc: 0.6125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.69204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1993 | loss: 0.69204 - acc: 0.6312 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.69835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1994 | loss: 0.69835 - acc: 0.5681 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.69603\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1995 | loss: 0.69603 - acc: 0.5913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.69994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1996 | loss: 0.69994 - acc: 0.5522 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.69746\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1997 | loss: 0.69746 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1998 | loss: 0.69523 - acc: 0.5993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.69323\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1999 | loss: 0.69323 - acc: 0.6193 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.69142\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2000 | loss: 0.69142 - acc: 0.6374 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2001  | total loss: \u001b[1m\u001b[32m0.68979\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2001 | loss: 0.68979 - acc: 0.6537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2002  | total loss: \u001b[1m\u001b[32m0.69433\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2002 | loss: 0.69433 - acc: 0.6083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2003  | total loss: \u001b[1m\u001b[32m0.69241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2003 | loss: 0.69241 - acc: 0.6275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2004  | total loss: \u001b[1m\u001b[32m0.69669\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2004 | loss: 0.69669 - acc: 0.5847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2005  | total loss: \u001b[1m\u001b[32m0.69453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2005 | loss: 0.69453 - acc: 0.6062 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2006  | total loss: \u001b[1m\u001b[32m0.69260\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2006 | loss: 0.69260 - acc: 0.6256 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2007  | total loss: \u001b[1m\u001b[32m0.69085\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2007 | loss: 0.69085 - acc: 0.6431 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2008  | total loss: \u001b[1m\u001b[32m0.68928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2008 | loss: 0.68928 - acc: 0.6588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2009  | total loss: \u001b[1m\u001b[32m0.68787\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2009 | loss: 0.68787 - acc: 0.6729 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2010  | total loss: \u001b[1m\u001b[32m0.68660\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2010 | loss: 0.68660 - acc: 0.6856 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2011  | total loss: \u001b[1m\u001b[32m0.68546\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2011 | loss: 0.68546 - acc: 0.6970 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2012  | total loss: \u001b[1m\u001b[32m0.68843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2012 | loss: 0.68843 - acc: 0.6673 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2013  | total loss: \u001b[1m\u001b[32m0.68710\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2013 | loss: 0.68710 - acc: 0.6806 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2014  | total loss: \u001b[1m\u001b[32m0.68591\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2014 | loss: 0.68591 - acc: 0.6925 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2015  | total loss: \u001b[1m\u001b[32m0.68483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2015 | loss: 0.68483 - acc: 0.7033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2016  | total loss: \u001b[1m\u001b[32m0.68386\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2016 | loss: 0.68386 - acc: 0.7130 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2017  | total loss: \u001b[1m\u001b[32m0.68299\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2017 | loss: 0.68299 - acc: 0.7217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2018  | total loss: \u001b[1m\u001b[32m0.68221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2018 | loss: 0.68221 - acc: 0.7295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2019  | total loss: \u001b[1m\u001b[32m0.68150\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2019 | loss: 0.68150 - acc: 0.7365 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2020  | total loss: \u001b[1m\u001b[32m0.68087\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2020 | loss: 0.68087 - acc: 0.7429 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2021  | total loss: \u001b[1m\u001b[32m0.68030\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2021 | loss: 0.68030 - acc: 0.7486 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2022  | total loss: \u001b[1m\u001b[32m0.67978\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2022 | loss: 0.67978 - acc: 0.7537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2023  | total loss: \u001b[1m\u001b[32m0.67932\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2023 | loss: 0.67932 - acc: 0.7584 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2024  | total loss: \u001b[1m\u001b[32m0.67891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2024 | loss: 0.67891 - acc: 0.7625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2025  | total loss: \u001b[1m\u001b[32m0.67853\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2025 | loss: 0.67853 - acc: 0.7663 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2026  | total loss: \u001b[1m\u001b[32m0.67819\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2026 | loss: 0.67819 - acc: 0.7696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2027  | total loss: \u001b[1m\u001b[32m0.67789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2027 | loss: 0.67789 - acc: 0.7727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2028  | total loss: \u001b[1m\u001b[32m0.67762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2028 | loss: 0.67762 - acc: 0.7754 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2029  | total loss: \u001b[1m\u001b[32m0.67737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2029 | loss: 0.67737 - acc: 0.7779 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2030  | total loss: \u001b[1m\u001b[32m0.67715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2030 | loss: 0.67715 - acc: 0.7801 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2031  | total loss: \u001b[1m\u001b[32m0.67695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2031 | loss: 0.67695 - acc: 0.7821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2032  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2032 | loss: 0.68277 - acc: 0.7239 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2033  | total loss: \u001b[1m\u001b[32m0.68201\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2033 | loss: 0.68201 - acc: 0.7315 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2034  | total loss: \u001b[1m\u001b[32m0.68833\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2034 | loss: 0.68833 - acc: 0.6683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2035  | total loss: \u001b[1m\u001b[32m0.68701\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2035 | loss: 0.68701 - acc: 0.6815 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2036  | total loss: \u001b[1m\u001b[32m0.68582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2036 | loss: 0.68582 - acc: 0.6934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2037  | total loss: \u001b[1m\u001b[32m0.68476\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2037 | loss: 0.68476 - acc: 0.7040 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2038  | total loss: \u001b[1m\u001b[32m0.68980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2038 | loss: 0.68980 - acc: 0.6536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2039  | total loss: \u001b[1m\u001b[32m0.68833\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2039 | loss: 0.68833 - acc: 0.6683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2040  | total loss: \u001b[1m\u001b[32m0.68702\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2040 | loss: 0.68702 - acc: 0.6814 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2041  | total loss: \u001b[1m\u001b[32m0.68583\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2041 | loss: 0.68583 - acc: 0.6933 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2042  | total loss: \u001b[1m\u001b[32m0.69276\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2042 | loss: 0.69276 - acc: 0.6240 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2043  | total loss: \u001b[1m\u001b[32m0.69100\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2043 | loss: 0.69100 - acc: 0.6416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2044  | total loss: \u001b[1m\u001b[32m0.68942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2044 | loss: 0.68942 - acc: 0.6574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2045  | total loss: \u001b[1m\u001b[32m0.68799\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2045 | loss: 0.68799 - acc: 0.6717 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2046  | total loss: \u001b[1m\u001b[32m0.68671\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2046 | loss: 0.68671 - acc: 0.6845 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2047  | total loss: \u001b[1m\u001b[32m0.68555\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2047 | loss: 0.68555 - acc: 0.6960 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2048  | total loss: \u001b[1m\u001b[32m0.68451\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2048 | loss: 0.68451 - acc: 0.7064 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2049  | total loss: \u001b[1m\u001b[32m0.68358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2049 | loss: 0.68358 - acc: 0.7158 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2050  | total loss: \u001b[1m\u001b[32m0.69074\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2050 | loss: 0.69074 - acc: 0.6442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2051  | total loss: \u001b[1m\u001b[32m0.68918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2051 | loss: 0.68918 - acc: 0.6598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2052  | total loss: \u001b[1m\u001b[32m0.69578\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2052 | loss: 0.69578 - acc: 0.5938 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2053  | total loss: \u001b[1m\u001b[32m0.69372\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2053 | loss: 0.69372 - acc: 0.6144 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2054  | total loss: \u001b[1m\u001b[32m0.69186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2054 | loss: 0.69186 - acc: 0.6330 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2055  | total loss: \u001b[1m\u001b[32m0.69019\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2055 | loss: 0.69019 - acc: 0.6497 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2056  | total loss: \u001b[1m\u001b[32m0.68869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2056 | loss: 0.68869 - acc: 0.6647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2057  | total loss: \u001b[1m\u001b[32m0.68733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2057 | loss: 0.68733 - acc: 0.6783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2058  | total loss: \u001b[1m\u001b[32m0.68612\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2058 | loss: 0.68612 - acc: 0.6904 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2059  | total loss: \u001b[1m\u001b[32m0.68502\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2059 | loss: 0.68502 - acc: 0.7014 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2060  | total loss: \u001b[1m\u001b[32m0.69103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2060 | loss: 0.69103 - acc: 0.6412 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2061  | total loss: \u001b[1m\u001b[32m0.68945\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2061 | loss: 0.68945 - acc: 0.6571 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2062  | total loss: \u001b[1m\u001b[32m0.68802\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2062 | loss: 0.68802 - acc: 0.6714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2063  | total loss: \u001b[1m\u001b[32m0.68673\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2063 | loss: 0.68673 - acc: 0.6843 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2064  | total loss: \u001b[1m\u001b[32m0.68557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2064 | loss: 0.68557 - acc: 0.6958 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2065  | total loss: \u001b[1m\u001b[32m0.68453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2065 | loss: 0.68453 - acc: 0.7063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2066  | total loss: \u001b[1m\u001b[32m0.68360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2066 | loss: 0.68360 - acc: 0.7156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2067  | total loss: \u001b[1m\u001b[32m0.68275\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2067 | loss: 0.68275 - acc: 0.7241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2068  | total loss: \u001b[1m\u001b[32m0.68199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2068 | loss: 0.68199 - acc: 0.7317 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2069  | total loss: \u001b[1m\u001b[32m0.68131\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2069 | loss: 0.68131 - acc: 0.7385 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2070  | total loss: \u001b[1m\u001b[32m0.68069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2070 | loss: 0.68069 - acc: 0.7446 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2071  | total loss: \u001b[1m\u001b[32m0.68014\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2071 | loss: 0.68014 - acc: 0.7502 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2072  | total loss: \u001b[1m\u001b[32m0.67964\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2072 | loss: 0.67964 - acc: 0.7552 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2073  | total loss: \u001b[1m\u001b[32m0.67919\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2073 | loss: 0.67919 - acc: 0.7596 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2074  | total loss: \u001b[1m\u001b[32m0.67879\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2074 | loss: 0.67879 - acc: 0.7637 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2075  | total loss: \u001b[1m\u001b[32m0.67843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2075 | loss: 0.67843 - acc: 0.7673 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2076  | total loss: \u001b[1m\u001b[32m0.67810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2076 | loss: 0.67810 - acc: 0.7706 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2077  | total loss: \u001b[1m\u001b[32m0.67781\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2077 | loss: 0.67781 - acc: 0.7735 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2078  | total loss: \u001b[1m\u001b[32m0.68354\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2078 | loss: 0.68354 - acc: 0.7162 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2079  | total loss: \u001b[1m\u001b[32m0.68270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2079 | loss: 0.68270 - acc: 0.7246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2080  | total loss: \u001b[1m\u001b[32m0.68695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2080 | loss: 0.68695 - acc: 0.6821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2081  | total loss: \u001b[1m\u001b[32m0.68577\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2081 | loss: 0.68577 - acc: 0.6939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2082  | total loss: \u001b[1m\u001b[32m0.68471\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2082 | loss: 0.68471 - acc: 0.7045 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2083  | total loss: \u001b[1m\u001b[32m0.68375\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2083 | loss: 0.68375 - acc: 0.7141 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2084  | total loss: \u001b[1m\u001b[32m0.68889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2084 | loss: 0.68889 - acc: 0.6626 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2085  | total loss: \u001b[1m\u001b[32m0.68752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2085 | loss: 0.68752 - acc: 0.6764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2086  | total loss: \u001b[1m\u001b[32m0.68628\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2086 | loss: 0.68628 - acc: 0.6887 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2087  | total loss: \u001b[1m\u001b[32m0.68517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2087 | loss: 0.68517 - acc: 0.6999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2088  | total loss: \u001b[1m\u001b[32m0.68417\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2088 | loss: 0.68417 - acc: 0.7099 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2089  | total loss: \u001b[1m\u001b[32m0.68327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2089 | loss: 0.68327 - acc: 0.7189 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2090  | total loss: \u001b[1m\u001b[32m0.68246\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2090 | loss: 0.68246 - acc: 0.7270 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2091  | total loss: \u001b[1m\u001b[32m0.68173\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2091 | loss: 0.68173 - acc: 0.7343 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2092  | total loss: \u001b[1m\u001b[32m0.68707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2092 | loss: 0.68707 - acc: 0.6809 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2093  | total loss: \u001b[1m\u001b[32m0.68588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2093 | loss: 0.68588 - acc: 0.6928 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2094  | total loss: \u001b[1m\u001b[32m0.68481\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2094 | loss: 0.68481 - acc: 0.7035 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2095  | total loss: \u001b[1m\u001b[32m0.68384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2095 | loss: 0.68384 - acc: 0.7132 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2096  | total loss: \u001b[1m\u001b[32m0.68297\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2096 | loss: 0.68297 - acc: 0.7218 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2097  | total loss: \u001b[1m\u001b[32m0.68219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2097 | loss: 0.68219 - acc: 0.7297 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2098  | total loss: \u001b[1m\u001b[32m0.68149\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2098 | loss: 0.68149 - acc: 0.7367 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2099  | total loss: \u001b[1m\u001b[32m0.68086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2099 | loss: 0.68086 - acc: 0.7430 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2100  | total loss: \u001b[1m\u001b[32m0.68029\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2100 | loss: 0.68029 - acc: 0.7487 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2101  | total loss: \u001b[1m\u001b[32m0.67977\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2101 | loss: 0.67977 - acc: 0.7538 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2102  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2102 | loss: 0.68531 - acc: 0.6985 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2103  | total loss: \u001b[1m\u001b[32m0.68430\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2103 | loss: 0.68430 - acc: 0.7086 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2104  | total loss: \u001b[1m\u001b[32m0.68338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2104 | loss: 0.68338 - acc: 0.7178 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2105  | total loss: \u001b[1m\u001b[32m0.68256\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2105 | loss: 0.68256 - acc: 0.7260 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2106  | total loss: \u001b[1m\u001b[32m0.68182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2106 | loss: 0.68182 - acc: 0.7334 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2107  | total loss: \u001b[1m\u001b[32m0.68115\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2107 | loss: 0.68115 - acc: 0.7400 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2108  | total loss: \u001b[1m\u001b[32m0.68055\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2108 | loss: 0.68055 - acc: 0.7460 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2109  | total loss: \u001b[1m\u001b[32m0.68002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2109 | loss: 0.68002 - acc: 0.7514 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2110  | total loss: \u001b[1m\u001b[32m0.67953\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2110 | loss: 0.67953 - acc: 0.7563 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2111  | total loss: \u001b[1m\u001b[32m0.67909\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2111 | loss: 0.67909 - acc: 0.7607 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2112  | total loss: \u001b[1m\u001b[32m0.67870\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2112 | loss: 0.67870 - acc: 0.7646 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2113  | total loss: \u001b[1m\u001b[32m0.67835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2113 | loss: 0.67835 - acc: 0.7681 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2114  | total loss: \u001b[1m\u001b[32m0.67803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2114 | loss: 0.67803 - acc: 0.7713 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2115  | total loss: \u001b[1m\u001b[32m0.67774\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2115 | loss: 0.67774 - acc: 0.7742 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2116  | total loss: \u001b[1m\u001b[32m0.67748\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2116 | loss: 0.67748 - acc: 0.7768 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2117  | total loss: \u001b[1m\u001b[32m0.67725\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2117 | loss: 0.67725 - acc: 0.7791 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2118  | total loss: \u001b[1m\u001b[32m0.67704\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2118 | loss: 0.67704 - acc: 0.7812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2119  | total loss: \u001b[1m\u001b[32m0.67685\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2119 | loss: 0.67685 - acc: 0.7831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2120  | total loss: \u001b[1m\u001b[32m0.67668\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2120 | loss: 0.67668 - acc: 0.7848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2121  | total loss: \u001b[1m\u001b[32m0.67653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2121 | loss: 0.67653 - acc: 0.7863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2122  | total loss: \u001b[1m\u001b[32m0.68239\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2122 | loss: 0.68239 - acc: 0.7277 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2123  | total loss: \u001b[1m\u001b[32m0.68167\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2123 | loss: 0.68167 - acc: 0.7349 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2124  | total loss: \u001b[1m\u001b[32m0.68802\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2124 | loss: 0.68802 - acc: 0.6714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2125  | total loss: \u001b[1m\u001b[32m0.68673\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2125 | loss: 0.68673 - acc: 0.6843 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2126  | total loss: \u001b[1m\u001b[32m0.68558\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2126 | loss: 0.68558 - acc: 0.6958 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2127  | total loss: \u001b[1m\u001b[32m0.68453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2127 | loss: 0.68453 - acc: 0.7063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2128  | total loss: \u001b[1m\u001b[32m0.68360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2128 | loss: 0.68360 - acc: 0.7156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2129  | total loss: \u001b[1m\u001b[32m0.68275\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2129 | loss: 0.68275 - acc: 0.7241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2130  | total loss: \u001b[1m\u001b[32m0.68199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2130 | loss: 0.68199 - acc: 0.7317 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2131  | total loss: \u001b[1m\u001b[32m0.68131\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2131 | loss: 0.68131 - acc: 0.7385 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2132  | total loss: \u001b[1m\u001b[32m0.68069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2132 | loss: 0.68069 - acc: 0.7446 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2133  | total loss: \u001b[1m\u001b[32m0.68014\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2133 | loss: 0.68014 - acc: 0.7502 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2134  | total loss: \u001b[1m\u001b[32m0.68764\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2134 | loss: 0.68764 - acc: 0.6752 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2135  | total loss: \u001b[1m\u001b[32m0.68639\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2135 | loss: 0.68639 - acc: 0.6876 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2136  | total loss: \u001b[1m\u001b[32m0.68527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2136 | loss: 0.68527 - acc: 0.6989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2137  | total loss: \u001b[1m\u001b[32m0.68426\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2137 | loss: 0.68426 - acc: 0.7090 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2138  | total loss: \u001b[1m\u001b[32m0.68335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2138 | loss: 0.68335 - acc: 0.7181 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2139  | total loss: \u001b[1m\u001b[32m0.68253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2139 | loss: 0.68253 - acc: 0.7263 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2140  | total loss: \u001b[1m\u001b[32m0.68179\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2140 | loss: 0.68179 - acc: 0.7337 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2141  | total loss: \u001b[1m\u001b[32m0.68113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2141 | loss: 0.68113 - acc: 0.7403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2142  | total loss: \u001b[1m\u001b[32m0.68053\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2142 | loss: 0.68053 - acc: 0.7463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2143  | total loss: \u001b[1m\u001b[32m0.68000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2143 | loss: 0.68000 - acc: 0.7516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2144  | total loss: \u001b[1m\u001b[32m0.67951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2144 | loss: 0.67951 - acc: 0.7565 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2145  | total loss: \u001b[1m\u001b[32m0.67908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2145 | loss: 0.67908 - acc: 0.7608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2146  | total loss: \u001b[1m\u001b[32m0.67868\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2146 | loss: 0.67868 - acc: 0.7647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2147  | total loss: \u001b[1m\u001b[32m0.67833\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2147 | loss: 0.67833 - acc: 0.7683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2148  | total loss: \u001b[1m\u001b[32m0.67801\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2148 | loss: 0.67801 - acc: 0.7714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2149  | total loss: \u001b[1m\u001b[32m0.67773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2149 | loss: 0.67773 - acc: 0.7743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2150  | total loss: \u001b[1m\u001b[32m0.67747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2150 | loss: 0.67747 - acc: 0.7769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2151  | total loss: \u001b[1m\u001b[32m0.67724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2151 | loss: 0.67724 - acc: 0.7792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2152  | total loss: \u001b[1m\u001b[32m0.67703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2152 | loss: 0.67703 - acc: 0.7813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2153  | total loss: \u001b[1m\u001b[32m0.67685\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2153 | loss: 0.67685 - acc: 0.7831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2154  | total loss: \u001b[1m\u001b[32m0.67668\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2154 | loss: 0.67668 - acc: 0.7848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2155  | total loss: \u001b[1m\u001b[32m0.67652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2155 | loss: 0.67652 - acc: 0.7863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2156  | total loss: \u001b[1m\u001b[32m0.67639\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2156 | loss: 0.67639 - acc: 0.7877 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2157  | total loss: \u001b[1m\u001b[32m0.67627\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2157 | loss: 0.67627 - acc: 0.7889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2158  | total loss: \u001b[1m\u001b[32m0.67615\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2158 | loss: 0.67615 - acc: 0.7900 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2159  | total loss: \u001b[1m\u001b[32m0.67606\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2159 | loss: 0.67606 - acc: 0.7910 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2160  | total loss: \u001b[1m\u001b[32m0.67597\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2160 | loss: 0.67597 - acc: 0.7919 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2161  | total loss: \u001b[1m\u001b[32m0.67588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2161 | loss: 0.67588 - acc: 0.7927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2162  | total loss: \u001b[1m\u001b[32m0.67581\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2162 | loss: 0.67581 - acc: 0.7935 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2163  | total loss: \u001b[1m\u001b[32m0.67575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2163 | loss: 0.67575 - acc: 0.7941 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2164  | total loss: \u001b[1m\u001b[32m0.67569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2164 | loss: 0.67569 - acc: 0.7947 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2165  | total loss: \u001b[1m\u001b[32m0.67564\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2165 | loss: 0.67564 - acc: 0.7952 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2166  | total loss: \u001b[1m\u001b[32m0.67559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2166 | loss: 0.67559 - acc: 0.7957 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2167  | total loss: \u001b[1m\u001b[32m0.67554\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2167 | loss: 0.67554 - acc: 0.7961 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2168  | total loss: \u001b[1m\u001b[32m0.67551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2168 | loss: 0.67551 - acc: 0.7965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2169  | total loss: \u001b[1m\u001b[32m0.67547\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2169 | loss: 0.67547 - acc: 0.7969 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2170  | total loss: \u001b[1m\u001b[32m0.68244\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2170 | loss: 0.68244 - acc: 0.7272 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2171  | total loss: \u001b[1m\u001b[32m0.68171\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2171 | loss: 0.68171 - acc: 0.7345 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2172  | total loss: \u001b[1m\u001b[32m0.68806\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2172 | loss: 0.68806 - acc: 0.6710 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2173  | total loss: \u001b[1m\u001b[32m0.68677\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2173 | loss: 0.68677 - acc: 0.6839 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2174  | total loss: \u001b[1m\u001b[32m0.69361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2174 | loss: 0.69361 - acc: 0.6155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2175  | total loss: \u001b[1m\u001b[32m0.69176\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2175 | loss: 0.69176 - acc: 0.6340 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2176  | total loss: \u001b[1m\u001b[32m0.69010\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2176 | loss: 0.69010 - acc: 0.6506 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2177  | total loss: \u001b[1m\u001b[32m0.68861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2177 | loss: 0.68861 - acc: 0.6655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2178  | total loss: \u001b[1m\u001b[32m0.68726\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2178 | loss: 0.68726 - acc: 0.6790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2179  | total loss: \u001b[1m\u001b[32m0.68605\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2179 | loss: 0.68605 - acc: 0.6911 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2180  | total loss: \u001b[1m\u001b[32m0.69296\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2180 | loss: 0.69296 - acc: 0.6220 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2181  | total loss: \u001b[1m\u001b[32m0.69118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2181 | loss: 0.69118 - acc: 0.6398 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2182  | total loss: \u001b[1m\u001b[32m0.69658\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2182 | loss: 0.69658 - acc: 0.5858 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2183  | total loss: \u001b[1m\u001b[32m0.69444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2183 | loss: 0.69444 - acc: 0.6072 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2184  | total loss: \u001b[1m\u001b[32m0.69251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2184 | loss: 0.69251 - acc: 0.6265 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2185  | total loss: \u001b[1m\u001b[32m0.69077\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2185 | loss: 0.69077 - acc: 0.6438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2186  | total loss: \u001b[1m\u001b[32m0.68921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2186 | loss: 0.68921 - acc: 0.6595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2187  | total loss: \u001b[1m\u001b[32m0.68781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2187 | loss: 0.68781 - acc: 0.6735 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2188  | total loss: \u001b[1m\u001b[32m0.68654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2188 | loss: 0.68654 - acc: 0.6862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2189  | total loss: \u001b[1m\u001b[32m0.68540\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2189 | loss: 0.68540 - acc: 0.6975 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2190  | total loss: \u001b[1m\u001b[32m0.68438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2190 | loss: 0.68438 - acc: 0.7078 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2191  | total loss: \u001b[1m\u001b[32m0.68346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2191 | loss: 0.68346 - acc: 0.7170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2192  | total loss: \u001b[1m\u001b[32m0.68263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2192 | loss: 0.68263 - acc: 0.7253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2193  | total loss: \u001b[1m\u001b[32m0.68188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2193 | loss: 0.68188 - acc: 0.7328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2194  | total loss: \u001b[1m\u001b[32m0.68121\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2194 | loss: 0.68121 - acc: 0.7395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2195  | total loss: \u001b[1m\u001b[32m0.68060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2195 | loss: 0.68060 - acc: 0.7456 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2196  | total loss: \u001b[1m\u001b[32m0.68006\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2196 | loss: 0.68006 - acc: 0.7510 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2197  | total loss: \u001b[1m\u001b[32m0.67957\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2197 | loss: 0.67957 - acc: 0.7559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2198  | total loss: \u001b[1m\u001b[32m0.67913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2198 | loss: 0.67913 - acc: 0.7603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2199  | total loss: \u001b[1m\u001b[32m0.67873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2199 | loss: 0.67873 - acc: 0.7643 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.67837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2200 | loss: 0.67837 - acc: 0.7678 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2201  | total loss: \u001b[1m\u001b[32m0.67805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2201 | loss: 0.67805 - acc: 0.7711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2202  | total loss: \u001b[1m\u001b[32m0.68376\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2202 | loss: 0.68376 - acc: 0.7140 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2203  | total loss: \u001b[1m\u001b[32m0.68290\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2203 | loss: 0.68290 - acc: 0.7226 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2204  | total loss: \u001b[1m\u001b[32m0.68913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2204 | loss: 0.68913 - acc: 0.6603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2205  | total loss: \u001b[1m\u001b[32m0.68773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2205 | loss: 0.68773 - acc: 0.6743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2206  | total loss: \u001b[1m\u001b[32m0.68647\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2206 | loss: 0.68647 - acc: 0.6868 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2207  | total loss: \u001b[1m\u001b[32m0.68534\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2207 | loss: 0.68534 - acc: 0.6982 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2208  | total loss: \u001b[1m\u001b[32m0.68432\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2208 | loss: 0.68432 - acc: 0.7083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2209  | total loss: \u001b[1m\u001b[32m0.68341\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2209 | loss: 0.68341 - acc: 0.7175 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2210  | total loss: \u001b[1m\u001b[32m0.69058\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2210 | loss: 0.69058 - acc: 0.6458 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2211  | total loss: \u001b[1m\u001b[32m0.68904\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2211 | loss: 0.68904 - acc: 0.6612 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2212  | total loss: \u001b[1m\u001b[32m0.69465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2212 | loss: 0.69465 - acc: 0.6051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2213  | total loss: \u001b[1m\u001b[32m0.69270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2213 | loss: 0.69270 - acc: 0.6246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2214  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2214 | loss: 0.69095 - acc: 0.6421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2215  | total loss: \u001b[1m\u001b[32m0.68937\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2215 | loss: 0.68937 - acc: 0.6579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2216  | total loss: \u001b[1m\u001b[32m0.68795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2216 | loss: 0.68795 - acc: 0.6721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2217  | total loss: \u001b[1m\u001b[32m0.68667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2217 | loss: 0.68667 - acc: 0.6849 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2218  | total loss: \u001b[1m\u001b[32m0.69352\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2218 | loss: 0.69352 - acc: 0.6164 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2219  | total loss: \u001b[1m\u001b[32m0.69168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2219 | loss: 0.69168 - acc: 0.6348 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2220  | total loss: \u001b[1m\u001b[32m0.69003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2220 | loss: 0.69003 - acc: 0.6513 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2221  | total loss: \u001b[1m\u001b[32m0.68854\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2221 | loss: 0.68854 - acc: 0.6662 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2222  | total loss: \u001b[1m\u001b[32m0.68720\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2222 | loss: 0.68720 - acc: 0.6795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2223  | total loss: \u001b[1m\u001b[32m0.68600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2223 | loss: 0.68600 - acc: 0.6916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2224  | total loss: \u001b[1m\u001b[32m0.68492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2224 | loss: 0.68492 - acc: 0.7024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2225  | total loss: \u001b[1m\u001b[32m0.68394\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2225 | loss: 0.68394 - acc: 0.7122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2226  | total loss: \u001b[1m\u001b[32m0.68306\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2226 | loss: 0.68306 - acc: 0.7210 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2227  | total loss: \u001b[1m\u001b[32m0.68227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2227 | loss: 0.68227 - acc: 0.7289 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2228  | total loss: \u001b[1m\u001b[32m0.68156\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2228 | loss: 0.68156 - acc: 0.7360 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2229  | total loss: \u001b[1m\u001b[32m0.68092\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2229 | loss: 0.68092 - acc: 0.7424 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2230  | total loss: \u001b[1m\u001b[32m0.68034\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2230 | loss: 0.68034 - acc: 0.7481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2231  | total loss: \u001b[1m\u001b[32m0.67983\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2231 | loss: 0.67983 - acc: 0.7533 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2232  | total loss: \u001b[1m\u001b[32m0.68736\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2232 | loss: 0.68736 - acc: 0.6780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2233  | total loss: \u001b[1m\u001b[32m0.68614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2233 | loss: 0.68614 - acc: 0.6902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2234  | total loss: \u001b[1m\u001b[32m0.69004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2234 | loss: 0.69004 - acc: 0.6512 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2235  | total loss: \u001b[1m\u001b[32m0.68855\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2235 | loss: 0.68855 - acc: 0.6661 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2236  | total loss: \u001b[1m\u001b[32m0.68721\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2236 | loss: 0.68721 - acc: 0.6795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2237  | total loss: \u001b[1m\u001b[32m0.68601\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2237 | loss: 0.68601 - acc: 0.6915 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2238  | total loss: \u001b[1m\u001b[32m0.68492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2238 | loss: 0.68492 - acc: 0.7024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2239  | total loss: \u001b[1m\u001b[32m0.68395\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2239 | loss: 0.68395 - acc: 0.7121 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2240  | total loss: \u001b[1m\u001b[32m0.68307\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2240 | loss: 0.68307 - acc: 0.7209 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2241  | total loss: \u001b[1m\u001b[32m0.68228\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2241 | loss: 0.68228 - acc: 0.7288 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2242  | total loss: \u001b[1m\u001b[32m0.68857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2242 | loss: 0.68857 - acc: 0.6659 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2243  | total loss: \u001b[1m\u001b[32m0.68722\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2243 | loss: 0.68722 - acc: 0.6793 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2244  | total loss: \u001b[1m\u001b[32m0.69202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2244 | loss: 0.69202 - acc: 0.6314 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2245  | total loss: \u001b[1m\u001b[32m0.69033\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2245 | loss: 0.69033 - acc: 0.6483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2246  | total loss: \u001b[1m\u001b[32m0.68881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2246 | loss: 0.68881 - acc: 0.6634 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2247  | total loss: \u001b[1m\u001b[32m0.68745\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2247 | loss: 0.68745 - acc: 0.6771 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2248  | total loss: \u001b[1m\u001b[32m0.68622\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2248 | loss: 0.68622 - acc: 0.6894 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2249  | total loss: \u001b[1m\u001b[32m0.68511\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2249 | loss: 0.68511 - acc: 0.7004 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2250  | total loss: \u001b[1m\u001b[32m0.68412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2250 | loss: 0.68412 - acc: 0.7104 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2251  | total loss: \u001b[1m\u001b[32m0.68322\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2251 | loss: 0.68322 - acc: 0.7194 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2252  | total loss: \u001b[1m\u001b[32m0.68242\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2252 | loss: 0.68242 - acc: 0.7274 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2253  | total loss: \u001b[1m\u001b[32m0.68169\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2253 | loss: 0.68169 - acc: 0.7347 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2254  | total loss: \u001b[1m\u001b[32m0.68704\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2254 | loss: 0.68704 - acc: 0.6812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2255  | total loss: \u001b[1m\u001b[32m0.68585\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2255 | loss: 0.68585 - acc: 0.6931 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2256  | total loss: \u001b[1m\u001b[32m0.69278\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2256 | loss: 0.69278 - acc: 0.6238 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2257  | total loss: \u001b[1m\u001b[32m0.69102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2257 | loss: 0.69102 - acc: 0.6414 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2258  | total loss: \u001b[1m\u001b[32m0.68943\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2258 | loss: 0.68943 - acc: 0.6573 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2259  | total loss: \u001b[1m\u001b[32m0.68800\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2259 | loss: 0.68800 - acc: 0.6715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2260  | total loss: \u001b[1m\u001b[32m0.69372\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2260 | loss: 0.69372 - acc: 0.6144 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2261  | total loss: \u001b[1m\u001b[32m0.69186\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2261 | loss: 0.69186 - acc: 0.6329 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2262  | total loss: \u001b[1m\u001b[32m0.69019\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2262 | loss: 0.69019 - acc: 0.6497 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2263  | total loss: \u001b[1m\u001b[32m0.68869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2263 | loss: 0.68869 - acc: 0.6647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2264  | total loss: \u001b[1m\u001b[32m0.68734\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2264 | loss: 0.68734 - acc: 0.6782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2265  | total loss: \u001b[1m\u001b[32m0.68612\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2265 | loss: 0.68612 - acc: 0.6904 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2266  | total loss: \u001b[1m\u001b[32m0.68502\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2266 | loss: 0.68502 - acc: 0.7014 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2267  | total loss: \u001b[1m\u001b[32m0.68404\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2267 | loss: 0.68404 - acc: 0.7112 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2268  | total loss: \u001b[1m\u001b[32m0.68315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2268 | loss: 0.68315 - acc: 0.7201 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2269  | total loss: \u001b[1m\u001b[32m0.68235\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2269 | loss: 0.68235 - acc: 0.7281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2270  | total loss: \u001b[1m\u001b[32m0.68963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2270 | loss: 0.68963 - acc: 0.6553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2271  | total loss: \u001b[1m\u001b[32m0.68818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2271 | loss: 0.68818 - acc: 0.6698 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2272  | total loss: \u001b[1m\u001b[32m0.69388\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2272 | loss: 0.69388 - acc: 0.6128 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2273  | total loss: \u001b[1m\u001b[32m0.69201\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2273 | loss: 0.69201 - acc: 0.6315 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2274  | total loss: \u001b[1m\u001b[32m0.69032\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2274 | loss: 0.69032 - acc: 0.6483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2275  | total loss: \u001b[1m\u001b[32m0.68881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2275 | loss: 0.68881 - acc: 0.6635 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2276  | total loss: \u001b[1m\u001b[32m0.69344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2276 | loss: 0.69344 - acc: 0.6172 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2277  | total loss: \u001b[1m\u001b[32m0.69161\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2277 | loss: 0.69161 - acc: 0.6354 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2278  | total loss: \u001b[1m\u001b[32m0.69497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2278 | loss: 0.69497 - acc: 0.6019 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2279  | total loss: \u001b[1m\u001b[32m0.69299\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2279 | loss: 0.69299 - acc: 0.6217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2280  | total loss: \u001b[1m\u001b[32m0.69920\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2280 | loss: 0.69920 - acc: 0.5595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2281  | total loss: \u001b[1m\u001b[32m0.69680\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2281 | loss: 0.69680 - acc: 0.5836 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2282  | total loss: \u001b[1m\u001b[32m0.69964\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2282 | loss: 0.69964 - acc: 0.5552 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2283  | total loss: \u001b[1m\u001b[32m0.69719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2283 | loss: 0.69719 - acc: 0.5797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2284  | total loss: \u001b[1m\u001b[32m0.69499\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2284 | loss: 0.69499 - acc: 0.6017 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2285  | total loss: \u001b[1m\u001b[32m0.69300\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2285 | loss: 0.69300 - acc: 0.6216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2286  | total loss: \u001b[1m\u001b[32m0.69922\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2286 | loss: 0.69922 - acc: 0.5594 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2287  | total loss: \u001b[1m\u001b[32m0.69681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2287 | loss: 0.69681 - acc: 0.5835 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2288  | total loss: \u001b[1m\u001b[32m0.69465\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2288 | loss: 0.69465 - acc: 0.6051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2289  | total loss: \u001b[1m\u001b[32m0.69270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2289 | loss: 0.69270 - acc: 0.6246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2290  | total loss: \u001b[1m\u001b[32m0.69094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2290 | loss: 0.69094 - acc: 0.6421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2291  | total loss: \u001b[1m\u001b[32m0.68937\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2291 | loss: 0.68937 - acc: 0.6579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2292  | total loss: \u001b[1m\u001b[32m0.69295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2292 | loss: 0.69295 - acc: 0.6221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2293  | total loss: \u001b[1m\u001b[32m0.69117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2293 | loss: 0.69117 - acc: 0.6399 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2294  | total loss: \u001b[1m\u001b[32m0.68957\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2294 | loss: 0.68957 - acc: 0.6559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2295  | total loss: \u001b[1m\u001b[32m0.68813\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2295 | loss: 0.68813 - acc: 0.6703 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2296  | total loss: \u001b[1m\u001b[32m0.69483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2296 | loss: 0.69483 - acc: 0.6033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2297  | total loss: \u001b[1m\u001b[32m0.69286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2297 | loss: 0.69286 - acc: 0.6230 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2298  | total loss: \u001b[1m\u001b[32m0.69109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2298 | loss: 0.69109 - acc: 0.6407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2299  | total loss: \u001b[1m\u001b[32m0.68950\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2299 | loss: 0.68950 - acc: 0.6566 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2300  | total loss: \u001b[1m\u001b[32m0.68806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2300 | loss: 0.68806 - acc: 0.6709 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2301  | total loss: \u001b[1m\u001b[32m0.68677\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2301 | loss: 0.68677 - acc: 0.6839 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2302  | total loss: \u001b[1m\u001b[32m0.69361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2302 | loss: 0.69361 - acc: 0.6155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2303  | total loss: \u001b[1m\u001b[32m0.69177\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2303 | loss: 0.69177 - acc: 0.6339 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2304  | total loss: \u001b[1m\u001b[32m0.69411\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2304 | loss: 0.69411 - acc: 0.6105 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2305  | total loss: \u001b[1m\u001b[32m0.69221\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2305 | loss: 0.69221 - acc: 0.6295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2306  | total loss: \u001b[1m\u001b[32m0.69051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2306 | loss: 0.69051 - acc: 0.6465 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2307  | total loss: \u001b[1m\u001b[32m0.68897\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2307 | loss: 0.68897 - acc: 0.6619 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2308  | total loss: \u001b[1m\u001b[32m0.68759\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2308 | loss: 0.68759 - acc: 0.6757 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2309  | total loss: \u001b[1m\u001b[32m0.68635\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2309 | loss: 0.68635 - acc: 0.6881 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2310  | total loss: \u001b[1m\u001b[32m0.68523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2310 | loss: 0.68523 - acc: 0.6993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2311  | total loss: \u001b[1m\u001b[32m0.68422\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2311 | loss: 0.68422 - acc: 0.7094 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2312  | total loss: \u001b[1m\u001b[32m0.68332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2312 | loss: 0.68332 - acc: 0.7184 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2313  | total loss: \u001b[1m\u001b[32m0.68250\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2313 | loss: 0.68250 - acc: 0.7266 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2314  | total loss: \u001b[1m\u001b[32m0.68177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2314 | loss: 0.68177 - acc: 0.7339 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2315  | total loss: \u001b[1m\u001b[32m0.68110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2315 | loss: 0.68110 - acc: 0.7405 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2316  | total loss: \u001b[1m\u001b[32m0.68051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2316 | loss: 0.68051 - acc: 0.7465 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2317  | total loss: \u001b[1m\u001b[32m0.67998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2317 | loss: 0.67998 - acc: 0.7518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2318  | total loss: \u001b[1m\u001b[32m0.68749\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2318 | loss: 0.68749 - acc: 0.6767 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2319  | total loss: \u001b[1m\u001b[32m0.68626\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2319 | loss: 0.68626 - acc: 0.6890 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2320  | total loss: \u001b[1m\u001b[32m0.68515\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2320 | loss: 0.68515 - acc: 0.7001 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2321  | total loss: \u001b[1m\u001b[32m0.68415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2321 | loss: 0.68415 - acc: 0.7101 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2322  | total loss: \u001b[1m\u001b[32m0.68325\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2322 | loss: 0.68325 - acc: 0.7191 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2323  | total loss: \u001b[1m\u001b[32m0.68244\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2323 | loss: 0.68244 - acc: 0.7272 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2324  | total loss: \u001b[1m\u001b[32m0.68171\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2324 | loss: 0.68171 - acc: 0.7344 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2325  | total loss: \u001b[1m\u001b[32m0.68106\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2325 | loss: 0.68106 - acc: 0.7410 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2326  | total loss: \u001b[1m\u001b[32m0.68047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2326 | loss: 0.68047 - acc: 0.7469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2327  | total loss: \u001b[1m\u001b[32m0.67994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2327 | loss: 0.67994 - acc: 0.7522 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2328  | total loss: \u001b[1m\u001b[32m0.67946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2328 | loss: 0.67946 - acc: 0.7570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2329  | total loss: \u001b[1m\u001b[32m0.67903\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2329 | loss: 0.67903 - acc: 0.7613 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2330  | total loss: \u001b[1m\u001b[32m0.67864\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2330 | loss: 0.67864 - acc: 0.7652 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2331  | total loss: \u001b[1m\u001b[32m0.67829\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2331 | loss: 0.67829 - acc: 0.7686 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2332  | total loss: \u001b[1m\u001b[32m0.67798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2332 | loss: 0.67798 - acc: 0.7718 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2333  | total loss: \u001b[1m\u001b[32m0.67770\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2333 | loss: 0.67770 - acc: 0.7746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2334  | total loss: \u001b[1m\u001b[32m0.67744\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2334 | loss: 0.67744 - acc: 0.7771 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2335  | total loss: \u001b[1m\u001b[32m0.67722\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2335 | loss: 0.67722 - acc: 0.7794 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2336  | total loss: \u001b[1m\u001b[32m0.67701\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2336 | loss: 0.67701 - acc: 0.7815 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2337  | total loss: \u001b[1m\u001b[32m0.67683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2337 | loss: 0.67683 - acc: 0.7833 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2338  | total loss: \u001b[1m\u001b[32m0.67666\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2338 | loss: 0.67666 - acc: 0.7850 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2339  | total loss: \u001b[1m\u001b[32m0.67651\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2339 | loss: 0.67651 - acc: 0.7865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2340  | total loss: \u001b[1m\u001b[32m0.67637\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2340 | loss: 0.67637 - acc: 0.7879 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2341  | total loss: \u001b[1m\u001b[32m0.67625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2341 | loss: 0.67625 - acc: 0.7891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2342  | total loss: \u001b[1m\u001b[32m0.67614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2342 | loss: 0.67614 - acc: 0.7902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2343  | total loss: \u001b[1m\u001b[32m0.67604\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2343 | loss: 0.67604 - acc: 0.7911 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2344  | total loss: \u001b[1m\u001b[32m0.67596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2344 | loss: 0.67596 - acc: 0.7920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2345  | total loss: \u001b[1m\u001b[32m0.67588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2345 | loss: 0.67588 - acc: 0.7928 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2346  | total loss: \u001b[1m\u001b[32m0.67580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2346 | loss: 0.67580 - acc: 0.7935 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2347  | total loss: \u001b[1m\u001b[32m0.67574\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2347 | loss: 0.67574 - acc: 0.7942 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2348  | total loss: \u001b[1m\u001b[32m0.67568\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2348 | loss: 0.67568 - acc: 0.7948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2349  | total loss: \u001b[1m\u001b[32m0.67563\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2349 | loss: 0.67563 - acc: 0.7953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2350  | total loss: \u001b[1m\u001b[32m0.68258\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2350 | loss: 0.68258 - acc: 0.7258 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2351  | total loss: \u001b[1m\u001b[32m0.68184\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2351 | loss: 0.68184 - acc: 0.7332 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2352  | total loss: \u001b[1m\u001b[32m0.68617\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2352 | loss: 0.68617 - acc: 0.6899 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2353  | total loss: \u001b[1m\u001b[32m0.68507\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2353 | loss: 0.68507 - acc: 0.7009 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2354  | total loss: \u001b[1m\u001b[32m0.68408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2354 | loss: 0.68408 - acc: 0.7108 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2355  | total loss: \u001b[1m\u001b[32m0.68319\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2355 | loss: 0.68319 - acc: 0.7197 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2356  | total loss: \u001b[1m\u001b[32m0.68838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2356 | loss: 0.68838 - acc: 0.6677 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2357  | total loss: \u001b[1m\u001b[32m0.68706\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2357 | loss: 0.68706 - acc: 0.6810 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2358  | total loss: \u001b[1m\u001b[32m0.69287\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2358 | loss: 0.69287 - acc: 0.6229 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2359  | total loss: \u001b[1m\u001b[32m0.69110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2359 | loss: 0.69110 - acc: 0.6406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2360  | total loss: \u001b[1m\u001b[32m0.69751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2360 | loss: 0.69751 - acc: 0.5765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2361  | total loss: \u001b[1m\u001b[32m0.69527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2361 | loss: 0.69527 - acc: 0.5989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2362  | total loss: \u001b[1m\u001b[32m0.69326\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2362 | loss: 0.69326 - acc: 0.6190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2363  | total loss: \u001b[1m\u001b[32m0.69145\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2363 | loss: 0.69145 - acc: 0.6371 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2364  | total loss: \u001b[1m\u001b[32m0.68982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2364 | loss: 0.68982 - acc: 0.6534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2365  | total loss: \u001b[1m\u001b[32m0.68835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2365 | loss: 0.68835 - acc: 0.6680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2366  | total loss: \u001b[1m\u001b[32m0.69404\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2366 | loss: 0.69404 - acc: 0.6112 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2367  | total loss: \u001b[1m\u001b[32m0.69215\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2367 | loss: 0.69215 - acc: 0.6301 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2368  | total loss: \u001b[1m\u001b[32m0.69845\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2368 | loss: 0.69845 - acc: 0.5671 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2369  | total loss: \u001b[1m\u001b[32m0.69612\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2369 | loss: 0.69612 - acc: 0.5904 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2370  | total loss: \u001b[1m\u001b[32m0.69402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2370 | loss: 0.69402 - acc: 0.6114 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2371  | total loss: \u001b[1m\u001b[32m0.69214\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2371 | loss: 0.69214 - acc: 0.6302 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2372  | total loss: \u001b[1m\u001b[32m0.69044\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2372 | loss: 0.69044 - acc: 0.6472 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2373  | total loss: \u001b[1m\u001b[32m0.68891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2373 | loss: 0.68891 - acc: 0.6625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2374  | total loss: \u001b[1m\u001b[32m0.68754\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2374 | loss: 0.68754 - acc: 0.6762 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2375  | total loss: \u001b[1m\u001b[32m0.68630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2375 | loss: 0.68630 - acc: 0.6886 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2376  | total loss: \u001b[1m\u001b[32m0.69048\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2376 | loss: 0.69048 - acc: 0.6297 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2377  | total loss: \u001b[1m\u001b[32m0.69048\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2377 | loss: 0.69048 - acc: 0.6468 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2378  | total loss: \u001b[1m\u001b[32m0.68895\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2378 | loss: 0.68895 - acc: 0.6621 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2379  | total loss: \u001b[1m\u001b[32m0.68757\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2379 | loss: 0.68757 - acc: 0.6759 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2380  | total loss: \u001b[1m\u001b[32m0.68633\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2380 | loss: 0.68633 - acc: 0.6883 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2381  | total loss: \u001b[1m\u001b[32m0.68521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2381 | loss: 0.68521 - acc: 0.6995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2382  | total loss: \u001b[1m\u001b[32m0.68421\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2382 | loss: 0.68421 - acc: 0.7095 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2383  | total loss: \u001b[1m\u001b[32m0.68330\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2383 | loss: 0.68330 - acc: 0.7186 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2384  | total loss: \u001b[1m\u001b[32m0.68249\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2384 | loss: 0.68249 - acc: 0.7267 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2385  | total loss: \u001b[1m\u001b[32m0.68175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2385 | loss: 0.68175 - acc: 0.7340 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2386  | total loss: \u001b[1m\u001b[32m0.68910\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2386 | loss: 0.68910 - acc: 0.6606 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2387  | total loss: \u001b[1m\u001b[32m0.68770\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2387 | loss: 0.68770 - acc: 0.6746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2388  | total loss: \u001b[1m\u001b[32m0.68645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2388 | loss: 0.68645 - acc: 0.6871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2389  | total loss: \u001b[1m\u001b[32m0.68532\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2389 | loss: 0.68532 - acc: 0.6984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2390  | total loss: \u001b[1m\u001b[32m0.68430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2390 | loss: 0.68430 - acc: 0.7086 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2391  | total loss: \u001b[1m\u001b[32m0.68339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2391 | loss: 0.68339 - acc: 0.7177 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2392  | total loss: \u001b[1m\u001b[32m0.68257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2392 | loss: 0.68257 - acc: 0.7259 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2393  | total loss: \u001b[1m\u001b[32m0.68716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2393 | loss: 0.68716 - acc: 0.7333 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2394  | total loss: \u001b[1m\u001b[32m0.68716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2394 | loss: 0.68716 - acc: 0.6800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2395  | total loss: \u001b[1m\u001b[32m0.68596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2395 | loss: 0.68596 - acc: 0.6920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2396  | total loss: \u001b[1m\u001b[32m0.68488\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2396 | loss: 0.68488 - acc: 0.7028 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2397  | total loss: \u001b[1m\u001b[32m0.68391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2397 | loss: 0.68391 - acc: 0.7125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2398  | total loss: \u001b[1m\u001b[32m0.69003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2398 | loss: 0.69003 - acc: 0.6513 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2399  | total loss: \u001b[1m\u001b[32m0.68854\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2399 | loss: 0.68854 - acc: 0.6661 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2400  | total loss: \u001b[1m\u001b[32m0.68721\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2400 | loss: 0.68721 - acc: 0.6795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2401  | total loss: \u001b[1m\u001b[32m0.68600\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2401 | loss: 0.68600 - acc: 0.6916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2402  | total loss: \u001b[1m\u001b[32m0.69192\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2402 | loss: 0.69192 - acc: 0.6324 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2403  | total loss: \u001b[1m\u001b[32m0.69024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2403 | loss: 0.69024 - acc: 0.6492 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2404  | total loss: \u001b[1m\u001b[32m0.68873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2404 | loss: 0.68873 - acc: 0.6643 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2405  | total loss: \u001b[1m\u001b[32m0.68738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2405 | loss: 0.68738 - acc: 0.6778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2406  | total loss: \u001b[1m\u001b[32m0.68615\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2406 | loss: 0.68615 - acc: 0.6901 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2407  | total loss: \u001b[1m\u001b[32m0.68505\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2407 | loss: 0.68505 - acc: 0.7010 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2408  | total loss: \u001b[1m\u001b[32m0.69106\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2408 | loss: 0.69106 - acc: 0.6409 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2409  | total loss: \u001b[1m\u001b[32m0.68947\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2409 | loss: 0.68947 - acc: 0.6568 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2410  | total loss: \u001b[1m\u001b[32m0.68804\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2410 | loss: 0.68804 - acc: 0.6712 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2411  | total loss: \u001b[1m\u001b[32m0.68675\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2411 | loss: 0.68675 - acc: 0.6840 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2412  | total loss: \u001b[1m\u001b[32m0.68559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2412 | loss: 0.68559 - acc: 0.6956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2413  | total loss: \u001b[1m\u001b[32m0.68455\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2413 | loss: 0.68455 - acc: 0.7061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2414  | total loss: \u001b[1m\u001b[32m0.68361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2414 | loss: 0.68361 - acc: 0.7155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2415  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2415 | loss: 0.68277 - acc: 0.7239 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2416  | total loss: \u001b[1m\u001b[32m0.69001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2416 | loss: 0.69001 - acc: 0.6515 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2417  | total loss: \u001b[1m\u001b[32m0.68852\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2417 | loss: 0.68852 - acc: 0.6664 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2418  | total loss: \u001b[1m\u001b[32m0.68718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2418 | loss: 0.68718 - acc: 0.6797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2419  | total loss: \u001b[1m\u001b[32m0.68598\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2419 | loss: 0.68598 - acc: 0.6918 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2420  | total loss: \u001b[1m\u001b[32m0.69290\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2420 | loss: 0.69290 - acc: 0.6226 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2421  | total loss: \u001b[1m\u001b[32m0.69113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2421 | loss: 0.69113 - acc: 0.6403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2422  | total loss: \u001b[1m\u001b[32m0.69653\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2422 | loss: 0.69653 - acc: 0.5863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2423  | total loss: \u001b[1m\u001b[32m0.69439\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2423 | loss: 0.69439 - acc: 0.6077 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2424  | total loss: \u001b[1m\u001b[32m0.69247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2424 | loss: 0.69247 - acc: 0.6269 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2425  | total loss: \u001b[1m\u001b[32m0.69074\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2425 | loss: 0.69074 - acc: 0.6442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2426  | total loss: \u001b[1m\u001b[32m0.68918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2426 | loss: 0.68918 - acc: 0.6598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2427  | total loss: \u001b[1m\u001b[32m0.68778\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2427 | loss: 0.68778 - acc: 0.6738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2428  | total loss: \u001b[1m\u001b[32m0.68652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2428 | loss: 0.68652 - acc: 0.6864 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2429  | total loss: \u001b[1m\u001b[32m0.68538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2429 | loss: 0.68538 - acc: 0.6978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2430  | total loss: \u001b[1m\u001b[32m0.69236\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2430 | loss: 0.69236 - acc: 0.6280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2431  | total loss: \u001b[1m\u001b[32m0.69064\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2431 | loss: 0.69064 - acc: 0.6452 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2432  | total loss: \u001b[1m\u001b[32m0.68909\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2432 | loss: 0.68909 - acc: 0.6607 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2433  | total loss: \u001b[1m\u001b[32m0.68770\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2433 | loss: 0.68770 - acc: 0.6746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2434  | total loss: \u001b[1m\u001b[32m0.68644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2434 | loss: 0.68644 - acc: 0.6872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2435  | total loss: \u001b[1m\u001b[32m0.68531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2435 | loss: 0.68531 - acc: 0.6984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2436  | total loss: \u001b[1m\u001b[32m0.68430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2436 | loss: 0.68430 - acc: 0.7086 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2437  | total loss: \u001b[1m\u001b[32m0.68339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2437 | loss: 0.68339 - acc: 0.7177 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2438  | total loss: \u001b[1m\u001b[32m0.68256\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2438 | loss: 0.68256 - acc: 0.7260 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2439  | total loss: \u001b[1m\u001b[32m0.68182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2439 | loss: 0.68182 - acc: 0.7334 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2440  | total loss: \u001b[1m\u001b[32m0.68716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2440 | loss: 0.68716 - acc: 0.6800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2441  | total loss: \u001b[1m\u001b[32m0.68596\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2441 | loss: 0.68596 - acc: 0.6920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2442  | total loss: \u001b[1m\u001b[32m0.68488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2442 | loss: 0.68488 - acc: 0.7028 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2443  | total loss: \u001b[1m\u001b[32m0.68390\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2443 | loss: 0.68390 - acc: 0.7125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2444  | total loss: \u001b[1m\u001b[32m0.68303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2444 | loss: 0.68303 - acc: 0.7213 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2445  | total loss: \u001b[1m\u001b[32m0.68224\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2445 | loss: 0.68224 - acc: 0.7292 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2446  | total loss: \u001b[1m\u001b[32m0.68753\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2446 | loss: 0.68753 - acc: 0.6762 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2447  | total loss: \u001b[1m\u001b[32m0.68630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2447 | loss: 0.68630 - acc: 0.6886 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2448  | total loss: \u001b[1m\u001b[32m0.68518\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2448 | loss: 0.68518 - acc: 0.6998 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2449  | total loss: \u001b[1m\u001b[32m0.68418\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2449 | loss: 0.68418 - acc: 0.7098 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2450  | total loss: \u001b[1m\u001b[32m0.68328\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2450 | loss: 0.68328 - acc: 0.7188 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2451  | total loss: \u001b[1m\u001b[32m0.68247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2451 | loss: 0.68247 - acc: 0.7269 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2452  | total loss: \u001b[1m\u001b[32m0.68174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2452 | loss: 0.68174 - acc: 0.7342 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2453  | total loss: \u001b[1m\u001b[32m0.68108\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2453 | loss: 0.68108 - acc: 0.7408 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2454  | total loss: \u001b[1m\u001b[32m0.68749\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2454 | loss: 0.68749 - acc: 0.6767 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2455  | total loss: \u001b[1m\u001b[32m0.68625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2455 | loss: 0.68625 - acc: 0.6891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2456  | total loss: \u001b[1m\u001b[32m0.68514\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2456 | loss: 0.68514 - acc: 0.7001 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2457  | total loss: \u001b[1m\u001b[32m0.68415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2457 | loss: 0.68415 - acc: 0.7101 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2458  | total loss: \u001b[1m\u001b[32m0.68325\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2458 | loss: 0.68325 - acc: 0.7191 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2459  | total loss: \u001b[1m\u001b[32m0.68244\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2459 | loss: 0.68244 - acc: 0.7272 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2460  | total loss: \u001b[1m\u001b[32m0.68671\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2460 | loss: 0.68671 - acc: 0.6845 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2461  | total loss: \u001b[1m\u001b[32m0.68555\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2461 | loss: 0.68555 - acc: 0.6960 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2462  | total loss: \u001b[1m\u001b[32m0.69252\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2462 | loss: 0.69252 - acc: 0.6264 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2463  | total loss: \u001b[1m\u001b[32m0.69078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2463 | loss: 0.69078 - acc: 0.6438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2464  | total loss: \u001b[1m\u001b[32m0.68922\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2464 | loss: 0.68922 - acc: 0.6594 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2465  | total loss: \u001b[1m\u001b[32m0.68781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2465 | loss: 0.68781 - acc: 0.6735 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2466  | total loss: \u001b[1m\u001b[32m0.68655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2466 | loss: 0.68655 - acc: 0.6861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2467  | total loss: \u001b[1m\u001b[32m0.68541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2467 | loss: 0.68541 - acc: 0.6975 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2468  | total loss: \u001b[1m\u001b[32m0.69238\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2468 | loss: 0.69238 - acc: 0.6278 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2469  | total loss: \u001b[1m\u001b[32m0.69066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2469 | loss: 0.69066 - acc: 0.6450 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2470  | total loss: \u001b[1m\u001b[32m0.69711\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2470 | loss: 0.69711 - acc: 0.5805 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2471  | total loss: \u001b[1m\u001b[32m0.69492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2471 | loss: 0.69492 - acc: 0.6024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2472  | total loss: \u001b[1m\u001b[32m0.69994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2472 | loss: 0.69994 - acc: 0.5522 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2473  | total loss: \u001b[1m\u001b[32m0.69746\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2473 | loss: 0.69746 - acc: 0.5770 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2474  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2474 | loss: 0.69523 - acc: 0.5993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2475  | total loss: \u001b[1m\u001b[32m0.69322\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2475 | loss: 0.69322 - acc: 0.6193 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2476  | total loss: \u001b[1m\u001b[32m0.69842\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2476 | loss: 0.69842 - acc: 0.5674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2477  | total loss: \u001b[1m\u001b[32m0.69609\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2477 | loss: 0.69609 - acc: 0.5907 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2478  | total loss: \u001b[1m\u001b[32m0.69400\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2478 | loss: 0.69400 - acc: 0.6116 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2479  | total loss: \u001b[1m\u001b[32m0.69211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2479 | loss: 0.69211 - acc: 0.6304 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2480  | total loss: \u001b[1m\u001b[32m0.68889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2480 | loss: 0.68889 - acc: 0.6474 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2481  | total loss: \u001b[1m\u001b[32m0.68889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2481 | loss: 0.68889 - acc: 0.6627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2482  | total loss: \u001b[1m\u001b[32m0.68752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2482 | loss: 0.68752 - acc: 0.6764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2483  | total loss: \u001b[1m\u001b[32m0.68628\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2483 | loss: 0.68628 - acc: 0.6888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2484  | total loss: \u001b[1m\u001b[32m0.68517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2484 | loss: 0.68517 - acc: 0.6999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2485  | total loss: \u001b[1m\u001b[32m0.68417\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2485 | loss: 0.68417 - acc: 0.7099 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2486  | total loss: \u001b[1m\u001b[32m0.69027\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2486 | loss: 0.69027 - acc: 0.6489 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2487  | total loss: \u001b[1m\u001b[32m0.68876\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2487 | loss: 0.68876 - acc: 0.6640 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2488  | total loss: \u001b[1m\u001b[32m0.69440\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2488 | loss: 0.69440 - acc: 0.6076 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2489  | total loss: \u001b[1m\u001b[32m0.69247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2489 | loss: 0.69247 - acc: 0.6268 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2490  | total loss: \u001b[1m\u001b[32m0.69074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2490 | loss: 0.69074 - acc: 0.6442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2491  | total loss: \u001b[1m\u001b[32m0.68918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2491 | loss: 0.68918 - acc: 0.6597 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2492  | total loss: \u001b[1m\u001b[32m0.68778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2492 | loss: 0.68778 - acc: 0.6738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2493  | total loss: \u001b[1m\u001b[32m0.68652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2493 | loss: 0.68652 - acc: 0.6864 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2494  | total loss: \u001b[1m\u001b[32m0.68538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2494 | loss: 0.68538 - acc: 0.6978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2495  | total loss: \u001b[1m\u001b[32m0.68436\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2495 | loss: 0.68436 - acc: 0.7080 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2496  | total loss: \u001b[1m\u001b[32m0.68344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2496 | loss: 0.68344 - acc: 0.7172 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2497  | total loss: \u001b[1m\u001b[32m0.68261\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2497 | loss: 0.68261 - acc: 0.7255 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2498  | total loss: \u001b[1m\u001b[32m0.68787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2498 | loss: 0.68787 - acc: 0.6729 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2499  | total loss: \u001b[1m\u001b[32m0.68660\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2499 | loss: 0.68660 - acc: 0.6856 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2500  | total loss: \u001b[1m\u001b[32m0.69245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2500 | loss: 0.69245 - acc: 0.6271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2501  | total loss: \u001b[1m\u001b[32m0.69072\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2501 | loss: 0.69072 - acc: 0.6444 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2502  | total loss: \u001b[1m\u001b[32m0.68917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2502 | loss: 0.68917 - acc: 0.6599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2503  | total loss: \u001b[1m\u001b[32m0.68777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2503 | loss: 0.68777 - acc: 0.6739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2504  | total loss: \u001b[1m\u001b[32m0.68651\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2504 | loss: 0.68651 - acc: 0.6865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2505  | total loss: \u001b[1m\u001b[32m0.68537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2505 | loss: 0.68537 - acc: 0.6979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2506  | total loss: \u001b[1m\u001b[32m0.68435\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2506 | loss: 0.68435 - acc: 0.7081 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2507  | total loss: \u001b[1m\u001b[32m0.68343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2507 | loss: 0.68343 - acc: 0.7173 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2508  | total loss: \u001b[1m\u001b[32m0.68260\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2508 | loss: 0.68260 - acc: 0.7256 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2509  | total loss: \u001b[1m\u001b[32m0.68186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2509 | loss: 0.68186 - acc: 0.7330 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2510  | total loss: \u001b[1m\u001b[32m0.68719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2510 | loss: 0.68719 - acc: 0.6797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2511  | total loss: \u001b[1m\u001b[32m0.68599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2511 | loss: 0.68599 - acc: 0.6917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2512  | total loss: \u001b[1m\u001b[32m0.68490\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2512 | loss: 0.68490 - acc: 0.7026 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2513  | total loss: \u001b[1m\u001b[32m0.68393\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2513 | loss: 0.68393 - acc: 0.7123 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2514  | total loss: \u001b[1m\u001b[32m0.68305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2514 | loss: 0.68305 - acc: 0.7211 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2515  | total loss: \u001b[1m\u001b[32m0.68226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2515 | loss: 0.68226 - acc: 0.7290 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2516  | total loss: \u001b[1m\u001b[32m0.68155\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 2516 | loss: 0.68155 - acc: 0.7361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2517  | total loss: \u001b[1m\u001b[32m0.68091\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2517 | loss: 0.68091 - acc: 0.7425 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2518  | total loss: \u001b[1m\u001b[32m0.68034\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2518 | loss: 0.68034 - acc: 0.7482 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2519  | total loss: \u001b[1m\u001b[32m0.67982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2519 | loss: 0.67982 - acc: 0.7534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2520  | total loss: \u001b[1m\u001b[32m0.67935\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2520 | loss: 0.67935 - acc: 0.7581 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2521  | total loss: \u001b[1m\u001b[32m0.67893\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2521 | loss: 0.67893 - acc: 0.7622 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2522  | total loss: \u001b[1m\u001b[32m0.67856\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2522 | loss: 0.67856 - acc: 0.7660 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2523  | total loss: \u001b[1m\u001b[32m0.67822\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2523 | loss: 0.67822 - acc: 0.7694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2524  | total loss: \u001b[1m\u001b[32m0.67791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2524 | loss: 0.67791 - acc: 0.7725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2525  | total loss: \u001b[1m\u001b[32m0.67764\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2525 | loss: 0.67764 - acc: 0.7752 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2526  | total loss: \u001b[1m\u001b[32m0.67739\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2526 | loss: 0.67739 - acc: 0.7777 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2527  | total loss: \u001b[1m\u001b[32m0.67717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2527 | loss: 0.67717 - acc: 0.7799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2528  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2528 | loss: 0.68196 - acc: 0.7319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2529  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2529 | loss: 0.68128 - acc: 0.7387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2530  | total loss: \u001b[1m\u001b[32m0.68067\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2530 | loss: 0.68067 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2531  | total loss: \u001b[1m\u001b[32m0.68012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2531 | loss: 0.68012 - acc: 0.7504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2532  | total loss: \u001b[1m\u001b[32m0.68762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2532 | loss: 0.68762 - acc: 0.6753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2533  | total loss: \u001b[1m\u001b[32m0.68638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2533 | loss: 0.68638 - acc: 0.6878 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2534  | total loss: \u001b[1m\u001b[32m0.68526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2534 | loss: 0.68526 - acc: 0.6990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2535  | total loss: \u001b[1m\u001b[32m0.68425\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2535 | loss: 0.68425 - acc: 0.7091 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2536  | total loss: \u001b[1m\u001b[32m0.68934\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2536 | loss: 0.68934 - acc: 0.6582 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2537  | total loss: \u001b[1m\u001b[32m0.68792\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2537 | loss: 0.68792 - acc: 0.6724 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2538  | total loss: \u001b[1m\u001b[32m0.69364\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2538 | loss: 0.69364 - acc: 0.6152 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2539  | total loss: \u001b[1m\u001b[32m0.69179\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2539 | loss: 0.69179 - acc: 0.6336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2540  | total loss: \u001b[1m\u001b[32m0.69013\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2540 | loss: 0.69013 - acc: 0.6503 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2541  | total loss: \u001b[1m\u001b[32m0.68863\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2541 | loss: 0.68863 - acc: 0.6652 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2542  | total loss: \u001b[1m\u001b[32m0.68729\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2542 | loss: 0.68729 - acc: 0.6787 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2543  | total loss: \u001b[1m\u001b[32m0.68607\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2543 | loss: 0.68607 - acc: 0.6909 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2544  | total loss: \u001b[1m\u001b[32m0.68498\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2544 | loss: 0.68498 - acc: 0.7018 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2545  | total loss: \u001b[1m\u001b[32m0.68400\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2545 | loss: 0.68400 - acc: 0.7116 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2546  | total loss: \u001b[1m\u001b[32m0.68312\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2546 | loss: 0.68312 - acc: 0.7204 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2547  | total loss: \u001b[1m\u001b[32m0.68232\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2547 | loss: 0.68232 - acc: 0.7284 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2548  | total loss: \u001b[1m\u001b[32m0.68160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2548 | loss: 0.68160 - acc: 0.7355 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2549  | total loss: \u001b[1m\u001b[32m0.68096\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2549 | loss: 0.68096 - acc: 0.7420 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2550  | total loss: \u001b[1m\u001b[32m0.68038\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2550 | loss: 0.68038 - acc: 0.7478 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2551  | total loss: \u001b[1m\u001b[32m0.67986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2551 | loss: 0.67986 - acc: 0.7530 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2552  | total loss: \u001b[1m\u001b[32m0.68639\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2552 | loss: 0.68639 - acc: 0.6877 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2553  | total loss: \u001b[1m\u001b[32m0.68526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2553 | loss: 0.68526 - acc: 0.6989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2554  | total loss: \u001b[1m\u001b[32m0.69125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2554 | loss: 0.69125 - acc: 0.6390 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2555  | total loss: \u001b[1m\u001b[32m0.68964\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2555 | loss: 0.68964 - acc: 0.6551 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2556  | total loss: \u001b[1m\u001b[32m0.69420\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2556 | loss: 0.69420 - acc: 0.6096 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2557  | total loss: \u001b[1m\u001b[32m0.69229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2557 | loss: 0.69229 - acc: 0.6287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2558  | total loss: \u001b[1m\u001b[32m0.69058\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2558 | loss: 0.69058 - acc: 0.6458 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2559  | total loss: \u001b[1m\u001b[32m0.68904\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2559 | loss: 0.68904 - acc: 0.6612 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2560  | total loss: \u001b[1m\u001b[32m0.68765\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2560 | loss: 0.68765 - acc: 0.6751 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2561  | total loss: \u001b[1m\u001b[32m0.68640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2561 | loss: 0.68640 - acc: 0.6876 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2562  | total loss: \u001b[1m\u001b[32m0.68528\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2562 | loss: 0.68528 - acc: 0.6988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2563  | total loss: \u001b[1m\u001b[32m0.68426\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2563 | loss: 0.68426 - acc: 0.7089 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2564  | total loss: \u001b[1m\u001b[32m0.68335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2564 | loss: 0.68335 - acc: 0.7181 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2565  | total loss: \u001b[1m\u001b[32m0.68253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2565 | loss: 0.68253 - acc: 0.7262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2566  | total loss: \u001b[1m\u001b[32m0.68180\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2566 | loss: 0.68180 - acc: 0.7336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2567  | total loss: \u001b[1m\u001b[32m0.68113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2567 | loss: 0.68113 - acc: 0.7403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2568  | total loss: \u001b[1m\u001b[32m0.68054\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2568 | loss: 0.68054 - acc: 0.7462 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2569  | total loss: \u001b[1m\u001b[32m0.68000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2569 | loss: 0.68000 - acc: 0.7516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2570  | total loss: \u001b[1m\u001b[32m0.67951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2570 | loss: 0.67951 - acc: 0.7564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2571  | total loss: \u001b[1m\u001b[32m0.67908\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2571 | loss: 0.67908 - acc: 0.7608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2572  | total loss: \u001b[1m\u001b[32m0.67869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2572 | loss: 0.67869 - acc: 0.7647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2573  | total loss: \u001b[1m\u001b[32m0.67833\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2573 | loss: 0.67833 - acc: 0.7683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2574  | total loss: \u001b[1m\u001b[32m0.67802\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2574 | loss: 0.67802 - acc: 0.7714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2575  | total loss: \u001b[1m\u001b[32m0.67773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2575 | loss: 0.67773 - acc: 0.7743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2576  | total loss: \u001b[1m\u001b[32m0.68447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2576 | loss: 0.68447 - acc: 0.7069 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2577  | total loss: \u001b[1m\u001b[32m0.68354\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2577 | loss: 0.68354 - acc: 0.7162 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2578  | total loss: \u001b[1m\u001b[32m0.68970\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2578 | loss: 0.68970 - acc: 0.6546 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.68825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2579 | loss: 0.68825 - acc: 0.6691 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.68694\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2580 | loss: 0.68694 - acc: 0.6822 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2581  | total loss: \u001b[1m\u001b[32m0.68576\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2581 | loss: 0.68576 - acc: 0.6940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2582  | total loss: \u001b[1m\u001b[32m0.68470\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2582 | loss: 0.68470 - acc: 0.7046 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2583  | total loss: \u001b[1m\u001b[32m0.68375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2583 | loss: 0.68375 - acc: 0.7141 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2584  | total loss: \u001b[1m\u001b[32m0.68289\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2584 | loss: 0.68289 - acc: 0.7227 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2585  | total loss: \u001b[1m\u001b[32m0.68212\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2585 | loss: 0.68212 - acc: 0.7304 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2586  | total loss: \u001b[1m\u001b[32m0.68142\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2586 | loss: 0.68142 - acc: 0.7374 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2587  | total loss: \u001b[1m\u001b[32m0.68079\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2587 | loss: 0.68079 - acc: 0.7437 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2588  | total loss: \u001b[1m\u001b[32m0.68023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2588 | loss: 0.68023 - acc: 0.7493 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2589  | total loss: \u001b[1m\u001b[32m0.67972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2589 | loss: 0.67972 - acc: 0.7544 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2590  | total loss: \u001b[1m\u001b[32m0.68627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2590 | loss: 0.68627 - acc: 0.6889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2591  | total loss: \u001b[1m\u001b[32m0.68516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2591 | loss: 0.68516 - acc: 0.7000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2592  | total loss: \u001b[1m\u001b[32m0.68416\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2592 | loss: 0.68416 - acc: 0.7100 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2593  | total loss: \u001b[1m\u001b[32m0.68326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2593 | loss: 0.68326 - acc: 0.7190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2594  | total loss: \u001b[1m\u001b[32m0.68245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2594 | loss: 0.68245 - acc: 0.7271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2595  | total loss: \u001b[1m\u001b[32m0.68172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2595 | loss: 0.68172 - acc: 0.7344 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2596  | total loss: \u001b[1m\u001b[32m0.68106\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2596 | loss: 0.68106 - acc: 0.7410 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2597  | total loss: \u001b[1m\u001b[32m0.68047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2597 | loss: 0.68047 - acc: 0.7469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2598  | total loss: \u001b[1m\u001b[32m0.67994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2598 | loss: 0.67994 - acc: 0.7522 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2599  | total loss: \u001b[1m\u001b[32m0.67946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2599 | loss: 0.67946 - acc: 0.7570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2600  | total loss: \u001b[1m\u001b[32m0.67903\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2600 | loss: 0.67903 - acc: 0.7613 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2601  | total loss: \u001b[1m\u001b[32m0.67864\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2601 | loss: 0.67864 - acc: 0.7651 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2602  | total loss: \u001b[1m\u001b[32m0.67830\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2602 | loss: 0.67830 - acc: 0.7686 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2603  | total loss: \u001b[1m\u001b[32m0.67798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2603 | loss: 0.67798 - acc: 0.7718 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2604  | total loss: \u001b[1m\u001b[32m0.68570\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2604 | loss: 0.68570 - acc: 0.6946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2605  | total loss: \u001b[1m\u001b[32m0.68465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2605 | loss: 0.68465 - acc: 0.7051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2606  | total loss: \u001b[1m\u001b[32m0.68370\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2606 | loss: 0.68370 - acc: 0.7146 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2607  | total loss: \u001b[1m\u001b[32m0.68284\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2607 | loss: 0.68284 - acc: 0.7232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2608  | total loss: \u001b[1m\u001b[32m0.68207\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2608 | loss: 0.68207 - acc: 0.7308 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2609  | total loss: \u001b[1m\u001b[32m0.68138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2609 | loss: 0.68138 - acc: 0.7378 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2610  | total loss: \u001b[1m\u001b[32m0.68076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2610 | loss: 0.68076 - acc: 0.7440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2611  | total loss: \u001b[1m\u001b[32m0.68020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2611 | loss: 0.68020 - acc: 0.7496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2612  | total loss: \u001b[1m\u001b[32m0.68570\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2612 | loss: 0.68570 - acc: 0.6946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2613  | total loss: \u001b[1m\u001b[32m0.68464\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2613 | loss: 0.68464 - acc: 0.7052 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2614  | total loss: \u001b[1m\u001b[32m0.68369\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2614 | loss: 0.68369 - acc: 0.7146 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2615  | total loss: \u001b[1m\u001b[32m0.68284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2615 | loss: 0.68284 - acc: 0.7232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2616  | total loss: \u001b[1m\u001b[32m0.68907\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2616 | loss: 0.68907 - acc: 0.6609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2617  | total loss: \u001b[1m\u001b[32m0.68768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2617 | loss: 0.68768 - acc: 0.6748 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2618  | total loss: \u001b[1m\u001b[32m0.69243\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2618 | loss: 0.69243 - acc: 0.6273 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2619  | total loss: \u001b[1m\u001b[32m0.69070\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2619 | loss: 0.69070 - acc: 0.6446 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2620  | total loss: \u001b[1m\u001b[32m0.68915\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2620 | loss: 0.68915 - acc: 0.6601 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2621  | total loss: \u001b[1m\u001b[32m0.68775\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2621 | loss: 0.68775 - acc: 0.6741 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2622  | total loss: \u001b[1m\u001b[32m0.68649\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2622 | loss: 0.68649 - acc: 0.6867 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2623  | total loss: \u001b[1m\u001b[32m0.68536\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2623 | loss: 0.68536 - acc: 0.6980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2624  | total loss: \u001b[1m\u001b[32m0.69134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2624 | loss: 0.69134 - acc: 0.6382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2625  | total loss: \u001b[1m\u001b[32m0.68972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2625 | loss: 0.68972 - acc: 0.6544 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2626  | total loss: \u001b[1m\u001b[32m0.68826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2626 | loss: 0.68826 - acc: 0.6690 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2627  | total loss: \u001b[1m\u001b[32m0.68695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2627 | loss: 0.68695 - acc: 0.6821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2628  | total loss: \u001b[1m\u001b[32m0.68577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2628 | loss: 0.68577 - acc: 0.6939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2629  | total loss: \u001b[1m\u001b[32m0.68471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2629 | loss: 0.68471 - acc: 0.7045 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2630  | total loss: \u001b[1m\u001b[32m0.69076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2630 | loss: 0.69076 - acc: 0.6440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2631  | total loss: \u001b[1m\u001b[32m0.68920\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2631 | loss: 0.68920 - acc: 0.6596 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2632  | total loss: \u001b[1m\u001b[32m0.68779\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2632 | loss: 0.68779 - acc: 0.6737 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2633  | total loss: \u001b[1m\u001b[32m0.68653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2633 | loss: 0.68653 - acc: 0.6863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2634  | total loss: \u001b[1m\u001b[32m0.68539\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2634 | loss: 0.68539 - acc: 0.6977 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2635  | total loss: \u001b[1m\u001b[32m0.68437\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2635 | loss: 0.68437 - acc: 0.7079 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2636  | total loss: \u001b[1m\u001b[32m0.69145\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2636 | loss: 0.69145 - acc: 0.6371 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2637  | total loss: \u001b[1m\u001b[32m0.68982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2637 | loss: 0.68982 - acc: 0.6534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2638  | total loss: \u001b[1m\u001b[32m0.68835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2638 | loss: 0.68835 - acc: 0.6681 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2639  | total loss: \u001b[1m\u001b[32m0.68703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2639 | loss: 0.68703 - acc: 0.6813 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2640  | total loss: \u001b[1m\u001b[32m0.69285\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2640 | loss: 0.69285 - acc: 0.6231 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2641  | total loss: \u001b[1m\u001b[32m0.69108\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2641 | loss: 0.69108 - acc: 0.6408 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2642  | total loss: \u001b[1m\u001b[32m0.68949\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2642 | loss: 0.68949 - acc: 0.6567 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2643  | total loss: \u001b[1m\u001b[32m0.68805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2643 | loss: 0.68805 - acc: 0.6711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2644  | total loss: \u001b[1m\u001b[32m0.68676\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2644 | loss: 0.68676 - acc: 0.6840 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2645  | total loss: \u001b[1m\u001b[32m0.68560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2645 | loss: 0.68560 - acc: 0.6956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2646  | total loss: \u001b[1m\u001b[32m0.68456\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2646 | loss: 0.68456 - acc: 0.7060 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2647  | total loss: \u001b[1m\u001b[32m0.68362\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2647 | loss: 0.68362 - acc: 0.7154 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2648  | total loss: \u001b[1m\u001b[32m0.69077\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2648 | loss: 0.69077 - acc: 0.6439 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2649  | total loss: \u001b[1m\u001b[32m0.68921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2649 | loss: 0.68921 - acc: 0.6595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2650  | total loss: \u001b[1m\u001b[32m0.69481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2650 | loss: 0.69481 - acc: 0.6035 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2651  | total loss: \u001b[1m\u001b[32m0.69284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2651 | loss: 0.69284 - acc: 0.6232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2652  | total loss: \u001b[1m\u001b[32m0.69607\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2652 | loss: 0.69607 - acc: 0.5909 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2653  | total loss: \u001b[1m\u001b[32m0.69398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2653 | loss: 0.69398 - acc: 0.6118 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2654  | total loss: \u001b[1m\u001b[32m0.69210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2654 | loss: 0.69210 - acc: 0.6306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2655  | total loss: \u001b[1m\u001b[32m0.69041\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2655 | loss: 0.69041 - acc: 0.6475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2656  | total loss: \u001b[1m\u001b[32m0.68888\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2656 | loss: 0.68888 - acc: 0.6628 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2657  | total loss: \u001b[1m\u001b[32m0.68751\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2657 | loss: 0.68751 - acc: 0.6765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2658  | total loss: \u001b[1m\u001b[32m0.68627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2658 | loss: 0.68627 - acc: 0.6889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2659  | total loss: \u001b[1m\u001b[32m0.68516\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2659 | loss: 0.68516 - acc: 0.7000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2660  | total loss: \u001b[1m\u001b[32m0.69116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2660 | loss: 0.69116 - acc: 0.6400 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2661  | total loss: \u001b[1m\u001b[32m0.68956\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2661 | loss: 0.68956 - acc: 0.6560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2662  | total loss: \u001b[1m\u001b[32m0.68812\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2662 | loss: 0.68812 - acc: 0.6704 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2663  | total loss: \u001b[1m\u001b[32m0.68682\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2663 | loss: 0.68682 - acc: 0.6833 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2664  | total loss: \u001b[1m\u001b[32m0.69366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2664 | loss: 0.69366 - acc: 0.6150 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2665  | total loss: \u001b[1m\u001b[32m0.69181\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2665 | loss: 0.69181 - acc: 0.6335 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2666  | total loss: \u001b[1m\u001b[32m0.69814\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2666 | loss: 0.69814 - acc: 0.5702 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2667  | total loss: \u001b[1m\u001b[32m0.69584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2667 | loss: 0.69584 - acc: 0.5931 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2668  | total loss: \u001b[1m\u001b[32m0.69978\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2668 | loss: 0.69978 - acc: 0.5538 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2669  | total loss: \u001b[1m\u001b[32m0.69731\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2669 | loss: 0.69731 - acc: 0.5784 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2670  | total loss: \u001b[1m\u001b[32m0.70110\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2670 | loss: 0.70110 - acc: 0.5406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2671  | total loss: \u001b[1m\u001b[32m0.69851\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2671 | loss: 0.69851 - acc: 0.5665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2672  | total loss: \u001b[1m\u001b[32m0.70217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2672 | loss: 0.70217 - acc: 0.5299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2673  | total loss: \u001b[1m\u001b[32m0.69947\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2673 | loss: 0.69947 - acc: 0.5569 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2674  | total loss: \u001b[1m\u001b[32m0.69704\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2674 | loss: 0.69704 - acc: 0.5812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2675  | total loss: \u001b[1m\u001b[32m0.69485\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2675 | loss: 0.69485 - acc: 0.6031 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2676  | total loss: \u001b[1m\u001b[32m0.69288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2676 | loss: 0.69288 - acc: 0.6228 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2677  | total loss: \u001b[1m\u001b[32m0.69111\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2677 | loss: 0.69111 - acc: 0.6405 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2678  | total loss: \u001b[1m\u001b[32m0.68951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2678 | loss: 0.68951 - acc: 0.6564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2679  | total loss: \u001b[1m\u001b[32m0.68808\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2679 | loss: 0.68808 - acc: 0.6708 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2680  | total loss: \u001b[1m\u001b[32m0.68679\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2680 | loss: 0.68679 - acc: 0.6837 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2681  | total loss: \u001b[1m\u001b[32m0.68562\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2681 | loss: 0.68562 - acc: 0.6954 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2682  | total loss: \u001b[1m\u001b[32m0.68958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2682 | loss: 0.68958 - acc: 0.6558 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2683  | total loss: \u001b[1m\u001b[32m0.68814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2683 | loss: 0.68814 - acc: 0.6702 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2684  | total loss: \u001b[1m\u001b[32m0.68684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2684 | loss: 0.68684 - acc: 0.6832 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2685  | total loss: \u001b[1m\u001b[32m0.68567\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2685 | loss: 0.68567 - acc: 0.6949 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2686  | total loss: \u001b[1m\u001b[32m0.69162\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2686 | loss: 0.69162 - acc: 0.6354 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2687  | total loss: \u001b[1m\u001b[32m0.68997\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2687 | loss: 0.68997 - acc: 0.6519 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2688  | total loss: \u001b[1m\u001b[32m0.68849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2688 | loss: 0.68849 - acc: 0.6667 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2689  | total loss: \u001b[1m\u001b[32m0.68716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2689 | loss: 0.68716 - acc: 0.6800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2690  | total loss: \u001b[1m\u001b[32m0.68596\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2690 | loss: 0.68596 - acc: 0.6920 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2691  | total loss: \u001b[1m\u001b[32m0.68488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2691 | loss: 0.68488 - acc: 0.7028 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2692  | total loss: \u001b[1m\u001b[32m0.68391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2692 | loss: 0.68391 - acc: 0.7125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2693  | total loss: \u001b[1m\u001b[32m0.68303\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2693 | loss: 0.68303 - acc: 0.7213 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2694  | total loss: \u001b[1m\u001b[32m0.68224\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2694 | loss: 0.68224 - acc: 0.7291 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2695  | total loss: \u001b[1m\u001b[32m0.68154\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2695 | loss: 0.68154 - acc: 0.7362 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2696  | total loss: \u001b[1m\u001b[32m0.68790\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2696 | loss: 0.68790 - acc: 0.6726 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2697  | total loss: \u001b[1m\u001b[32m0.68662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2697 | loss: 0.68662 - acc: 0.6853 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2698  | total loss: \u001b[1m\u001b[32m0.69348\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2698 | loss: 0.69348 - acc: 0.6168 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2699  | total loss: \u001b[1m\u001b[32m0.69165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2699 | loss: 0.69165 - acc: 0.6351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2700  | total loss: \u001b[1m\u001b[32m0.69700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2700 | loss: 0.69700 - acc: 0.5816 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2701  | total loss: \u001b[1m\u001b[32m0.69481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2701 | loss: 0.69481 - acc: 0.6035 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2702  | total loss: \u001b[1m\u001b[32m0.70085\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2702 | loss: 0.70085 - acc: 0.5431 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2703  | total loss: \u001b[1m\u001b[32m0.69828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2703 | loss: 0.69828 - acc: 0.5688 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2704  | total loss: \u001b[1m\u001b[32m0.70397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2704 | loss: 0.70397 - acc: 0.5119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2705  | total loss: \u001b[1m\u001b[32m0.70109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2705 | loss: 0.70109 - acc: 0.5407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2706  | total loss: \u001b[1m\u001b[32m0.69849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2706 | loss: 0.69849 - acc: 0.5667 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2707  | total loss: \u001b[1m\u001b[32m0.69616\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2707 | loss: 0.69616 - acc: 0.5900 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2708  | total loss: \u001b[1m\u001b[32m0.69406\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2708 | loss: 0.69406 - acc: 0.6110 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2709  | total loss: \u001b[1m\u001b[32m0.69217\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2709 | loss: 0.69217 - acc: 0.6299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2710  | total loss: \u001b[1m\u001b[32m0.69047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2710 | loss: 0.69047 - acc: 0.6469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2711  | total loss: \u001b[1m\u001b[32m0.68894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2711 | loss: 0.68894 - acc: 0.6622 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2712  | total loss: \u001b[1m\u001b[32m0.68756\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2712 | loss: 0.68756 - acc: 0.6760 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2713  | total loss: \u001b[1m\u001b[32m0.68632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2713 | loss: 0.68632 - acc: 0.6884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2714  | total loss: \u001b[1m\u001b[32m0.68520\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2714 | loss: 0.68520 - acc: 0.6996 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2715  | total loss: \u001b[1m\u001b[32m0.68420\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2715 | loss: 0.68420 - acc: 0.7096 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2716  | total loss: \u001b[1m\u001b[32m0.68330\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2716 | loss: 0.68330 - acc: 0.7186 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2717  | total loss: \u001b[1m\u001b[32m0.68248\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2717 | loss: 0.68248 - acc: 0.7268 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2718  | total loss: \u001b[1m\u001b[32m0.68175\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2718 | loss: 0.68175 - acc: 0.7341 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2719  | total loss: \u001b[1m\u001b[32m0.68109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2719 | loss: 0.68109 - acc: 0.7407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2720  | total loss: \u001b[1m\u001b[32m0.68050\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2720 | loss: 0.68050 - acc: 0.7466 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2721  | total loss: \u001b[1m\u001b[32m0.67996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2721 | loss: 0.67996 - acc: 0.7520 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2722  | total loss: \u001b[1m\u001b[32m0.67948\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2722 | loss: 0.67948 - acc: 0.7568 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2723  | total loss: \u001b[1m\u001b[32m0.67905\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2723 | loss: 0.67905 - acc: 0.7611 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2724  | total loss: \u001b[1m\u001b[32m0.68666\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2724 | loss: 0.68666 - acc: 0.6850 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2725  | total loss: \u001b[1m\u001b[32m0.68551\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2725 | loss: 0.68551 - acc: 0.6965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2726  | total loss: \u001b[1m\u001b[32m0.68448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2726 | loss: 0.68448 - acc: 0.7068 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2727  | total loss: \u001b[1m\u001b[32m0.68354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2727 | loss: 0.68354 - acc: 0.7161 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2728  | total loss: \u001b[1m\u001b[32m0.68271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2728 | loss: 0.68271 - acc: 0.7245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2729  | total loss: \u001b[1m\u001b[32m0.68195\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2729 | loss: 0.68195 - acc: 0.7321 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2730  | total loss: \u001b[1m\u001b[32m0.68127\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2730 | loss: 0.68127 - acc: 0.7389 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2731  | total loss: \u001b[1m\u001b[32m0.68066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2731 | loss: 0.68066 - acc: 0.7450 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2732  | total loss: \u001b[1m\u001b[32m0.68011\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2732 | loss: 0.68011 - acc: 0.7505 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2733  | total loss: \u001b[1m\u001b[32m0.67962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2733 | loss: 0.67962 - acc: 0.7554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2734  | total loss: \u001b[1m\u001b[32m0.67917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2734 | loss: 0.67917 - acc: 0.7599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2735  | total loss: \u001b[1m\u001b[32m0.67877\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2735 | loss: 0.67877 - acc: 0.7639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2736  | total loss: \u001b[1m\u001b[32m0.67841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2736 | loss: 0.67841 - acc: 0.7675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2737  | total loss: \u001b[1m\u001b[32m0.67808\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2737 | loss: 0.67808 - acc: 0.7708 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2738  | total loss: \u001b[1m\u001b[32m0.67779\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2738 | loss: 0.67779 - acc: 0.7737 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2739  | total loss: \u001b[1m\u001b[32m0.67753\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2739 | loss: 0.67753 - acc: 0.7763 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2740  | total loss: \u001b[1m\u001b[32m0.68529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2740 | loss: 0.68529 - acc: 0.6987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2741  | total loss: \u001b[1m\u001b[32m0.68428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2741 | loss: 0.68428 - acc: 0.7088 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2742  | total loss: \u001b[1m\u001b[32m0.68337\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2742 | loss: 0.68337 - acc: 0.7179 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2743  | total loss: \u001b[1m\u001b[32m0.68254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2743 | loss: 0.68254 - acc: 0.7261 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2744  | total loss: \u001b[1m\u001b[32m0.68181\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2744 | loss: 0.68181 - acc: 0.7335 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2745  | total loss: \u001b[1m\u001b[32m0.68114\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2745 | loss: 0.68114 - acc: 0.7402 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2746  | total loss: \u001b[1m\u001b[32m0.68054\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2746 | loss: 0.68054 - acc: 0.7462 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2747  | total loss: \u001b[1m\u001b[32m0.68000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2747 | loss: 0.68000 - acc: 0.7515 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2748  | total loss: \u001b[1m\u001b[32m0.67952\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2748 | loss: 0.67952 - acc: 0.7564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.67908\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2749 | loss: 0.67908 - acc: 0.7607 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.67869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2750 | loss: 0.67869 - acc: 0.7647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2751  | total loss: \u001b[1m\u001b[32m0.67834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2751 | loss: 0.67834 - acc: 0.7682 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2752  | total loss: \u001b[1m\u001b[32m0.67802\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2752 | loss: 0.67802 - acc: 0.7714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2753  | total loss: \u001b[1m\u001b[32m0.67773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2753 | loss: 0.67773 - acc: 0.7742 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2754  | total loss: \u001b[1m\u001b[32m0.67748\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2754 | loss: 0.67748 - acc: 0.7768 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2755  | total loss: \u001b[1m\u001b[32m0.67724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2755 | loss: 0.67724 - acc: 0.7791 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2756  | total loss: \u001b[1m\u001b[32m0.67704\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2756 | loss: 0.67704 - acc: 0.7812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2757  | total loss: \u001b[1m\u001b[32m0.67685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2757 | loss: 0.67685 - acc: 0.7831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2758  | total loss: \u001b[1m\u001b[32m0.68368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2758 | loss: 0.68368 - acc: 0.7148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2759  | total loss: \u001b[1m\u001b[32m0.68283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2759 | loss: 0.68283 - acc: 0.7233 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2760  | total loss: \u001b[1m\u001b[32m0.68206\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2760 | loss: 0.68206 - acc: 0.7310 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2761  | total loss: \u001b[1m\u001b[32m0.68137\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2761 | loss: 0.68137 - acc: 0.7379 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2762  | total loss: \u001b[1m\u001b[32m0.68775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2762 | loss: 0.68775 - acc: 0.6741 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2763  | total loss: \u001b[1m\u001b[32m0.68649\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2763 | loss: 0.68649 - acc: 0.6867 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2764  | total loss: \u001b[1m\u001b[32m0.68536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2764 | loss: 0.68536 - acc: 0.6980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2765  | total loss: \u001b[1m\u001b[32m0.68434\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2765 | loss: 0.68434 - acc: 0.7082 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2766  | total loss: \u001b[1m\u001b[32m0.68342\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2766 | loss: 0.68342 - acc: 0.7174 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2767  | total loss: \u001b[1m\u001b[32m0.68259\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2767 | loss: 0.68259 - acc: 0.7257 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2768  | total loss: \u001b[1m\u001b[32m0.68185\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2768 | loss: 0.68185 - acc: 0.7331 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2769  | total loss: \u001b[1m\u001b[32m0.68118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2769 | loss: 0.68118 - acc: 0.7398 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2770  | total loss: \u001b[1m\u001b[32m0.68058\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2770 | loss: 0.68058 - acc: 0.7458 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2771  | total loss: \u001b[1m\u001b[32m0.68004\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2771 | loss: 0.68004 - acc: 0.7512 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2772  | total loss: \u001b[1m\u001b[32m0.67955\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2772 | loss: 0.67955 - acc: 0.7561 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2773  | total loss: \u001b[1m\u001b[32m0.67911\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2773 | loss: 0.67911 - acc: 0.7605 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2774  | total loss: \u001b[1m\u001b[32m0.67871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2774 | loss: 0.67871 - acc: 0.7644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2775  | total loss: \u001b[1m\u001b[32m0.67836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2775 | loss: 0.67836 - acc: 0.7680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2776  | total loss: \u001b[1m\u001b[32m0.67804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2776 | loss: 0.67804 - acc: 0.7712 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2777  | total loss: \u001b[1m\u001b[32m0.67775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2777 | loss: 0.67775 - acc: 0.7741 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2778  | total loss: \u001b[1m\u001b[32m0.67749\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2778 | loss: 0.67749 - acc: 0.7767 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2779  | total loss: \u001b[1m\u001b[32m0.67726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2779 | loss: 0.67726 - acc: 0.7790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2780  | total loss: \u001b[1m\u001b[32m0.67705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2780 | loss: 0.67705 - acc: 0.7811 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2781  | total loss: \u001b[1m\u001b[32m0.67686\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2781 | loss: 0.67686 - acc: 0.7830 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2782  | total loss: \u001b[1m\u001b[32m0.67669\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2782 | loss: 0.67669 - acc: 0.7847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2783  | total loss: \u001b[1m\u001b[32m0.67654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2783 | loss: 0.67654 - acc: 0.7862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2784  | total loss: \u001b[1m\u001b[32m0.67640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2784 | loss: 0.67640 - acc: 0.7876 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2785  | total loss: \u001b[1m\u001b[32m0.67627\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2785 | loss: 0.67627 - acc: 0.7888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2786  | total loss: \u001b[1m\u001b[32m0.68316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2786 | loss: 0.68316 - acc: 0.7200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2787  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2787 | loss: 0.68236 - acc: 0.7280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2788  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2788 | loss: 0.68164 - acc: 0.7352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2789  | total loss: \u001b[1m\u001b[32m0.68099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2789 | loss: 0.68099 - acc: 0.7416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2790  | total loss: \u001b[1m\u001b[32m0.68841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2790 | loss: 0.68841 - acc: 0.6675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2791  | total loss: \u001b[1m\u001b[32m0.68709\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2791 | loss: 0.68709 - acc: 0.6807 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2792  | total loss: \u001b[1m\u001b[32m0.69189\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2792 | loss: 0.69189 - acc: 0.6327 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2793  | total loss: \u001b[1m\u001b[32m0.69022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2793 | loss: 0.69022 - acc: 0.6494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2794  | total loss: \u001b[1m\u001b[32m0.68871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2794 | loss: 0.68871 - acc: 0.6645 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2795  | total loss: \u001b[1m\u001b[32m0.68736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2795 | loss: 0.68736 - acc: 0.6780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2796  | total loss: \u001b[1m\u001b[32m0.68614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2796 | loss: 0.68614 - acc: 0.6902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2797  | total loss: \u001b[1m\u001b[32m0.68504\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2797 | loss: 0.68504 - acc: 0.7012 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2798  | total loss: \u001b[1m\u001b[32m0.68405\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2798 | loss: 0.68405 - acc: 0.7111 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2799  | total loss: \u001b[1m\u001b[32m0.68316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2799 | loss: 0.68316 - acc: 0.7200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2800  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2800 | loss: 0.68236 - acc: 0.7280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2801  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2801 | loss: 0.68164 - acc: 0.7352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2802  | total loss: \u001b[1m\u001b[32m0.68099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2802 | loss: 0.68099 - acc: 0.7417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2803  | total loss: \u001b[1m\u001b[32m0.68041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2803 | loss: 0.68041 - acc: 0.7475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2804  | total loss: \u001b[1m\u001b[32m0.67988\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2804 | loss: 0.67988 - acc: 0.7527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2805  | total loss: \u001b[1m\u001b[32m0.67941\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2805 | loss: 0.67941 - acc: 0.7575 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2806  | total loss: \u001b[1m\u001b[32m0.68699\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2806 | loss: 0.68699 - acc: 0.6817 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2807  | total loss: \u001b[1m\u001b[32m0.68580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2807 | loss: 0.68580 - acc: 0.6935 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2808  | total loss: \u001b[1m\u001b[32m0.68474\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2808 | loss: 0.68474 - acc: 0.7042 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2809  | total loss: \u001b[1m\u001b[32m0.68378\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2809 | loss: 0.68378 - acc: 0.7138 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2810  | total loss: \u001b[1m\u001b[32m0.68292\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2810 | loss: 0.68292 - acc: 0.7224 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2811  | total loss: \u001b[1m\u001b[32m0.68214\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2811 | loss: 0.68214 - acc: 0.7302 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2812  | total loss: \u001b[1m\u001b[32m0.68144\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2812 | loss: 0.68144 - acc: 0.7371 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2813  | total loss: \u001b[1m\u001b[32m0.68082\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2813 | loss: 0.68082 - acc: 0.7434 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2814  | total loss: \u001b[1m\u001b[32m0.68025\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2814 | loss: 0.68025 - acc: 0.7491 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2815  | total loss: \u001b[1m\u001b[32m0.67974\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2815 | loss: 0.67974 - acc: 0.7542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2816  | total loss: \u001b[1m\u001b[32m0.67928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2816 | loss: 0.67928 - acc: 0.7588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2817  | total loss: \u001b[1m\u001b[32m0.67887\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2817 | loss: 0.67887 - acc: 0.7629 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2818  | total loss: \u001b[1m\u001b[32m0.67850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2818 | loss: 0.67850 - acc: 0.7666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2819  | total loss: \u001b[1m\u001b[32m0.67817\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2819 | loss: 0.67817 - acc: 0.7699 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2820  | total loss: \u001b[1m\u001b[32m0.67786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2820 | loss: 0.67786 - acc: 0.7729 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2821  | total loss: \u001b[1m\u001b[32m0.67759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2821 | loss: 0.67759 - acc: 0.7756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2822  | total loss: \u001b[1m\u001b[32m0.67735\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2822 | loss: 0.67735 - acc: 0.7781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2823  | total loss: \u001b[1m\u001b[32m0.67713\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2823 | loss: 0.67713 - acc: 0.7803 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2824  | total loss: \u001b[1m\u001b[32m0.67693\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2824 | loss: 0.67693 - acc: 0.7822 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2825  | total loss: \u001b[1m\u001b[32m0.67676\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2825 | loss: 0.67676 - acc: 0.7840 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2826  | total loss: \u001b[1m\u001b[32m0.67660\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2826 | loss: 0.67660 - acc: 0.7856 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2827  | total loss: \u001b[1m\u001b[32m0.67645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2827 | loss: 0.67645 - acc: 0.7871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2828  | total loss: \u001b[1m\u001b[32m0.67632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2828 | loss: 0.67632 - acc: 0.7884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2829  | total loss: \u001b[1m\u001b[32m0.67621\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2829 | loss: 0.67621 - acc: 0.7895 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2830  | total loss: \u001b[1m\u001b[32m0.67610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2830 | loss: 0.67610 - acc: 0.7906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2831  | total loss: \u001b[1m\u001b[32m0.67601\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2831 | loss: 0.67601 - acc: 0.7915 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2832  | total loss: \u001b[1m\u001b[32m0.68392\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2832 | loss: 0.68392 - acc: 0.7124 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2833  | total loss: \u001b[1m\u001b[32m0.68305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2833 | loss: 0.68305 - acc: 0.7211 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2834  | total loss: \u001b[1m\u001b[32m0.68226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2834 | loss: 0.68226 - acc: 0.7290 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2835  | total loss: \u001b[1m\u001b[32m0.68155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2835 | loss: 0.68155 - acc: 0.7361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2836  | total loss: \u001b[1m\u001b[32m0.68091\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2836 | loss: 0.68091 - acc: 0.7425 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2837  | total loss: \u001b[1m\u001b[32m0.68033\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2837 | loss: 0.68033 - acc: 0.7482 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2838  | total loss: \u001b[1m\u001b[32m0.68782\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2838 | loss: 0.68782 - acc: 0.6734 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2839  | total loss: \u001b[1m\u001b[32m0.68655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2839 | loss: 0.68655 - acc: 0.6861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2840  | total loss: \u001b[1m\u001b[32m0.68541\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2840 | loss: 0.68541 - acc: 0.6975 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2841  | total loss: \u001b[1m\u001b[32m0.68439\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2841 | loss: 0.68439 - acc: 0.7077 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2842  | total loss: \u001b[1m\u001b[32m0.68346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2842 | loss: 0.68346 - acc: 0.7170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2843  | total loss: \u001b[1m\u001b[32m0.68263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2843 | loss: 0.68263 - acc: 0.7253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2844  | total loss: \u001b[1m\u001b[32m0.68189\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2844 | loss: 0.68189 - acc: 0.7327 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2845  | total loss: \u001b[1m\u001b[32m0.68121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2845 | loss: 0.68121 - acc: 0.7395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2846  | total loss: \u001b[1m\u001b[32m0.68861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2846 | loss: 0.68861 - acc: 0.6655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2847  | total loss: \u001b[1m\u001b[32m0.68726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2847 | loss: 0.68726 - acc: 0.6790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2848  | total loss: \u001b[1m\u001b[32m0.68605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2848 | loss: 0.68605 - acc: 0.6911 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2849  | total loss: \u001b[1m\u001b[32m0.68496\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2849 | loss: 0.68496 - acc: 0.7020 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2850  | total loss: \u001b[1m\u001b[32m0.68398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2850 | loss: 0.68398 - acc: 0.7118 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2851  | total loss: \u001b[1m\u001b[32m0.68310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2851 | loss: 0.68310 - acc: 0.7206 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2852  | total loss: \u001b[1m\u001b[32m0.68231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2852 | loss: 0.68231 - acc: 0.7285 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2853  | total loss: \u001b[1m\u001b[32m0.68159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2853 | loss: 0.68159 - acc: 0.7357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2854  | total loss: \u001b[1m\u001b[32m0.68095\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2854 | loss: 0.68095 - acc: 0.7421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2855  | total loss: \u001b[1m\u001b[32m0.68037\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2855 | loss: 0.68037 - acc: 0.7479 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2856  | total loss: \u001b[1m\u001b[32m0.67985\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2856 | loss: 0.67985 - acc: 0.7531 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2857  | total loss: \u001b[1m\u001b[32m0.67938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2857 | loss: 0.67938 - acc: 0.7578 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2858  | total loss: \u001b[1m\u001b[32m0.67896\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2858 | loss: 0.67896 - acc: 0.7620 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2859  | total loss: \u001b[1m\u001b[32m0.67858\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2859 | loss: 0.67858 - acc: 0.7658 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2860  | total loss: \u001b[1m\u001b[32m0.67824\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2860 | loss: 0.67824 - acc: 0.7692 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2861  | total loss: \u001b[1m\u001b[32m0.67793\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2861 | loss: 0.67793 - acc: 0.7723 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2862  | total loss: \u001b[1m\u001b[32m0.67765\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2862 | loss: 0.67765 - acc: 0.7751 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2863  | total loss: \u001b[1m\u001b[32m0.67740\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2863 | loss: 0.67740 - acc: 0.7776 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2864  | total loss: \u001b[1m\u001b[32m0.67718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2864 | loss: 0.67718 - acc: 0.7798 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2865  | total loss: \u001b[1m\u001b[32m0.67698\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2865 | loss: 0.67698 - acc: 0.7818 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2866  | total loss: \u001b[1m\u001b[32m0.67679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2866 | loss: 0.67679 - acc: 0.7836 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2867  | total loss: \u001b[1m\u001b[32m0.67663\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2867 | loss: 0.67663 - acc: 0.7853 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2868  | total loss: \u001b[1m\u001b[32m0.67648\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2868 | loss: 0.67648 - acc: 0.7868 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2869  | total loss: \u001b[1m\u001b[32m0.67635\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2869 | loss: 0.67635 - acc: 0.7881 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2870  | total loss: \u001b[1m\u001b[32m0.68423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2870 | loss: 0.68423 - acc: 0.7093 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2871  | total loss: \u001b[1m\u001b[32m0.68332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2871 | loss: 0.68332 - acc: 0.7183 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2872  | total loss: \u001b[1m\u001b[32m0.68251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2872 | loss: 0.68251 - acc: 0.7265 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2873  | total loss: \u001b[1m\u001b[32m0.68177\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2873 | loss: 0.68177 - acc: 0.7339 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2874  | total loss: \u001b[1m\u001b[32m0.68811\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2874 | loss: 0.68811 - acc: 0.6705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2875  | total loss: \u001b[1m\u001b[32m0.68682\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2875 | loss: 0.68682 - acc: 0.6834 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2876  | total loss: \u001b[1m\u001b[32m0.69365\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2876 | loss: 0.69365 - acc: 0.6151 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2877  | total loss: \u001b[1m\u001b[32m0.69180\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2877 | loss: 0.69180 - acc: 0.6336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2878  | total loss: \u001b[1m\u001b[32m0.69014\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2878 | loss: 0.69014 - acc: 0.6502 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2879  | total loss: \u001b[1m\u001b[32m0.68864\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2879 | loss: 0.68864 - acc: 0.6652 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2880  | total loss: \u001b[1m\u001b[32m0.68729\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2880 | loss: 0.68729 - acc: 0.6787 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2881  | total loss: \u001b[1m\u001b[32m0.68608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2881 | loss: 0.68608 - acc: 0.6908 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2882  | total loss: \u001b[1m\u001b[32m0.68499\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2882 | loss: 0.68499 - acc: 0.7017 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2883  | total loss: \u001b[1m\u001b[32m0.68400\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2883 | loss: 0.68400 - acc: 0.7116 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2884  | total loss: \u001b[1m\u001b[32m0.69112\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2884 | loss: 0.69112 - acc: 0.6404 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2885  | total loss: \u001b[1m\u001b[32m0.68952\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2885 | loss: 0.68952 - acc: 0.6564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2886  | total loss: \u001b[1m\u001b[32m0.68809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2886 | loss: 0.68809 - acc: 0.6707 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2887  | total loss: \u001b[1m\u001b[32m0.68679\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2887 | loss: 0.68679 - acc: 0.6837 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2888  | total loss: \u001b[1m\u001b[32m0.69363\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2888 | loss: 0.69363 - acc: 0.6153 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2889  | total loss: \u001b[1m\u001b[32m0.69178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2889 | loss: 0.69178 - acc: 0.6338 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2890  | total loss: \u001b[1m\u001b[32m0.69012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2890 | loss: 0.69012 - acc: 0.6504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2891  | total loss: \u001b[1m\u001b[32m0.68862\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2891 | loss: 0.68862 - acc: 0.6653 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2892  | total loss: \u001b[1m\u001b[32m0.68728\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2892 | loss: 0.68728 - acc: 0.6788 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2893  | total loss: \u001b[1m\u001b[32m0.68607\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2893 | loss: 0.68607 - acc: 0.6909 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2894  | total loss: \u001b[1m\u001b[32m0.68498\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2894 | loss: 0.68498 - acc: 0.7018 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2895  | total loss: \u001b[1m\u001b[32m0.68399\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2895 | loss: 0.68399 - acc: 0.7117 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2896  | total loss: \u001b[1m\u001b[32m0.68311\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2896 | loss: 0.68311 - acc: 0.7205 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2897  | total loss: \u001b[1m\u001b[32m0.68232\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2897 | loss: 0.68232 - acc: 0.7284 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2898  | total loss: \u001b[1m\u001b[32m0.68160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2898 | loss: 0.68160 - acc: 0.7356 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2899  | total loss: \u001b[1m\u001b[32m0.68096\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2899 | loss: 0.68096 - acc: 0.7420 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2900  | total loss: \u001b[1m\u001b[32m0.68038\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2900 | loss: 0.68038 - acc: 0.7478 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2901  | total loss: \u001b[1m\u001b[32m0.67985\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2901 | loss: 0.67985 - acc: 0.7530 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2902  | total loss: \u001b[1m\u001b[32m0.67938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2902 | loss: 0.67938 - acc: 0.7577 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2903  | total loss: \u001b[1m\u001b[32m0.67896\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2903 | loss: 0.67896 - acc: 0.7620 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2904  | total loss: \u001b[1m\u001b[32m0.68458\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2904 | loss: 0.68458 - acc: 0.7058 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2905  | total loss: \u001b[1m\u001b[32m0.68364\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2905 | loss: 0.68364 - acc: 0.7152 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2906  | total loss: \u001b[1m\u001b[32m0.68279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2906 | loss: 0.68279 - acc: 0.7237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2907  | total loss: \u001b[1m\u001b[32m0.68203\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2907 | loss: 0.68203 - acc: 0.7313 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2908  | total loss: \u001b[1m\u001b[32m0.68134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2908 | loss: 0.68134 - acc: 0.7382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2909  | total loss: \u001b[1m\u001b[32m0.68072\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2909 | loss: 0.68072 - acc: 0.7444 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2910  | total loss: \u001b[1m\u001b[32m0.68017\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2910 | loss: 0.68017 - acc: 0.7499 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2911  | total loss: \u001b[1m\u001b[32m0.67967\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2911 | loss: 0.67967 - acc: 0.7549 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2912  | total loss: \u001b[1m\u001b[32m0.68722\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2912 | loss: 0.68722 - acc: 0.6794 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2913  | total loss: \u001b[1m\u001b[32m0.68601\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2913 | loss: 0.68601 - acc: 0.6915 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2914  | total loss: \u001b[1m\u001b[32m0.68492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2914 | loss: 0.68492 - acc: 0.7023 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2915  | total loss: \u001b[1m\u001b[32m0.68395\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2915 | loss: 0.68395 - acc: 0.7121 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2916  | total loss: \u001b[1m\u001b[32m0.68307\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2916 | loss: 0.68307 - acc: 0.7209 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2917  | total loss: \u001b[1m\u001b[32m0.68228\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2917 | loss: 0.68228 - acc: 0.7288 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2918  | total loss: \u001b[1m\u001b[32m0.68157\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2918 | loss: 0.68157 - acc: 0.7359 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2919  | total loss: \u001b[1m\u001b[32m0.68093\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2919 | loss: 0.68093 - acc: 0.7423 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2920  | total loss: \u001b[1m\u001b[32m0.68035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2920 | loss: 0.68035 - acc: 0.7481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2921  | total loss: \u001b[1m\u001b[32m0.67983\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2921 | loss: 0.67983 - acc: 0.7533 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2922  | total loss: \u001b[1m\u001b[32m0.67936\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2922 | loss: 0.67936 - acc: 0.7580 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2923  | total loss: \u001b[1m\u001b[32m0.67894\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2923 | loss: 0.67894 - acc: 0.7622 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2924  | total loss: \u001b[1m\u001b[32m0.67856\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2924 | loss: 0.67856 - acc: 0.7659 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2925  | total loss: \u001b[1m\u001b[32m0.67822\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2925 | loss: 0.67822 - acc: 0.7694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2926  | total loss: \u001b[1m\u001b[32m0.67792\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2926 | loss: 0.67792 - acc: 0.7724 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2927  | total loss: \u001b[1m\u001b[32m0.67764\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2927 | loss: 0.67764 - acc: 0.7752 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2928  | total loss: \u001b[1m\u001b[32m0.67739\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2928 | loss: 0.67739 - acc: 0.7777 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2929  | total loss: \u001b[1m\u001b[32m0.67717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2929 | loss: 0.67717 - acc: 0.7799 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2930  | total loss: \u001b[1m\u001b[32m0.67697\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2930 | loss: 0.67697 - acc: 0.7819 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2931  | total loss: \u001b[1m\u001b[32m0.67679\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2931 | loss: 0.67679 - acc: 0.7837 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2932  | total loss: \u001b[1m\u001b[32m0.67662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2932 | loss: 0.67662 - acc: 0.7853 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2933  | total loss: \u001b[1m\u001b[32m0.67648\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2933 | loss: 0.67648 - acc: 0.7868 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2934  | total loss: \u001b[1m\u001b[32m0.68235\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2934 | loss: 0.68235 - acc: 0.7281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2935  | total loss: \u001b[1m\u001b[32m0.68163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2935 | loss: 0.68163 - acc: 0.7353 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2936  | total loss: \u001b[1m\u001b[32m0.68098\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2936 | loss: 0.68098 - acc: 0.7418 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2937  | total loss: \u001b[1m\u001b[32m0.68040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2937 | loss: 0.68040 - acc: 0.7476 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2938  | total loss: \u001b[1m\u001b[32m0.68587\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2938 | loss: 0.68587 - acc: 0.6928 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2939  | total loss: \u001b[1m\u001b[32m0.68480\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2939 | loss: 0.68480 - acc: 0.7036 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2940  | total loss: \u001b[1m\u001b[32m0.69184\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 2940 | loss: 0.69184 - acc: 0.6332 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2941  | total loss: \u001b[1m\u001b[32m0.69017\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2941 | loss: 0.69017 - acc: 0.6499 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2942  | total loss: \u001b[1m\u001b[32m0.68867\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2942 | loss: 0.68867 - acc: 0.6649 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2943  | total loss: \u001b[1m\u001b[32m0.68732\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2943 | loss: 0.68732 - acc: 0.6784 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2944  | total loss: \u001b[1m\u001b[32m0.68610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2944 | loss: 0.68610 - acc: 0.6906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2945  | total loss: \u001b[1m\u001b[32m0.68501\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2945 | loss: 0.68501 - acc: 0.7015 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2946  | total loss: \u001b[1m\u001b[32m0.68402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2946 | loss: 0.68402 - acc: 0.7114 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2947  | total loss: \u001b[1m\u001b[32m0.68314\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2947 | loss: 0.68314 - acc: 0.7202 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2948  | total loss: \u001b[1m\u001b[32m0.68234\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2948 | loss: 0.68234 - acc: 0.7282 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2949  | total loss: \u001b[1m\u001b[32m0.68162\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2949 | loss: 0.68162 - acc: 0.7354 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2950  | total loss: \u001b[1m\u001b[32m0.68097\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2950 | loss: 0.68097 - acc: 0.7418 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2951  | total loss: \u001b[1m\u001b[32m0.68039\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2951 | loss: 0.68039 - acc: 0.7477 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2952  | total loss: \u001b[1m\u001b[32m0.67987\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2952 | loss: 0.67987 - acc: 0.7529 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2953  | total loss: \u001b[1m\u001b[32m0.67940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2953 | loss: 0.67940 - acc: 0.7576 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2954  | total loss: \u001b[1m\u001b[32m0.67897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2954 | loss: 0.67897 - acc: 0.7618 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2955  | total loss: \u001b[1m\u001b[32m0.67859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2955 | loss: 0.67859 - acc: 0.7657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2956  | total loss: \u001b[1m\u001b[32m0.67825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2956 | loss: 0.67825 - acc: 0.7691 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2957  | total loss: \u001b[1m\u001b[32m0.67794\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2957 | loss: 0.67794 - acc: 0.7722 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2958  | total loss: \u001b[1m\u001b[32m0.67766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2958 | loss: 0.67766 - acc: 0.7750 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2959  | total loss: \u001b[1m\u001b[32m0.67741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2959 | loss: 0.67741 - acc: 0.7775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2960  | total loss: \u001b[1m\u001b[32m0.67719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2960 | loss: 0.67719 - acc: 0.7797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2961  | total loss: \u001b[1m\u001b[32m0.67698\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2961 | loss: 0.67698 - acc: 0.7817 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2962  | total loss: \u001b[1m\u001b[32m0.68380\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2962 | loss: 0.68380 - acc: 0.7136 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2963  | total loss: \u001b[1m\u001b[32m0.68294\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2963 | loss: 0.68294 - acc: 0.7222 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2964  | total loss: \u001b[1m\u001b[32m0.68216\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2964 | loss: 0.68216 - acc: 0.7300 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2965  | total loss: \u001b[1m\u001b[32m0.68146\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2965 | loss: 0.68146 - acc: 0.7370 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2966  | total loss: \u001b[1m\u001b[32m0.68083\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2966 | loss: 0.68083 - acc: 0.7433 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2967  | total loss: \u001b[1m\u001b[32m0.68026\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2967 | loss: 0.68026 - acc: 0.7490 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2968  | total loss: \u001b[1m\u001b[32m0.67975\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2968 | loss: 0.67975 - acc: 0.7541 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2969  | total loss: \u001b[1m\u001b[32m0.67929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2969 | loss: 0.67929 - acc: 0.7587 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2970  | total loss: \u001b[1m\u001b[32m0.67888\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2970 | loss: 0.67888 - acc: 0.7628 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2971  | total loss: \u001b[1m\u001b[32m0.67851\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2971 | loss: 0.67851 - acc: 0.7665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2972  | total loss: \u001b[1m\u001b[32m0.68317\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2972 | loss: 0.68317 - acc: 0.7199 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2973  | total loss: \u001b[1m\u001b[32m0.68237\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2973 | loss: 0.68237 - acc: 0.7279 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2974  | total loss: \u001b[1m\u001b[32m0.68165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2974 | loss: 0.68165 - acc: 0.7351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2975  | total loss: \u001b[1m\u001b[32m0.68100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2975 | loss: 0.68100 - acc: 0.7416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2976  | total loss: \u001b[1m\u001b[32m0.68042\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2976 | loss: 0.68042 - acc: 0.7474 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2977  | total loss: \u001b[1m\u001b[32m0.67989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2977 | loss: 0.67989 - acc: 0.7527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2978  | total loss: \u001b[1m\u001b[32m0.67942\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2978 | loss: 0.67942 - acc: 0.7574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2979  | total loss: \u001b[1m\u001b[32m0.67899\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2979 | loss: 0.67899 - acc: 0.7617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2980  | total loss: \u001b[1m\u001b[32m0.67861\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2980 | loss: 0.67861 - acc: 0.7655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2981  | total loss: \u001b[1m\u001b[32m0.67826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2981 | loss: 0.67826 - acc: 0.7690 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2982  | total loss: \u001b[1m\u001b[32m0.68595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2982 | loss: 0.68595 - acc: 0.6921 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2983  | total loss: \u001b[1m\u001b[32m0.68487\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2983 | loss: 0.68487 - acc: 0.7029 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2984  | total loss: \u001b[1m\u001b[32m0.68390\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2984 | loss: 0.68390 - acc: 0.7126 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2985  | total loss: \u001b[1m\u001b[32m0.68303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2985 | loss: 0.68303 - acc: 0.7213 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2986  | total loss: \u001b[1m\u001b[32m0.68924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2986 | loss: 0.68924 - acc: 0.6592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2987  | total loss: \u001b[1m\u001b[32m0.68783\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2987 | loss: 0.68783 - acc: 0.6733 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2988  | total loss: \u001b[1m\u001b[32m0.68657\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2988 | loss: 0.68657 - acc: 0.6859 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2989  | total loss: \u001b[1m\u001b[32m0.68542\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2989 | loss: 0.68542 - acc: 0.6973 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2990  | total loss: \u001b[1m\u001b[32m0.68440\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2990 | loss: 0.68440 - acc: 0.7076 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2991  | total loss: \u001b[1m\u001b[32m0.68347\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2991 | loss: 0.68347 - acc: 0.7168 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2992  | total loss: \u001b[1m\u001b[32m0.68264\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2992 | loss: 0.68264 - acc: 0.7252 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2993  | total loss: \u001b[1m\u001b[32m0.68189\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2993 | loss: 0.68189 - acc: 0.7326 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2994  | total loss: \u001b[1m\u001b[32m0.68122\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2994 | loss: 0.68122 - acc: 0.7394 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2995  | total loss: \u001b[1m\u001b[32m0.68061\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2995 | loss: 0.68061 - acc: 0.7454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2996  | total loss: \u001b[1m\u001b[32m0.68007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2996 | loss: 0.68007 - acc: 0.7509 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2997  | total loss: \u001b[1m\u001b[32m0.67958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2997 | loss: 0.67958 - acc: 0.7558 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2998  | total loss: \u001b[1m\u001b[32m0.67914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 2998 | loss: 0.67914 - acc: 0.7602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.67874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2999 | loss: 0.67874 - acc: 0.7642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.67838\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3000 | loss: 0.67838 - acc: 0.7678 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3001  | total loss: \u001b[1m\u001b[32m0.67806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3001 | loss: 0.67806 - acc: 0.7710 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3002  | total loss: \u001b[1m\u001b[32m0.68577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3002 | loss: 0.68577 - acc: 0.6939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3003  | total loss: \u001b[1m\u001b[32m0.68471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3003 | loss: 0.68471 - acc: 0.7045 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3004  | total loss: \u001b[1m\u001b[32m0.68375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3004 | loss: 0.68375 - acc: 0.7141 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3005  | total loss: \u001b[1m\u001b[32m0.68289\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3005 | loss: 0.68289 - acc: 0.7227 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3006  | total loss: \u001b[1m\u001b[32m0.68212\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3006 | loss: 0.68212 - acc: 0.7304 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3007  | total loss: \u001b[1m\u001b[32m0.68142\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3007 | loss: 0.68142 - acc: 0.7374 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3008  | total loss: \u001b[1m\u001b[32m0.68880\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3008 | loss: 0.68880 - acc: 0.6636 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3009  | total loss: \u001b[1m\u001b[32m0.68743\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3009 | loss: 0.68743 - acc: 0.6773 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3010  | total loss: \u001b[1m\u001b[32m0.68621\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3010 | loss: 0.68621 - acc: 0.6895 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3011  | total loss: \u001b[1m\u001b[32m0.68510\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3011 | loss: 0.68510 - acc: 0.7006 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3012  | total loss: \u001b[1m\u001b[32m0.69011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3012 | loss: 0.69011 - acc: 0.6505 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3013  | total loss: \u001b[1m\u001b[32m0.68861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3013 | loss: 0.68861 - acc: 0.6655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3014  | total loss: \u001b[1m\u001b[32m0.68727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3014 | loss: 0.68727 - acc: 0.6789 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3015  | total loss: \u001b[1m\u001b[32m0.68606\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3015 | loss: 0.68606 - acc: 0.6910 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3016  | total loss: \u001b[1m\u001b[32m0.69197\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3016 | loss: 0.69197 - acc: 0.6319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3017  | total loss: \u001b[1m\u001b[32m0.69029\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3017 | loss: 0.69029 - acc: 0.6487 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3018  | total loss: \u001b[1m\u001b[32m0.68877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3018 | loss: 0.68877 - acc: 0.6639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3019  | total loss: \u001b[1m\u001b[32m0.68741\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3019 | loss: 0.68741 - acc: 0.6775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3020  | total loss: \u001b[1m\u001b[32m0.68619\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3020 | loss: 0.68619 - acc: 0.6897 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3021  | total loss: \u001b[1m\u001b[32m0.68508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3021 | loss: 0.68508 - acc: 0.7008 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3022  | total loss: \u001b[1m\u001b[32m0.69209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3022 | loss: 0.69209 - acc: 0.6307 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3023  | total loss: \u001b[1m\u001b[32m0.69040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3023 | loss: 0.69040 - acc: 0.6476 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3024  | total loss: \u001b[1m\u001b[32m0.68887\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3024 | loss: 0.68887 - acc: 0.6628 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3025  | total loss: \u001b[1m\u001b[32m0.68750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3025 | loss: 0.68750 - acc: 0.6766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3026  | total loss: \u001b[1m\u001b[32m0.68627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3026 | loss: 0.68627 - acc: 0.6889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3027  | total loss: \u001b[1m\u001b[32m0.68516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3027 | loss: 0.68516 - acc: 0.7000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3028  | total loss: \u001b[1m\u001b[32m0.68416\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3028 | loss: 0.68416 - acc: 0.7100 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3029  | total loss: \u001b[1m\u001b[32m0.68326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3029 | loss: 0.68326 - acc: 0.7190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3030  | total loss: \u001b[1m\u001b[32m0.68945\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3030 | loss: 0.68945 - acc: 0.6571 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3031  | total loss: \u001b[1m\u001b[32m0.68802\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3031 | loss: 0.68802 - acc: 0.6714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3032  | total loss: \u001b[1m\u001b[32m0.68673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3032 | loss: 0.68673 - acc: 0.6843 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3033  | total loss: \u001b[1m\u001b[32m0.68558\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3033 | loss: 0.68558 - acc: 0.6958 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3034  | total loss: \u001b[1m\u001b[32m0.68453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3034 | loss: 0.68453 - acc: 0.7063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3035  | total loss: \u001b[1m\u001b[32m0.68360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3035 | loss: 0.68360 - acc: 0.7156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3036  | total loss: \u001b[1m\u001b[32m0.68875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3036 | loss: 0.68875 - acc: 0.6641 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3037  | total loss: \u001b[1m\u001b[32m0.68739\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3037 | loss: 0.68739 - acc: 0.6777 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3038  | total loss: \u001b[1m\u001b[32m0.68617\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3038 | loss: 0.68617 - acc: 0.6899 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3039  | total loss: \u001b[1m\u001b[32m0.68507\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3039 | loss: 0.68507 - acc: 0.7009 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3040  | total loss: \u001b[1m\u001b[32m0.68408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3040 | loss: 0.68408 - acc: 0.7108 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3041  | total loss: \u001b[1m\u001b[32m0.68319\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3041 | loss: 0.68319 - acc: 0.7197 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3042  | total loss: \u001b[1m\u001b[32m0.68238\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3042 | loss: 0.68238 - acc: 0.7278 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3043  | total loss: \u001b[1m\u001b[32m0.68166\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3043 | loss: 0.68166 - acc: 0.7350 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3044  | total loss: \u001b[1m\u001b[32m0.68101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3044 | loss: 0.68101 - acc: 0.7415 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3045  | total loss: \u001b[1m\u001b[32m0.68043\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3045 | loss: 0.68043 - acc: 0.7473 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3046  | total loss: \u001b[1m\u001b[32m0.67990\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3046 | loss: 0.67990 - acc: 0.7526 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3047  | total loss: \u001b[1m\u001b[32m0.67942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3047 | loss: 0.67942 - acc: 0.7573 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3048  | total loss: \u001b[1m\u001b[32m0.67900\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3048 | loss: 0.67900 - acc: 0.7616 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3049  | total loss: \u001b[1m\u001b[32m0.67861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3049 | loss: 0.67861 - acc: 0.7654 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3050  | total loss: \u001b[1m\u001b[32m0.67827\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3050 | loss: 0.67827 - acc: 0.7689 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3051  | total loss: \u001b[1m\u001b[32m0.67796\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3051 | loss: 0.67796 - acc: 0.7720 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3052  | total loss: \u001b[1m\u001b[32m0.67768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3052 | loss: 0.67768 - acc: 0.7748 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3053  | total loss: \u001b[1m\u001b[32m0.67743\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3053 | loss: 0.67743 - acc: 0.7773 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3054  | total loss: \u001b[1m\u001b[32m0.67720\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3054 | loss: 0.67720 - acc: 0.7796 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3055  | total loss: \u001b[1m\u001b[32m0.67700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3055 | loss: 0.67700 - acc: 0.7816 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3056  | total loss: \u001b[1m\u001b[32m0.67681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3056 | loss: 0.67681 - acc: 0.7835 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3057  | total loss: \u001b[1m\u001b[32m0.67665\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3057 | loss: 0.67665 - acc: 0.7851 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3058  | total loss: \u001b[1m\u001b[32m0.67650\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3058 | loss: 0.67650 - acc: 0.7866 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3059  | total loss: \u001b[1m\u001b[32m0.67636\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3059 | loss: 0.67636 - acc: 0.7880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3060  | total loss: \u001b[1m\u001b[32m0.67624\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3060 | loss: 0.67624 - acc: 0.7892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3061  | total loss: \u001b[1m\u001b[32m0.67613\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3061 | loss: 0.67613 - acc: 0.7902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3062  | total loss: \u001b[1m\u001b[32m0.67604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3062 | loss: 0.67604 - acc: 0.7912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3063  | total loss: \u001b[1m\u001b[32m0.67595\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3063 | loss: 0.67595 - acc: 0.7921 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3064  | total loss: \u001b[1m\u001b[32m0.67587\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3064 | loss: 0.67587 - acc: 0.7929 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3065  | total loss: \u001b[1m\u001b[32m0.67580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3065 | loss: 0.67580 - acc: 0.7936 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3066  | total loss: \u001b[1m\u001b[32m0.67574\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3066 | loss: 0.67574 - acc: 0.7942 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3067  | total loss: \u001b[1m\u001b[32m0.67568\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3067 | loss: 0.67568 - acc: 0.7948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3068  | total loss: \u001b[1m\u001b[32m0.67563\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3068 | loss: 0.67563 - acc: 0.7953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3069  | total loss: \u001b[1m\u001b[32m0.67558\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3069 | loss: 0.67558 - acc: 0.7958 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3070  | total loss: \u001b[1m\u001b[32m0.67554\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3070 | loss: 0.67554 - acc: 0.7962 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3071  | total loss: \u001b[1m\u001b[32m0.67550\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3071 | loss: 0.67550 - acc: 0.7966 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3072  | total loss: \u001b[1m\u001b[32m0.67547\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3072 | loss: 0.67547 - acc: 0.7969 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3073  | total loss: \u001b[1m\u001b[32m0.67543\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3073 | loss: 0.67543 - acc: 0.7972 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3074  | total loss: \u001b[1m\u001b[32m0.67541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3074 | loss: 0.67541 - acc: 0.7975 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3075  | total loss: \u001b[1m\u001b[32m0.67538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3075 | loss: 0.67538 - acc: 0.7978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3076  | total loss: \u001b[1m\u001b[32m0.67536\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3076 | loss: 0.67536 - acc: 0.7980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3077  | total loss: \u001b[1m\u001b[32m0.67534\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3077 | loss: 0.67534 - acc: 0.7982 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3078  | total loss: \u001b[1m\u001b[32m0.67532\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3078 | loss: 0.67532 - acc: 0.7984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3079  | total loss: \u001b[1m\u001b[32m0.67531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3079 | loss: 0.67531 - acc: 0.7985 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3080  | total loss: \u001b[1m\u001b[32m0.67529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3080 | loss: 0.67529 - acc: 0.7987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3081  | total loss: \u001b[1m\u001b[32m0.67528\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3081 | loss: 0.67528 - acc: 0.7988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3082  | total loss: \u001b[1m\u001b[32m0.67527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3082 | loss: 0.67527 - acc: 0.7989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3083  | total loss: \u001b[1m\u001b[32m0.67526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3083 | loss: 0.67526 - acc: 0.7990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3084  | total loss: \u001b[1m\u001b[32m0.67525\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3084 | loss: 0.67525 - acc: 0.7991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3085  | total loss: \u001b[1m\u001b[32m0.67524\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3085 | loss: 0.67524 - acc: 0.7992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3086  | total loss: \u001b[1m\u001b[32m0.67523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3086 | loss: 0.67523 - acc: 0.7993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3087  | total loss: \u001b[1m\u001b[32m0.67522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3087 | loss: 0.67522 - acc: 0.7994 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3088  | total loss: \u001b[1m\u001b[32m0.68022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3088 | loss: 0.68022 - acc: 0.7494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3089  | total loss: \u001b[1m\u001b[32m0.67971\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3089 | loss: 0.67971 - acc: 0.7545 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3090  | total loss: \u001b[1m\u001b[32m0.67925\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3090 | loss: 0.67925 - acc: 0.7590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3091  | total loss: \u001b[1m\u001b[32m0.67885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3091 | loss: 0.67885 - acc: 0.7631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3092  | total loss: \u001b[1m\u001b[32m0.67848\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3092 | loss: 0.67848 - acc: 0.7668 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3093  | total loss: \u001b[1m\u001b[32m0.67814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3093 | loss: 0.67814 - acc: 0.7701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3094  | total loss: \u001b[1m\u001b[32m0.67785\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3094 | loss: 0.67785 - acc: 0.7731 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3095  | total loss: \u001b[1m\u001b[32m0.67758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3095 | loss: 0.67758 - acc: 0.7758 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3096  | total loss: \u001b[1m\u001b[32m0.67734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3096 | loss: 0.67734 - acc: 0.7782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3097  | total loss: \u001b[1m\u001b[32m0.67712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3097 | loss: 0.67712 - acc: 0.7804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3098  | total loss: \u001b[1m\u001b[32m0.67692\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3098 | loss: 0.67692 - acc: 0.7824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3099  | total loss: \u001b[1m\u001b[32m0.67675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3099 | loss: 0.67675 - acc: 0.7841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3100  | total loss: \u001b[1m\u001b[32m0.67659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3100 | loss: 0.67659 - acc: 0.7857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3101  | total loss: \u001b[1m\u001b[32m0.67644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3101 | loss: 0.67644 - acc: 0.7871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3102  | total loss: \u001b[1m\u001b[32m0.67632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3102 | loss: 0.67632 - acc: 0.7884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3103  | total loss: \u001b[1m\u001b[32m0.67620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3103 | loss: 0.67620 - acc: 0.7896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3104  | total loss: \u001b[1m\u001b[32m0.67610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3104 | loss: 0.67610 - acc: 0.7906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3105  | total loss: \u001b[1m\u001b[32m0.67600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3105 | loss: 0.67600 - acc: 0.7916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3106  | total loss: \u001b[1m\u001b[32m0.67592\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3106 | loss: 0.67592 - acc: 0.7924 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3107  | total loss: \u001b[1m\u001b[32m0.67584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3107 | loss: 0.67584 - acc: 0.7932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3108  | total loss: \u001b[1m\u001b[32m0.67577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3108 | loss: 0.67577 - acc: 0.7939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3109  | total loss: \u001b[1m\u001b[32m0.67571\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3109 | loss: 0.67571 - acc: 0.7945 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3110  | total loss: \u001b[1m\u001b[32m0.67566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3110 | loss: 0.67566 - acc: 0.7950 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3111  | total loss: \u001b[1m\u001b[32m0.67561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3111 | loss: 0.67561 - acc: 0.7955 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3112  | total loss: \u001b[1m\u001b[32m0.68356\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3112 | loss: 0.68356 - acc: 0.7160 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3113  | total loss: \u001b[1m\u001b[32m0.68272\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3113 | loss: 0.68272 - acc: 0.7244 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3114  | total loss: \u001b[1m\u001b[32m0.68197\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3114 | loss: 0.68197 - acc: 0.7319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3115  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3115 | loss: 0.68128 - acc: 0.7387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3116  | total loss: \u001b[1m\u001b[32m0.68067\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3116 | loss: 0.68067 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3117  | total loss: \u001b[1m\u001b[32m0.68012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3117 | loss: 0.68012 - acc: 0.7504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3118  | total loss: \u001b[1m\u001b[32m0.67962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3118 | loss: 0.67962 - acc: 0.7553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3119  | total loss: \u001b[1m\u001b[32m0.67918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3119 | loss: 0.67918 - acc: 0.7598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3120  | total loss: \u001b[1m\u001b[32m0.68478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3120 | loss: 0.68478 - acc: 0.7038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3121  | total loss: \u001b[1m\u001b[32m0.68381\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3121 | loss: 0.68381 - acc: 0.7134 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3122  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3122 | loss: 0.69095 - acc: 0.6421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3123  | total loss: \u001b[1m\u001b[32m0.68937\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3123 | loss: 0.68937 - acc: 0.6579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3124  | total loss: \u001b[1m\u001b[32m0.68795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3124 | loss: 0.68795 - acc: 0.6721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3125  | total loss: \u001b[1m\u001b[32m0.68667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3125 | loss: 0.68667 - acc: 0.6849 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3126  | total loss: \u001b[1m\u001b[32m0.68552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3126 | loss: 0.68552 - acc: 0.6964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3127  | total loss: \u001b[1m\u001b[32m0.68448\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3127 | loss: 0.68448 - acc: 0.7068 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3128  | total loss: \u001b[1m\u001b[32m0.68355\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3128 | loss: 0.68355 - acc: 0.7161 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3129  | total loss: \u001b[1m\u001b[32m0.68271\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3129 | loss: 0.68271 - acc: 0.7245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3130  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3130 | loss: 0.68196 - acc: 0.7320 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3131  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3131 | loss: 0.68128 - acc: 0.7388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3132  | total loss: \u001b[1m\u001b[32m0.68066\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3132 | loss: 0.68066 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3133  | total loss: \u001b[1m\u001b[32m0.68011\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3133 | loss: 0.68011 - acc: 0.7504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3134  | total loss: \u001b[1m\u001b[32m0.67962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3134 | loss: 0.67962 - acc: 0.7554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3135  | total loss: \u001b[1m\u001b[32m0.67917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3135 | loss: 0.67917 - acc: 0.7599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3136  | total loss: \u001b[1m\u001b[32m0.67877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3136 | loss: 0.67877 - acc: 0.7639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3137  | total loss: \u001b[1m\u001b[32m0.67841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3137 | loss: 0.67841 - acc: 0.7675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3138  | total loss: \u001b[1m\u001b[32m0.68408\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3138 | loss: 0.68408 - acc: 0.7107 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3139  | total loss: \u001b[1m\u001b[32m0.68319\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3139 | loss: 0.68319 - acc: 0.7197 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3140  | total loss: \u001b[1m\u001b[32m0.68239\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3140 | loss: 0.68239 - acc: 0.7277 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3141  | total loss: \u001b[1m\u001b[32m0.68167\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3141 | loss: 0.68167 - acc: 0.7349 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3142  | total loss: \u001b[1m\u001b[32m0.68102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3142 | loss: 0.68102 - acc: 0.7414 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3143  | total loss: \u001b[1m\u001b[32m0.68043\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3143 | loss: 0.68043 - acc: 0.7473 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3144  | total loss: \u001b[1m\u001b[32m0.67990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3144 | loss: 0.67990 - acc: 0.7526 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3145  | total loss: \u001b[1m\u001b[32m0.67943\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3145 | loss: 0.67943 - acc: 0.7573 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3146  | total loss: \u001b[1m\u001b[32m0.67900\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3146 | loss: 0.67900 - acc: 0.7616 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3147  | total loss: \u001b[1m\u001b[32m0.67862\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3147 | loss: 0.67862 - acc: 0.7654 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3148  | total loss: \u001b[1m\u001b[32m0.67827\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3148 | loss: 0.67827 - acc: 0.7689 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3149  | total loss: \u001b[1m\u001b[32m0.67796\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3149 | loss: 0.67796 - acc: 0.7720 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3150  | total loss: \u001b[1m\u001b[32m0.67768\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3150 | loss: 0.67768 - acc: 0.7748 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3151  | total loss: \u001b[1m\u001b[32m0.67743\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3151 | loss: 0.67743 - acc: 0.7773 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3152  | total loss: \u001b[1m\u001b[32m0.67720\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3152 | loss: 0.67720 - acc: 0.7796 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3153  | total loss: \u001b[1m\u001b[32m0.67700\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3153 | loss: 0.67700 - acc: 0.7816 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3154  | total loss: \u001b[1m\u001b[32m0.67681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3154 | loss: 0.67681 - acc: 0.7835 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3155  | total loss: \u001b[1m\u001b[32m0.67665\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3155 | loss: 0.67665 - acc: 0.7851 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3156  | total loss: \u001b[1m\u001b[32m0.68250\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3156 | loss: 0.68250 - acc: 0.7266 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3157  | total loss: \u001b[1m\u001b[32m0.68176\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3157 | loss: 0.68176 - acc: 0.7339 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3158  | total loss: \u001b[1m\u001b[32m0.68110\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3158 | loss: 0.68110 - acc: 0.7405 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3159  | total loss: \u001b[1m\u001b[32m0.68051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3159 | loss: 0.68051 - acc: 0.7465 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3160  | total loss: \u001b[1m\u001b[32m0.67997\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3160 | loss: 0.67997 - acc: 0.7518 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3161  | total loss: \u001b[1m\u001b[32m0.67949\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3161 | loss: 0.67949 - acc: 0.7567 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3162  | total loss: \u001b[1m\u001b[32m0.68506\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3162 | loss: 0.68506 - acc: 0.7010 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3163  | total loss: \u001b[1m\u001b[32m0.68407\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3163 | loss: 0.68407 - acc: 0.7109 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3164  | total loss: \u001b[1m\u001b[32m0.68318\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3164 | loss: 0.68318 - acc: 0.7198 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3165  | total loss: \u001b[1m\u001b[32m0.68238\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3165 | loss: 0.68238 - acc: 0.7278 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3166  | total loss: \u001b[1m\u001b[32m0.68165\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3166 | loss: 0.68165 - acc: 0.7350 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3167  | total loss: \u001b[1m\u001b[32m0.68101\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3167 | loss: 0.68101 - acc: 0.7415 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3168  | total loss: \u001b[1m\u001b[32m0.68042\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3168 | loss: 0.68042 - acc: 0.7474 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3169  | total loss: \u001b[1m\u001b[32m0.67989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3169 | loss: 0.67989 - acc: 0.7526 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3170  | total loss: \u001b[1m\u001b[32m0.67942\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3170 | loss: 0.67942 - acc: 0.7574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3171  | total loss: \u001b[1m\u001b[32m0.67899\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3171 | loss: 0.67899 - acc: 0.7616 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3172  | total loss: \u001b[1m\u001b[32m0.68561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3172 | loss: 0.68561 - acc: 0.6955 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3173  | total loss: \u001b[1m\u001b[32m0.68457\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3173 | loss: 0.68457 - acc: 0.7059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3174  | total loss: \u001b[1m\u001b[32m0.68363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3174 | loss: 0.68363 - acc: 0.7153 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3175  | total loss: \u001b[1m\u001b[32m0.68278\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3175 | loss: 0.68278 - acc: 0.7238 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3176  | total loss: \u001b[1m\u001b[32m0.68202\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3176 | loss: 0.68202 - acc: 0.7314 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3177  | total loss: \u001b[1m\u001b[32m0.68133\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3177 | loss: 0.68133 - acc: 0.7383 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3178  | total loss: \u001b[1m\u001b[32m0.68071\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3178 | loss: 0.68071 - acc: 0.7445 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3179  | total loss: \u001b[1m\u001b[32m0.68016\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3179 | loss: 0.68016 - acc: 0.7500 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3180  | total loss: \u001b[1m\u001b[32m0.68666\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3180 | loss: 0.68666 - acc: 0.6850 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3181  | total loss: \u001b[1m\u001b[32m0.68551\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3181 | loss: 0.68551 - acc: 0.6965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3182  | total loss: \u001b[1m\u001b[32m0.68447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3182 | loss: 0.68447 - acc: 0.7069 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3183  | total loss: \u001b[1m\u001b[32m0.68354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3183 | loss: 0.68354 - acc: 0.7162 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3184  | total loss: \u001b[1m\u001b[32m0.69070\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3184 | loss: 0.69070 - acc: 0.6446 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3185  | total loss: \u001b[1m\u001b[32m0.68915\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3185 | loss: 0.68915 - acc: 0.6601 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3186  | total loss: \u001b[1m\u001b[32m0.69575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3186 | loss: 0.69575 - acc: 0.5941 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3187  | total loss: \u001b[1m\u001b[32m0.69369\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3187 | loss: 0.69369 - acc: 0.6147 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3188  | total loss: \u001b[1m\u001b[32m0.69184\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3188 | loss: 0.69184 - acc: 0.6332 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3189  | total loss: \u001b[1m\u001b[32m0.69017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3189 | loss: 0.69017 - acc: 0.6499 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3190  | total loss: \u001b[1m\u001b[32m0.68867\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3190 | loss: 0.68867 - acc: 0.6649 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3191  | total loss: \u001b[1m\u001b[32m0.68732\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3191 | loss: 0.68732 - acc: 0.6784 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3192  | total loss: \u001b[1m\u001b[32m0.68610\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3192 | loss: 0.68610 - acc: 0.6906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3193  | total loss: \u001b[1m\u001b[32m0.68501\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3193 | loss: 0.68501 - acc: 0.7015 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3194  | total loss: \u001b[1m\u001b[32m0.68402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3194 | loss: 0.68402 - acc: 0.7114 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3195  | total loss: \u001b[1m\u001b[32m0.68314\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3195 | loss: 0.68314 - acc: 0.7202 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3196  | total loss: \u001b[1m\u001b[32m0.68234\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3196 | loss: 0.68234 - acc: 0.7282 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3197  | total loss: \u001b[1m\u001b[32m0.68162\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3197 | loss: 0.68162 - acc: 0.7354 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3198  | total loss: \u001b[1m\u001b[32m0.68097\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3198 | loss: 0.68097 - acc: 0.7418 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3199  | total loss: \u001b[1m\u001b[32m0.68039\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3199 | loss: 0.68039 - acc: 0.7477 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3200  | total loss: \u001b[1m\u001b[32m0.67987\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3200 | loss: 0.67987 - acc: 0.7529 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3201  | total loss: \u001b[1m\u001b[32m0.67940\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3201 | loss: 0.67940 - acc: 0.7576 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3202  | total loss: \u001b[1m\u001b[32m0.67897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3202 | loss: 0.67897 - acc: 0.7618 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3203  | total loss: \u001b[1m\u001b[32m0.67859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3203 | loss: 0.67859 - acc: 0.7657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3204  | total loss: \u001b[1m\u001b[32m0.67825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3204 | loss: 0.67825 - acc: 0.7691 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3205  | total loss: \u001b[1m\u001b[32m0.67794\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3205 | loss: 0.67794 - acc: 0.7722 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3206  | total loss: \u001b[1m\u001b[32m0.68466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3206 | loss: 0.68466 - acc: 0.7050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3207  | total loss: \u001b[1m\u001b[32m0.68371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3207 | loss: 0.68371 - acc: 0.7145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3208  | total loss: \u001b[1m\u001b[32m0.68286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3208 | loss: 0.68286 - acc: 0.7230 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3209  | total loss: \u001b[1m\u001b[32m0.68209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3209 | loss: 0.68209 - acc: 0.7307 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3210  | total loss: \u001b[1m\u001b[32m0.68139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3210 | loss: 0.68139 - acc: 0.7376 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3211  | total loss: \u001b[1m\u001b[32m0.68077\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3211 | loss: 0.68077 - acc: 0.7439 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3212  | total loss: \u001b[1m\u001b[32m0.68021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3212 | loss: 0.68021 - acc: 0.7495 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3213  | total loss: \u001b[1m\u001b[32m0.67970\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3213 | loss: 0.67970 - acc: 0.7545 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3214  | total loss: \u001b[1m\u001b[32m0.67925\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3214 | loss: 0.67925 - acc: 0.7591 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3215  | total loss: \u001b[1m\u001b[32m0.67884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3215 | loss: 0.67884 - acc: 0.7632 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3216  | total loss: \u001b[1m\u001b[32m0.67847\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3216 | loss: 0.67847 - acc: 0.7669 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3217  | total loss: \u001b[1m\u001b[32m0.67814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3217 | loss: 0.67814 - acc: 0.7702 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3218  | total loss: \u001b[1m\u001b[32m0.67784\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3218 | loss: 0.67784 - acc: 0.7732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3219  | total loss: \u001b[1m\u001b[32m0.67757\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3219 | loss: 0.67757 - acc: 0.7758 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3220  | total loss: \u001b[1m\u001b[32m0.67733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3220 | loss: 0.67733 - acc: 0.7783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3221  | total loss: \u001b[1m\u001b[32m0.67712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3221 | loss: 0.67712 - acc: 0.7804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3222  | total loss: \u001b[1m\u001b[32m0.67692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3222 | loss: 0.67692 - acc: 0.7824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3223  | total loss: \u001b[1m\u001b[32m0.67674\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3223 | loss: 0.67674 - acc: 0.7842 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3224  | total loss: \u001b[1m\u001b[32m0.67659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3224 | loss: 0.67659 - acc: 0.7857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3225  | total loss: \u001b[1m\u001b[32m0.67644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3225 | loss: 0.67644 - acc: 0.7872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3226  | total loss: \u001b[1m\u001b[32m0.67631\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3226 | loss: 0.67631 - acc: 0.7884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3227  | total loss: \u001b[1m\u001b[32m0.67620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3227 | loss: 0.67620 - acc: 0.7896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3228  | total loss: \u001b[1m\u001b[32m0.68209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3228 | loss: 0.68209 - acc: 0.7306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3229  | total loss: \u001b[1m\u001b[32m0.68140\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3229 | loss: 0.68140 - acc: 0.7376 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3230  | total loss: \u001b[1m\u001b[32m0.68078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3230 | loss: 0.68078 - acc: 0.7438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3231  | total loss: \u001b[1m\u001b[32m0.68022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3231 | loss: 0.68022 - acc: 0.7494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3232  | total loss: \u001b[1m\u001b[32m0.67971\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3232 | loss: 0.67971 - acc: 0.7545 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3233  | total loss: \u001b[1m\u001b[32m0.67925\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3233 | loss: 0.67925 - acc: 0.7590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3234  | total loss: \u001b[1m\u001b[32m0.67884\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3234 | loss: 0.67884 - acc: 0.7631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3235  | total loss: \u001b[1m\u001b[32m0.67848\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3235 | loss: 0.67848 - acc: 0.7668 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3236  | total loss: \u001b[1m\u001b[32m0.67814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3236 | loss: 0.67814 - acc: 0.7701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3237  | total loss: \u001b[1m\u001b[32m0.67785\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3237 | loss: 0.67785 - acc: 0.7731 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3238  | total loss: \u001b[1m\u001b[32m0.67758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3238 | loss: 0.67758 - acc: 0.7758 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3239  | total loss: \u001b[1m\u001b[32m0.67734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3239 | loss: 0.67734 - acc: 0.7782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3240  | total loss: \u001b[1m\u001b[32m0.67712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3240 | loss: 0.67712 - acc: 0.7804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3241  | total loss: \u001b[1m\u001b[32m0.67692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3241 | loss: 0.67692 - acc: 0.7824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3242  | total loss: \u001b[1m\u001b[32m0.67675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3242 | loss: 0.67675 - acc: 0.7841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3243  | total loss: \u001b[1m\u001b[32m0.67659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3243 | loss: 0.67659 - acc: 0.7857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3244  | total loss: \u001b[1m\u001b[32m0.67644\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3244 | loss: 0.67644 - acc: 0.7871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3245  | total loss: \u001b[1m\u001b[32m0.67632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3245 | loss: 0.67632 - acc: 0.7884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3246  | total loss: \u001b[1m\u001b[32m0.67620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3246 | loss: 0.67620 - acc: 0.7896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3247  | total loss: \u001b[1m\u001b[32m0.67610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3247 | loss: 0.67610 - acc: 0.7906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3248  | total loss: \u001b[1m\u001b[32m0.67600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3248 | loss: 0.67600 - acc: 0.7916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3249  | total loss: \u001b[1m\u001b[32m0.67592\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3249 | loss: 0.67592 - acc: 0.7924 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3250  | total loss: \u001b[1m\u001b[32m0.67584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3250 | loss: 0.67584 - acc: 0.7932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3251  | total loss: \u001b[1m\u001b[32m0.67577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3251 | loss: 0.67577 - acc: 0.7939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3252  | total loss: \u001b[1m\u001b[32m0.68371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3252 | loss: 0.68371 - acc: 0.7145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3253  | total loss: \u001b[1m\u001b[32m0.68286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3253 | loss: 0.68286 - acc: 0.7230 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3254  | total loss: \u001b[1m\u001b[32m0.69009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3254 | loss: 0.69009 - acc: 0.6507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3255  | total loss: \u001b[1m\u001b[32m0.68859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3255 | loss: 0.68859 - acc: 0.6656 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3256  | total loss: \u001b[1m\u001b[32m0.68725\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3256 | loss: 0.68725 - acc: 0.6791 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3257  | total loss: \u001b[1m\u001b[32m0.68604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3257 | loss: 0.68604 - acc: 0.6912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3258  | total loss: \u001b[1m\u001b[32m0.68495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3258 | loss: 0.68495 - acc: 0.7021 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3259  | total loss: \u001b[1m\u001b[32m0.68397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3259 | loss: 0.68397 - acc: 0.7119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3260  | total loss: \u001b[1m\u001b[32m0.69109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3260 | loss: 0.69109 - acc: 0.6407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3261  | total loss: \u001b[1m\u001b[32m0.68950\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3261 | loss: 0.68950 - acc: 0.6566 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3262  | total loss: \u001b[1m\u001b[32m0.68806\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3262 | loss: 0.68806 - acc: 0.6709 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3263  | total loss: \u001b[1m\u001b[32m0.68677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3263 | loss: 0.68677 - acc: 0.6838 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3264  | total loss: \u001b[1m\u001b[32m0.68561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3264 | loss: 0.68561 - acc: 0.6955 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3265  | total loss: \u001b[1m\u001b[32m0.68457\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3265 | loss: 0.68457 - acc: 0.7059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3266  | total loss: \u001b[1m\u001b[32m0.68363\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3266 | loss: 0.68363 - acc: 0.7153 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3267  | total loss: \u001b[1m\u001b[32m0.68278\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3267 | loss: 0.68278 - acc: 0.7238 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3268  | total loss: \u001b[1m\u001b[32m0.68202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3268 | loss: 0.68202 - acc: 0.7314 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3269  | total loss: \u001b[1m\u001b[32m0.68133\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3269 | loss: 0.68133 - acc: 0.7383 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3270  | total loss: \u001b[1m\u001b[32m0.68071\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3270 | loss: 0.68071 - acc: 0.7444 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3271  | total loss: \u001b[1m\u001b[32m0.68016\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3271 | loss: 0.68016 - acc: 0.7500 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3272  | total loss: \u001b[1m\u001b[32m0.67966\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3272 | loss: 0.67966 - acc: 0.7550 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3273  | total loss: \u001b[1m\u001b[32m0.67921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3273 | loss: 0.67921 - acc: 0.7595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3274  | total loss: \u001b[1m\u001b[32m0.67880\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3274 | loss: 0.67880 - acc: 0.7635 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3275  | total loss: \u001b[1m\u001b[32m0.67844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3275 | loss: 0.67844 - acc: 0.7672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3276  | total loss: \u001b[1m\u001b[32m0.67811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3276 | loss: 0.67811 - acc: 0.7705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3277  | total loss: \u001b[1m\u001b[32m0.67782\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3277 | loss: 0.67782 - acc: 0.7734 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3278  | total loss: \u001b[1m\u001b[32m0.68455\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3278 | loss: 0.68455 - acc: 0.7061 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3279  | total loss: \u001b[1m\u001b[32m0.68361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3279 | loss: 0.68361 - acc: 0.7155 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3280  | total loss: \u001b[1m\u001b[32m0.68277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3280 | loss: 0.68277 - acc: 0.7239 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3281  | total loss: \u001b[1m\u001b[32m0.68201\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3281 | loss: 0.68201 - acc: 0.7315 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3282  | total loss: \u001b[1m\u001b[32m0.68132\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3282 | loss: 0.68132 - acc: 0.7384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3283  | total loss: \u001b[1m\u001b[32m0.68070\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3283 | loss: 0.68070 - acc: 0.7445 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3284  | total loss: \u001b[1m\u001b[32m0.68015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3284 | loss: 0.68015 - acc: 0.7501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3285  | total loss: \u001b[1m\u001b[32m0.67965\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3285 | loss: 0.67965 - acc: 0.7551 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3286  | total loss: \u001b[1m\u001b[32m0.68620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3286 | loss: 0.68620 - acc: 0.6896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3287  | total loss: \u001b[1m\u001b[32m0.68510\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3287 | loss: 0.68510 - acc: 0.7006 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3288  | total loss: \u001b[1m\u001b[32m0.68410\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3288 | loss: 0.68410 - acc: 0.7106 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3289  | total loss: \u001b[1m\u001b[32m0.68321\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3289 | loss: 0.68321 - acc: 0.7195 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3290  | total loss: \u001b[1m\u001b[32m0.68240\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3290 | loss: 0.68240 - acc: 0.7275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3291  | total loss: \u001b[1m\u001b[32m0.68168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3291 | loss: 0.68168 - acc: 0.7348 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3292  | total loss: \u001b[1m\u001b[32m0.68103\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3292 | loss: 0.68103 - acc: 0.7413 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3293  | total loss: \u001b[1m\u001b[32m0.68044\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3293 | loss: 0.68044 - acc: 0.7472 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3294  | total loss: \u001b[1m\u001b[32m0.67991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3294 | loss: 0.67991 - acc: 0.7525 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3295  | total loss: \u001b[1m\u001b[32m0.67944\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3295 | loss: 0.67944 - acc: 0.7572 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3296  | total loss: \u001b[1m\u001b[32m0.67901\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3296 | loss: 0.67901 - acc: 0.7615 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3297  | total loss: \u001b[1m\u001b[32m0.67862\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3297 | loss: 0.67862 - acc: 0.7653 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3298  | total loss: \u001b[1m\u001b[32m0.67828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3298 | loss: 0.67828 - acc: 0.7688 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3299  | total loss: \u001b[1m\u001b[32m0.67797\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3299 | loss: 0.67797 - acc: 0.7719 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3300  | total loss: \u001b[1m\u001b[32m0.67769\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3300 | loss: 0.67769 - acc: 0.7747 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3301  | total loss: \u001b[1m\u001b[32m0.67743\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3301 | loss: 0.67743 - acc: 0.7773 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3302  | total loss: \u001b[1m\u001b[32m0.67721\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3302 | loss: 0.67721 - acc: 0.7795 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3303  | total loss: \u001b[1m\u001b[32m0.67700\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3303 | loss: 0.67700 - acc: 0.7816 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3304  | total loss: \u001b[1m\u001b[32m0.67682\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3304 | loss: 0.67682 - acc: 0.7834 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3305  | total loss: \u001b[1m\u001b[32m0.67665\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3305 | loss: 0.67665 - acc: 0.7851 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3306  | total loss: \u001b[1m\u001b[32m0.67650\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3306 | loss: 0.67650 - acc: 0.7866 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3307  | total loss: \u001b[1m\u001b[32m0.67637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3307 | loss: 0.67637 - acc: 0.7879 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3308  | total loss: \u001b[1m\u001b[32m0.67625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3308 | loss: 0.67625 - acc: 0.7891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3309  | total loss: \u001b[1m\u001b[32m0.67614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3309 | loss: 0.67614 - acc: 0.7902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3310  | total loss: \u001b[1m\u001b[32m0.68404\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3310 | loss: 0.68404 - acc: 0.7112 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3311  | total loss: \u001b[1m\u001b[32m0.68315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3311 | loss: 0.68315 - acc: 0.7201 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3312  | total loss: \u001b[1m\u001b[32m0.68235\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3312 | loss: 0.68235 - acc: 0.7281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3313  | total loss: \u001b[1m\u001b[32m0.68163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3313 | loss: 0.68163 - acc: 0.7353 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3314  | total loss: \u001b[1m\u001b[32m0.68799\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3314 | loss: 0.68799 - acc: 0.6717 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3315  | total loss: \u001b[1m\u001b[32m0.68670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3315 | loss: 0.68670 - acc: 0.6846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3316  | total loss: \u001b[1m\u001b[32m0.69255\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3316 | loss: 0.69255 - acc: 0.6261 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3317  | total loss: \u001b[1m\u001b[32m0.69081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3317 | loss: 0.69081 - acc: 0.6435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3318  | total loss: \u001b[1m\u001b[32m0.69524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3318 | loss: 0.69524 - acc: 0.5991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3319  | total loss: \u001b[1m\u001b[32m0.69324\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3319 | loss: 0.69324 - acc: 0.6192 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3320  | total loss: \u001b[1m\u001b[32m0.69143\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3320 | loss: 0.69143 - acc: 0.6373 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3321  | total loss: \u001b[1m\u001b[32m0.68980\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3321 | loss: 0.68980 - acc: 0.6536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3322  | total loss: \u001b[1m\u001b[32m0.68834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3322 | loss: 0.68834 - acc: 0.6682 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3323  | total loss: \u001b[1m\u001b[32m0.68702\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3323 | loss: 0.68702 - acc: 0.6814 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3324  | total loss: \u001b[1m\u001b[32m0.68583\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3324 | loss: 0.68583 - acc: 0.6933 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3325  | total loss: \u001b[1m\u001b[32m0.68477\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3325 | loss: 0.68477 - acc: 0.7039 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3326  | total loss: \u001b[1m\u001b[32m0.69081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3326 | loss: 0.69081 - acc: 0.6435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3327  | total loss: \u001b[1m\u001b[32m0.68924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3327 | loss: 0.68924 - acc: 0.6592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3328  | total loss: \u001b[1m\u001b[32m0.69583\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3328 | loss: 0.69583 - acc: 0.5933 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3329  | total loss: \u001b[1m\u001b[32m0.69376\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3329 | loss: 0.69376 - acc: 0.6139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3330  | total loss: \u001b[1m\u001b[32m0.69190\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3330 | loss: 0.69190 - acc: 0.6325 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3331  | total loss: \u001b[1m\u001b[32m0.69023\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3331 | loss: 0.69023 - acc: 0.6493 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3332  | total loss: \u001b[1m\u001b[32m0.69572\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3332 | loss: 0.69572 - acc: 0.5944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3333  | total loss: \u001b[1m\u001b[32m0.69367\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3333 | loss: 0.69367 - acc: 0.6149 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3334  | total loss: \u001b[1m\u001b[32m0.69182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3334 | loss: 0.69182 - acc: 0.6334 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3335  | total loss: \u001b[1m\u001b[32m0.69015\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3335 | loss: 0.69015 - acc: 0.6501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3336  | total loss: \u001b[1m\u001b[32m0.69465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3336 | loss: 0.69465 - acc: 0.6051 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3337  | total loss: \u001b[1m\u001b[32m0.69270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3337 | loss: 0.69270 - acc: 0.6246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3338  | total loss: \u001b[1m\u001b[32m0.69095\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3338 | loss: 0.69095 - acc: 0.6421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3339  | total loss: \u001b[1m\u001b[32m0.68937\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3339 | loss: 0.68937 - acc: 0.6579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3340  | total loss: \u001b[1m\u001b[32m0.68795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3340 | loss: 0.68795 - acc: 0.6721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3341  | total loss: \u001b[1m\u001b[32m0.68667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3341 | loss: 0.68667 - acc: 0.6849 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3342  | total loss: \u001b[1m\u001b[32m0.68552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3342 | loss: 0.68552 - acc: 0.6964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3343  | total loss: \u001b[1m\u001b[32m0.68448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3343 | loss: 0.68448 - acc: 0.7068 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3344  | total loss: \u001b[1m\u001b[32m0.68355\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3344 | loss: 0.68355 - acc: 0.7161 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3345  | total loss: \u001b[1m\u001b[32m0.68271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3345 | loss: 0.68271 - acc: 0.7245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3346  | total loss: \u001b[1m\u001b[32m0.68196\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3346 | loss: 0.68196 - acc: 0.7320 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3347  | total loss: \u001b[1m\u001b[32m0.68128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3347 | loss: 0.68128 - acc: 0.7388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3348  | total loss: \u001b[1m\u001b[32m0.68066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3348 | loss: 0.68066 - acc: 0.7449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3349  | total loss: \u001b[1m\u001b[32m0.68011\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3349 | loss: 0.68011 - acc: 0.7505 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3350  | total loss: \u001b[1m\u001b[32m0.67962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3350 | loss: 0.67962 - acc: 0.7554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3351  | total loss: \u001b[1m\u001b[32m0.67917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3351 | loss: 0.67917 - acc: 0.7599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3352  | total loss: \u001b[1m\u001b[32m0.67877\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3352 | loss: 0.67877 - acc: 0.7639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3353  | total loss: \u001b[1m\u001b[32m0.67841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3353 | loss: 0.67841 - acc: 0.7675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3354  | total loss: \u001b[1m\u001b[32m0.67808\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3354 | loss: 0.67808 - acc: 0.7707 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3355  | total loss: \u001b[1m\u001b[32m0.67779\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3355 | loss: 0.67779 - acc: 0.7737 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3356  | total loss: \u001b[1m\u001b[32m0.67753\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3356 | loss: 0.67753 - acc: 0.7763 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3357  | total loss: \u001b[1m\u001b[32m0.67729\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3357 | loss: 0.67729 - acc: 0.7787 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3358  | total loss: \u001b[1m\u001b[32m0.67708\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3358 | loss: 0.67708 - acc: 0.7808 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3359  | total loss: \u001b[1m\u001b[32m0.67689\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3359 | loss: 0.67689 - acc: 0.7827 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3360  | total loss: \u001b[1m\u001b[32m0.67671\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3360 | loss: 0.67671 - acc: 0.7845 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3361  | total loss: \u001b[1m\u001b[32m0.67656\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3361 | loss: 0.67656 - acc: 0.7860 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3362  | total loss: \u001b[1m\u001b[32m0.67642\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3362 | loss: 0.67642 - acc: 0.7874 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3363  | total loss: \u001b[1m\u001b[32m0.67629\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3363 | loss: 0.67629 - acc: 0.7887 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3364  | total loss: \u001b[1m\u001b[32m0.67618\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3364 | loss: 0.67618 - acc: 0.7898 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3365  | total loss: \u001b[1m\u001b[32m0.67608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3365 | loss: 0.67608 - acc: 0.7908 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3366  | total loss: \u001b[1m\u001b[32m0.67599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3366 | loss: 0.67599 - acc: 0.7917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3367  | total loss: \u001b[1m\u001b[32m0.67590\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3367 | loss: 0.67590 - acc: 0.7926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3368  | total loss: \u001b[1m\u001b[32m0.67583\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3368 | loss: 0.67583 - acc: 0.7933 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3369  | total loss: \u001b[1m\u001b[32m0.67576\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3369 | loss: 0.67576 - acc: 0.7940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3370  | total loss: \u001b[1m\u001b[32m0.68270\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3370 | loss: 0.68270 - acc: 0.7246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3371  | total loss: \u001b[1m\u001b[32m0.68195\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3371 | loss: 0.68195 - acc: 0.7321 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3372  | total loss: \u001b[1m\u001b[32m0.68127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3372 | loss: 0.68127 - acc: 0.7389 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3373  | total loss: \u001b[1m\u001b[32m0.68066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3373 | loss: 0.68066 - acc: 0.7450 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3374  | total loss: \u001b[1m\u001b[32m0.68011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3374 | loss: 0.68011 - acc: 0.7505 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3375  | total loss: \u001b[1m\u001b[32m0.67961\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3375 | loss: 0.67961 - acc: 0.7555 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3376  | total loss: \u001b[1m\u001b[32m0.68517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3376 | loss: 0.68517 - acc: 0.6999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3377  | total loss: \u001b[1m\u001b[32m0.68417\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3377 | loss: 0.68417 - acc: 0.7099 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3378  | total loss: \u001b[1m\u001b[32m0.68327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3378 | loss: 0.68327 - acc: 0.7189 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3379  | total loss: \u001b[1m\u001b[32m0.68245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3379 | loss: 0.68245 - acc: 0.7270 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3380  | total loss: \u001b[1m\u001b[32m0.68173\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3380 | loss: 0.68173 - acc: 0.7343 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3381  | total loss: \u001b[1m\u001b[32m0.68107\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3381 | loss: 0.68107 - acc: 0.7409 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3382  | total loss: \u001b[1m\u001b[32m0.68048\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3382 | loss: 0.68048 - acc: 0.7468 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3383  | total loss: \u001b[1m\u001b[32m0.67995\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3383 | loss: 0.67995 - acc: 0.7521 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3384  | total loss: \u001b[1m\u001b[32m0.67947\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3384 | loss: 0.67947 - acc: 0.7569 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3385  | total loss: \u001b[1m\u001b[32m0.67904\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3385 | loss: 0.67904 - acc: 0.7612 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3386  | total loss: \u001b[1m\u001b[32m0.67865\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3386 | loss: 0.67865 - acc: 0.7651 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3387  | total loss: \u001b[1m\u001b[32m0.67830\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3387 | loss: 0.67830 - acc: 0.7686 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3388  | total loss: \u001b[1m\u001b[32m0.67799\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3388 | loss: 0.67799 - acc: 0.7717 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3389  | total loss: \u001b[1m\u001b[32m0.67770\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3389 | loss: 0.67770 - acc: 0.7746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3390  | total loss: \u001b[1m\u001b[32m0.67745\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3390 | loss: 0.67745 - acc: 0.7771 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3391  | total loss: \u001b[1m\u001b[32m0.67722\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3391 | loss: 0.67722 - acc: 0.7794 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3392  | total loss: \u001b[1m\u001b[32m0.67701\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3392 | loss: 0.67701 - acc: 0.7815 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3393  | total loss: \u001b[1m\u001b[32m0.67683\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3393 | loss: 0.67683 - acc: 0.7833 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3394  | total loss: \u001b[1m\u001b[32m0.67666\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3394 | loss: 0.67666 - acc: 0.7850 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3395  | total loss: \u001b[1m\u001b[32m0.67651\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3395 | loss: 0.67651 - acc: 0.7865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3396  | total loss: \u001b[1m\u001b[32m0.67638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3396 | loss: 0.67638 - acc: 0.7878 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3397  | total loss: \u001b[1m\u001b[32m0.67625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3397 | loss: 0.67625 - acc: 0.7890 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3398  | total loss: \u001b[1m\u001b[32m0.67614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3398 | loss: 0.67614 - acc: 0.7901 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3399  | total loss: \u001b[1m\u001b[32m0.67605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3399 | loss: 0.67605 - acc: 0.7911 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3400  | total loss: \u001b[1m\u001b[32m0.68396\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3400 | loss: 0.68396 - acc: 0.7120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3401  | total loss: \u001b[1m\u001b[32m0.68308\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3401 | loss: 0.68308 - acc: 0.7208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3402  | total loss: \u001b[1m\u001b[32m0.68229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3402 | loss: 0.68229 - acc: 0.7287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3403  | total loss: \u001b[1m\u001b[32m0.68157\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3403 | loss: 0.68157 - acc: 0.7359 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3404  | total loss: \u001b[1m\u001b[32m0.68093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3404 | loss: 0.68093 - acc: 0.7423 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3405  | total loss: \u001b[1m\u001b[32m0.68035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3405 | loss: 0.68035 - acc: 0.7480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3406  | total loss: \u001b[1m\u001b[32m0.67983\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3406 | loss: 0.67983 - acc: 0.7532 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3407  | total loss: \u001b[1m\u001b[32m0.67937\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3407 | loss: 0.67937 - acc: 0.7579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3408  | total loss: \u001b[1m\u001b[32m0.68595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3408 | loss: 0.68595 - acc: 0.6921 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3409  | total loss: \u001b[1m\u001b[32m0.68487\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3409 | loss: 0.68487 - acc: 0.7029 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3410  | total loss: \u001b[1m\u001b[32m0.68990\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3410 | loss: 0.68990 - acc: 0.6526 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3411  | total loss: \u001b[1m\u001b[32m0.68842\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3411 | loss: 0.68842 - acc: 0.6674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3412  | total loss: \u001b[1m\u001b[32m0.68710\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3412 | loss: 0.68710 - acc: 0.6806 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3413  | total loss: \u001b[1m\u001b[32m0.68590\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3413 | loss: 0.68590 - acc: 0.6926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3414  | total loss: \u001b[1m\u001b[32m0.69183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3414 | loss: 0.69183 - acc: 0.6333 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3415  | total loss: \u001b[1m\u001b[32m0.69016\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3415 | loss: 0.69016 - acc: 0.6500 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3416  | total loss: \u001b[1m\u001b[32m0.68866\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3416 | loss: 0.68866 - acc: 0.6650 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3417  | total loss: \u001b[1m\u001b[32m0.68731\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3417 | loss: 0.68731 - acc: 0.6785 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3418  | total loss: \u001b[1m\u001b[32m0.68610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3418 | loss: 0.68610 - acc: 0.6906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3419  | total loss: \u001b[1m\u001b[32m0.68500\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3419 | loss: 0.68500 - acc: 0.7016 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3420  | total loss: \u001b[1m\u001b[32m0.68402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3420 | loss: 0.68402 - acc: 0.7114 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3421  | total loss: \u001b[1m\u001b[32m0.68313\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3421 | loss: 0.68313 - acc: 0.7203 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3422  | total loss: \u001b[1m\u001b[32m0.68233\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3422 | loss: 0.68233 - acc: 0.7282 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3423  | total loss: \u001b[1m\u001b[32m0.68162\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3423 | loss: 0.68162 - acc: 0.7354 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3424  | total loss: \u001b[1m\u001b[32m0.68897\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3424 | loss: 0.68897 - acc: 0.6619 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3425  | total loss: \u001b[1m\u001b[32m0.68759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3425 | loss: 0.68759 - acc: 0.6757 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3426  | total loss: \u001b[1m\u001b[32m0.68635\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3426 | loss: 0.68635 - acc: 0.6881 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3427  | total loss: \u001b[1m\u001b[32m0.68523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3427 | loss: 0.68523 - acc: 0.6993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3428  | total loss: \u001b[1m\u001b[32m0.69022\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3428 | loss: 0.69022 - acc: 0.6494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3429  | total loss: \u001b[1m\u001b[32m0.68871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3429 | loss: 0.68871 - acc: 0.6644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3430  | total loss: \u001b[1m\u001b[32m0.68736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3430 | loss: 0.68736 - acc: 0.6780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3431  | total loss: \u001b[1m\u001b[32m0.68614\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3431 | loss: 0.68614 - acc: 0.6902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3432  | total loss: \u001b[1m\u001b[32m0.68504\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3432 | loss: 0.68504 - acc: 0.7012 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3433  | total loss: \u001b[1m\u001b[32m0.68405\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3433 | loss: 0.68405 - acc: 0.7111 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3434  | total loss: \u001b[1m\u001b[32m0.68316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3434 | loss: 0.68316 - acc: 0.7200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3435  | total loss: \u001b[1m\u001b[32m0.68236\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3435 | loss: 0.68236 - acc: 0.7280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3436  | total loss: \u001b[1m\u001b[32m0.68164\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3436 | loss: 0.68164 - acc: 0.7352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3437  | total loss: \u001b[1m\u001b[32m0.68099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3437 | loss: 0.68099 - acc: 0.7416 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3438  | total loss: \u001b[1m\u001b[32m0.68041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3438 | loss: 0.68041 - acc: 0.7475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3439  | total loss: \u001b[1m\u001b[32m0.67989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3439 | loss: 0.67989 - acc: 0.7527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3440  | total loss: \u001b[1m\u001b[32m0.67941\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3440 | loss: 0.67941 - acc: 0.7575 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3441  | total loss: \u001b[1m\u001b[32m0.67899\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3441 | loss: 0.67899 - acc: 0.7617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3442  | total loss: \u001b[1m\u001b[32m0.67860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3442 | loss: 0.67860 - acc: 0.7655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3443  | total loss: \u001b[1m\u001b[32m0.67826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3443 | loss: 0.67826 - acc: 0.7690 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3444  | total loss: \u001b[1m\u001b[32m0.67795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3444 | loss: 0.67795 - acc: 0.7721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3445  | total loss: \u001b[1m\u001b[32m0.67767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3445 | loss: 0.67767 - acc: 0.7749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3446  | total loss: \u001b[1m\u001b[32m0.68442\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3446 | loss: 0.68442 - acc: 0.7074 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3447  | total loss: \u001b[1m\u001b[32m0.68349\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3447 | loss: 0.68349 - acc: 0.7167 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3448  | total loss: \u001b[1m\u001b[32m0.68666\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3448 | loss: 0.68666 - acc: 0.6850 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3449  | total loss: \u001b[1m\u001b[32m0.68551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3449 | loss: 0.68551 - acc: 0.6965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3450  | total loss: \u001b[1m\u001b[32m0.68447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3450 | loss: 0.68447 - acc: 0.7068 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3451  | total loss: \u001b[1m\u001b[32m0.68354\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3451 | loss: 0.68354 - acc: 0.7162 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3452  | total loss: \u001b[1m\u001b[32m0.68270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3452 | loss: 0.68270 - acc: 0.7245 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3453  | total loss: \u001b[1m\u001b[32m0.68195\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3453 | loss: 0.68195 - acc: 0.7321 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3454  | total loss: \u001b[1m\u001b[32m0.68127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3454 | loss: 0.68127 - acc: 0.7389 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3455  | total loss: \u001b[1m\u001b[32m0.68066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3455 | loss: 0.68066 - acc: 0.7450 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3456  | total loss: \u001b[1m\u001b[32m0.68811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3456 | loss: 0.68811 - acc: 0.6705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3457  | total loss: \u001b[1m\u001b[32m0.68681\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3457 | loss: 0.68681 - acc: 0.6834 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3458  | total loss: \u001b[1m\u001b[32m0.68565\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3458 | loss: 0.68565 - acc: 0.6951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3459  | total loss: \u001b[1m\u001b[32m0.68460\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3459 | loss: 0.68460 - acc: 0.7056 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3460  | total loss: \u001b[1m\u001b[32m0.69066\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3460 | loss: 0.69066 - acc: 0.6450 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3461  | total loss: \u001b[1m\u001b[32m0.68911\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3461 | loss: 0.68911 - acc: 0.6605 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3462  | total loss: \u001b[1m\u001b[32m0.68771\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3462 | loss: 0.68771 - acc: 0.6745 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3463  | total loss: \u001b[1m\u001b[32m0.68646\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3463 | loss: 0.68646 - acc: 0.6870 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3464  | total loss: \u001b[1m\u001b[32m0.68533\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3464 | loss: 0.68533 - acc: 0.6983 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3465  | total loss: \u001b[1m\u001b[32m0.68431\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3465 | loss: 0.68431 - acc: 0.7085 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3466  | total loss: \u001b[1m\u001b[32m0.68339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3466 | loss: 0.68339 - acc: 0.7176 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3467  | total loss: \u001b[1m\u001b[32m0.68257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3467 | loss: 0.68257 - acc: 0.7259 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3468  | total loss: \u001b[1m\u001b[32m0.68183\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3468 | loss: 0.68183 - acc: 0.7333 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3469  | total loss: \u001b[1m\u001b[32m0.68116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3469 | loss: 0.68116 - acc: 0.7400 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3470  | total loss: \u001b[1m\u001b[32m0.68756\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3470 | loss: 0.68756 - acc: 0.6760 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3471  | total loss: \u001b[1m\u001b[32m0.68632\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3471 | loss: 0.68632 - acc: 0.6884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3472  | total loss: \u001b[1m\u001b[32m0.68521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3472 | loss: 0.68521 - acc: 0.6995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3473  | total loss: \u001b[1m\u001b[32m0.68420\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3473 | loss: 0.68420 - acc: 0.7096 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3474  | total loss: \u001b[1m\u001b[32m0.68330\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3474 | loss: 0.68330 - acc: 0.7186 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3475  | total loss: \u001b[1m\u001b[32m0.68248\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3475 | loss: 0.68248 - acc: 0.7268 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3476  | total loss: \u001b[1m\u001b[32m0.68175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3476 | loss: 0.68175 - acc: 0.7341 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3477  | total loss: \u001b[1m\u001b[32m0.68109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3477 | loss: 0.68109 - acc: 0.7407 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3478  | total loss: \u001b[1m\u001b[32m0.68750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3478 | loss: 0.68750 - acc: 0.6766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3479  | total loss: \u001b[1m\u001b[32m0.68626\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3479 | loss: 0.68626 - acc: 0.6889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3480  | total loss: \u001b[1m\u001b[32m0.69315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3480 | loss: 0.69315 - acc: 0.6201 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3481  | total loss: \u001b[1m\u001b[32m0.69135\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3481 | loss: 0.69135 - acc: 0.6380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3482  | total loss: \u001b[1m\u001b[32m0.69673\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3482 | loss: 0.69673 - acc: 0.5842 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3483  | total loss: \u001b[1m\u001b[32m0.69458\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3483 | loss: 0.69458 - acc: 0.6058 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3484  | total loss: \u001b[1m\u001b[32m0.69864\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3484 | loss: 0.69864 - acc: 0.5652 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3485  | total loss: \u001b[1m\u001b[32m0.69629\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3485 | loss: 0.69629 - acc: 0.5887 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3486  | total loss: \u001b[1m\u001b[32m0.70017\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3486 | loss: 0.70017 - acc: 0.5498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3487  | total loss: \u001b[1m\u001b[32m0.69767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3487 | loss: 0.69767 - acc: 0.5749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3488  | total loss: \u001b[1m\u001b[32m0.69542\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3488 | loss: 0.69542 - acc: 0.5974 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3489  | total loss: \u001b[1m\u001b[32m0.69340\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3489 | loss: 0.69340 - acc: 0.6176 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3490  | total loss: \u001b[1m\u001b[32m0.69857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3490 | loss: 0.69857 - acc: 0.5659 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3491  | total loss: \u001b[1m\u001b[32m0.69623\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3491 | loss: 0.69623 - acc: 0.5893 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3492  | total loss: \u001b[1m\u001b[32m0.69412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3492 | loss: 0.69412 - acc: 0.6104 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3493  | total loss: \u001b[1m\u001b[32m0.69223\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3493 | loss: 0.69223 - acc: 0.6293 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3494  | total loss: \u001b[1m\u001b[32m0.69052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3494 | loss: 0.69052 - acc: 0.6464 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3495  | total loss: \u001b[1m\u001b[32m0.68898\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3495 | loss: 0.68898 - acc: 0.6617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3496  | total loss: \u001b[1m\u001b[32m0.69560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3496 | loss: 0.69560 - acc: 0.5956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3497  | total loss: \u001b[1m\u001b[32m0.69356\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3497 | loss: 0.69356 - acc: 0.6160 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3498  | total loss: \u001b[1m\u001b[32m0.69172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3498 | loss: 0.69172 - acc: 0.6344 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3499  | total loss: \u001b[1m\u001b[32m0.69006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3499 | loss: 0.69006 - acc: 0.6510 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3500  | total loss: \u001b[1m\u001b[32m0.69457\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3500 | loss: 0.69457 - acc: 0.6059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3501  | total loss: \u001b[1m\u001b[32m0.69263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3501 | loss: 0.69263 - acc: 0.6253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3502  | total loss: \u001b[1m\u001b[32m0.69088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3502 | loss: 0.69088 - acc: 0.6428 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3503  | total loss: \u001b[1m\u001b[32m0.68931\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3503 | loss: 0.68931 - acc: 0.6585 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3504  | total loss: \u001b[1m\u001b[32m0.68790\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3504 | loss: 0.68790 - acc: 0.6726 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3505  | total loss: \u001b[1m\u001b[32m0.68662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3505 | loss: 0.68662 - acc: 0.6854 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3506  | total loss: \u001b[1m\u001b[32m0.69348\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3506 | loss: 0.69348 - acc: 0.6168 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3507  | total loss: \u001b[1m\u001b[32m0.69164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3507 | loss: 0.69164 - acc: 0.6352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3508  | total loss: \u001b[1m\u001b[32m0.69699\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3508 | loss: 0.69699 - acc: 0.5816 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3509  | total loss: \u001b[1m\u001b[32m0.69481\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3509 | loss: 0.69481 - acc: 0.6035 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3510  | total loss: \u001b[1m\u001b[32m0.69985\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3510 | loss: 0.69985 - acc: 0.5531 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3511  | total loss: \u001b[1m\u001b[32m0.69738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3511 | loss: 0.69738 - acc: 0.5778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3512  | total loss: \u001b[1m\u001b[32m0.69516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3512 | loss: 0.69516 - acc: 0.6000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3513  | total loss: \u001b[1m\u001b[32m0.69316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3513 | loss: 0.69316 - acc: 0.6200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3514  | total loss: \u001b[1m\u001b[32m0.69136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3514 | loss: 0.69136 - acc: 0.6380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3515  | total loss: \u001b[1m\u001b[32m0.68974\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3515 | loss: 0.68974 - acc: 0.6542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3516  | total loss: \u001b[1m\u001b[32m0.68828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3516 | loss: 0.68828 - acc: 0.6688 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3517  | total loss: \u001b[1m\u001b[32m0.68697\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3517 | loss: 0.68697 - acc: 0.6819 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3518  | total loss: \u001b[1m\u001b[32m0.69279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3518 | loss: 0.69279 - acc: 0.6237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3519  | total loss: \u001b[1m\u001b[32m0.69102\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3519 | loss: 0.69102 - acc: 0.6414 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3520  | total loss: \u001b[1m\u001b[32m0.69744\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3520 | loss: 0.69744 - acc: 0.5772 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3521  | total loss: \u001b[1m\u001b[32m0.69521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3521 | loss: 0.69521 - acc: 0.5995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3522  | total loss: \u001b[1m\u001b[32m0.70020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3522 | loss: 0.70020 - acc: 0.5495 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3523  | total loss: \u001b[1m\u001b[32m0.69770\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3523 | loss: 0.69770 - acc: 0.5746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3524  | total loss: \u001b[1m\u001b[32m0.69544\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3524 | loss: 0.69544 - acc: 0.5971 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3525  | total loss: \u001b[1m\u001b[32m0.69342\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3525 | loss: 0.69342 - acc: 0.6174 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3526  | total loss: \u001b[1m\u001b[32m0.69959\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3526 | loss: 0.69959 - acc: 0.5557 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3527  | total loss: \u001b[1m\u001b[32m0.69715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3527 | loss: 0.69715 - acc: 0.5801 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3528  | total loss: \u001b[1m\u001b[32m0.69495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3528 | loss: 0.69495 - acc: 0.6021 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3529  | total loss: \u001b[1m\u001b[32m0.69297\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3529 | loss: 0.69297 - acc: 0.6219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3530  | total loss: \u001b[1m\u001b[32m0.69119\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3530 | loss: 0.69119 - acc: 0.6397 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3531  | total loss: \u001b[1m\u001b[32m0.68958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3531 | loss: 0.68958 - acc: 0.6557 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3532  | total loss: \u001b[1m\u001b[32m0.68814\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3532 | loss: 0.68814 - acc: 0.6702 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3533  | total loss: \u001b[1m\u001b[32m0.68684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3533 | loss: 0.68684 - acc: 0.6831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3534  | total loss: \u001b[1m\u001b[32m0.68567\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3534 | loss: 0.68567 - acc: 0.6948 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3535  | total loss: \u001b[1m\u001b[32m0.68462\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3535 | loss: 0.68462 - acc: 0.7053 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3536  | total loss: \u001b[1m\u001b[32m0.68367\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3536 | loss: 0.68367 - acc: 0.7148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3537  | total loss: \u001b[1m\u001b[32m0.68281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3537 | loss: 0.68281 - acc: 0.7233 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3538  | total loss: \u001b[1m\u001b[32m0.68780\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3538 | loss: 0.68780 - acc: 0.6710 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3539  | total loss: \u001b[1m\u001b[32m0.68754\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3539 | loss: 0.68754 - acc: 0.6739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3540  | total loss: \u001b[1m\u001b[32m0.68838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3540 | loss: 0.68838 - acc: 0.6665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3541  | total loss: \u001b[1m\u001b[32m0.68914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3541 | loss: 0.68914 - acc: 0.6599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3542  | total loss: \u001b[1m\u001b[32m0.69368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3542 | loss: 0.69368 - acc: 0.6139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3543  | total loss: \u001b[1m\u001b[32m0.69384\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3543 | loss: 0.69384 - acc: 0.6125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3544  | total loss: \u001b[1m\u001b[32m0.69896\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3544 | loss: 0.69896 - acc: 0.5612 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3545  | total loss: \u001b[1m\u001b[32m0.69890\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3545 | loss: 0.69890 - acc: 0.5651 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3546  | total loss: \u001b[1m\u001b[32m0.69853\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3546 | loss: 0.69853 - acc: 0.5686 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3547  | total loss: \u001b[1m\u001b[32m0.69884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3547 | loss: 0.69884 - acc: 0.5617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3548  | total loss: \u001b[1m\u001b[32m0.70442\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3548 | loss: 0.70442 - acc: 0.5056 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3549  | total loss: \u001b[1m\u001b[32m0.70449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3549 | loss: 0.70449 - acc: 0.5050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3550  | total loss: \u001b[1m\u001b[32m0.70467\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3550 | loss: 0.70467 - acc: 0.5045 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3551  | total loss: \u001b[1m\u001b[32m0.70477\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3551 | loss: 0.70477 - acc: 0.5041 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3552  | total loss: \u001b[1m\u001b[32m0.70481\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3552 | loss: 0.70481 - acc: 0.5037 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3553  | total loss: \u001b[1m\u001b[32m0.70485\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3553 | loss: 0.70485 - acc: 0.5033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3554  | total loss: \u001b[1m\u001b[32m0.70488\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3554 | loss: 0.70488 - acc: 0.5030 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3555  | total loss: \u001b[1m\u001b[32m0.70491\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3555 | loss: 0.70491 - acc: 0.5027 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3556  | total loss: \u001b[1m\u001b[32m0.70494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3556 | loss: 0.70494 - acc: 0.5024 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3557  | total loss: \u001b[1m\u001b[32m0.70496\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3557 | loss: 0.70496 - acc: 0.5022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3558  | total loss: \u001b[1m\u001b[32m0.70499\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3558 | loss: 0.70499 - acc: 0.5019 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3559  | total loss: \u001b[1m\u001b[32m0.70501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3559 | loss: 0.70501 - acc: 0.5017 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3560  | total loss: \u001b[1m\u001b[32m0.71002\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3560 | loss: 0.71002 - acc: 0.4516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3561  | total loss: \u001b[1m\u001b[32m0.70934\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3561 | loss: 0.70934 - acc: 0.4564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3562  | total loss: \u001b[1m\u001b[32m0.70792\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3562 | loss: 0.70792 - acc: 0.4708 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3563  | total loss: \u001b[1m\u001b[32m0.70663\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3563 | loss: 0.70663 - acc: 0.4837 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3564  | total loss: \u001b[1m\u001b[32m0.70539\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3564 | loss: 0.70539 - acc: 0.4953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3565  | total loss: \u001b[1m\u001b[32m0.70437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3565 | loss: 0.70437 - acc: 0.5058 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3566  | total loss: \u001b[1m\u001b[32m0.70345\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3566 | loss: 0.70345 - acc: 0.5152 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3567  | total loss: \u001b[1m\u001b[32m0.70281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3567 | loss: 0.70281 - acc: 0.5237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3568  | total loss: \u001b[1m\u001b[32m0.70205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3568 | loss: 0.70205 - acc: 0.5313 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3569  | total loss: \u001b[1m\u001b[32m0.70136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3569 | loss: 0.70136 - acc: 0.5382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3570  | total loss: \u001b[1m\u001b[32m0.70574\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3570 | loss: 0.70574 - acc: 0.4944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3571  | total loss: \u001b[1m\u001b[32m0.70468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3571 | loss: 0.70468 - acc: 0.5049 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3572  | total loss: \u001b[1m\u001b[32m0.70873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3572 | loss: 0.70873 - acc: 0.4644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3573  | total loss: \u001b[1m\u001b[32m0.70737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3573 | loss: 0.70737 - acc: 0.4780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3574  | total loss: \u001b[1m\u001b[32m0.70615\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3574 | loss: 0.70615 - acc: 0.4902 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3575  | total loss: \u001b[1m\u001b[32m0.70505\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3575 | loss: 0.70505 - acc: 0.5012 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3576  | total loss: \u001b[1m\u001b[32m0.71006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3576 | loss: 0.71006 - acc: 0.4511 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3577  | total loss: \u001b[1m\u001b[32m0.70858\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3577 | loss: 0.70858 - acc: 0.4660 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3578  | total loss: \u001b[1m\u001b[32m0.71223\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3578 | loss: 0.71223 - acc: 0.4294 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3579  | total loss: \u001b[1m\u001b[32m0.71053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3579 | loss: 0.71053 - acc: 0.4464 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3580  | total loss: \u001b[1m\u001b[32m0.71199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3580 | loss: 0.71199 - acc: 0.4318 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3581  | total loss: \u001b[1m\u001b[32m0.71031\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3581 | loss: 0.71031 - acc: 0.4486 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3582  | total loss: \u001b[1m\u001b[32m0.70880\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3582 | loss: 0.70880 - acc: 0.4637 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3583  | total loss: \u001b[1m\u001b[32m0.70744\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3583 | loss: 0.70744 - acc: 0.4774 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3584  | total loss: \u001b[1m\u001b[32m0.70621\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3584 | loss: 0.70621 - acc: 0.4896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3585  | total loss: \u001b[1m\u001b[32m0.70511\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3585 | loss: 0.70511 - acc: 0.5007 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3586  | total loss: \u001b[1m\u001b[32m0.70911\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3586 | loss: 0.70911 - acc: 0.4606 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3587  | total loss: \u001b[1m\u001b[32m0.70772\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3587 | loss: 0.70772 - acc: 0.4745 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3588  | total loss: \u001b[1m\u001b[32m0.70646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3588 | loss: 0.70646 - acc: 0.4871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3589  | total loss: \u001b[1m\u001b[32m0.70533\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3589 | loss: 0.70533 - acc: 0.4984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3590  | total loss: \u001b[1m\u001b[32m0.70832\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3590 | loss: 0.70832 - acc: 0.4685 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3591  | total loss: \u001b[1m\u001b[32m0.70700\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3591 | loss: 0.70700 - acc: 0.4817 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3592  | total loss: \u001b[1m\u001b[32m0.70982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3592 | loss: 0.70982 - acc: 0.4535 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3593  | total loss: \u001b[1m\u001b[32m0.70835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3593 | loss: 0.70835 - acc: 0.4682 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3594  | total loss: \u001b[1m\u001b[32m0.71103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3594 | loss: 0.71103 - acc: 0.4413 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3595  | total loss: \u001b[1m\u001b[32m0.70944\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3595 | loss: 0.70944 - acc: 0.4572 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3596  | total loss: \u001b[1m\u001b[32m0.71202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3596 | loss: 0.71202 - acc: 0.4315 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3597  | total loss: \u001b[1m\u001b[32m0.71033\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3597 | loss: 0.71033 - acc: 0.4483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3598  | total loss: \u001b[1m\u001b[32m0.70881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3598 | loss: 0.70881 - acc: 0.4635 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3599  | total loss: \u001b[1m\u001b[32m0.70745\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3599 | loss: 0.70745 - acc: 0.4772 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3600  | total loss: \u001b[1m\u001b[32m0.70622\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3600 | loss: 0.70622 - acc: 0.4894 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3601  | total loss: \u001b[1m\u001b[32m0.70511\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3601 | loss: 0.70511 - acc: 0.5005 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3602  | total loss: \u001b[1m\u001b[32m0.70412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3602 | loss: 0.70412 - acc: 0.5104 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3603  | total loss: \u001b[1m\u001b[32m0.70322\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3603 | loss: 0.70322 - acc: 0.5194 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3604  | total loss: \u001b[1m\u001b[32m0.70742\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3604 | loss: 0.70742 - acc: 0.4775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3605  | total loss: \u001b[1m\u001b[32m0.70619\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3605 | loss: 0.70619 - acc: 0.4897 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3606  | total loss: \u001b[1m\u001b[32m0.71009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3606 | loss: 0.71009 - acc: 0.4507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3607  | total loss: \u001b[1m\u001b[32m0.70859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3607 | loss: 0.70859 - acc: 0.4657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3608  | total loss: \u001b[1m\u001b[32m0.70725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3608 | loss: 0.70725 - acc: 0.4791 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3609  | total loss: \u001b[1m\u001b[32m0.70604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3609 | loss: 0.70604 - acc: 0.4912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3610  | total loss: \u001b[1m\u001b[32m0.70495\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3610 | loss: 0.70495 - acc: 0.5021 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3611  | total loss: \u001b[1m\u001b[32m0.70397\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3611 | loss: 0.70397 - acc: 0.5119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3612  | total loss: \u001b[1m\u001b[32m0.70309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3612 | loss: 0.70309 - acc: 0.5207 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3613  | total loss: \u001b[1m\u001b[32m0.70230\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3613 | loss: 0.70230 - acc: 0.5286 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3614  | total loss: \u001b[1m\u001b[32m0.70159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3614 | loss: 0.70159 - acc: 0.5358 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3615  | total loss: \u001b[1m\u001b[32m0.70094\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3615 | loss: 0.70094 - acc: 0.5422 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3616  | total loss: \u001b[1m\u001b[32m0.70036\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3616 | loss: 0.70036 - acc: 0.5480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3617  | total loss: \u001b[1m\u001b[32m0.69984\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3617 | loss: 0.69984 - acc: 0.5532 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3618  | total loss: \u001b[1m\u001b[32m0.69938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3618 | loss: 0.69938 - acc: 0.5578 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3619  | total loss: \u001b[1m\u001b[32m0.69895\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3619 | loss: 0.69895 - acc: 0.5621 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3620  | total loss: \u001b[1m\u001b[32m0.69857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3620 | loss: 0.69857 - acc: 0.5659 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3621  | total loss: \u001b[1m\u001b[32m0.69823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3621 | loss: 0.69823 - acc: 0.5693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3622  | total loss: \u001b[1m\u001b[32m0.69793\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3622 | loss: 0.69793 - acc: 0.5723 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3623  | total loss: \u001b[1m\u001b[32m0.69765\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3623 | loss: 0.69765 - acc: 0.5751 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3624  | total loss: \u001b[1m\u001b[32m0.69740\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3624 | loss: 0.69740 - acc: 0.5776 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3625  | total loss: \u001b[1m\u001b[32m0.69718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3625 | loss: 0.69718 - acc: 0.5798 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3626  | total loss: \u001b[1m\u001b[32m0.70197\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3626 | loss: 0.70197 - acc: 0.5319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3627  | total loss: \u001b[1m\u001b[32m0.70129\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3627 | loss: 0.70129 - acc: 0.5387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3628  | total loss: \u001b[1m\u001b[32m0.70068\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3628 | loss: 0.70068 - acc: 0.5448 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3629  | total loss: \u001b[1m\u001b[32m0.70013\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3629 | loss: 0.70013 - acc: 0.5503 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3630  | total loss: \u001b[1m\u001b[32m0.70463\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3630 | loss: 0.70463 - acc: 0.5053 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3631  | total loss: \u001b[1m\u001b[32m0.70368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3631 | loss: 0.70368 - acc: 0.5148 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3632  | total loss: \u001b[1m\u001b[32m0.70283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3632 | loss: 0.70283 - acc: 0.5233 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3633  | total loss: \u001b[1m\u001b[32m0.70206\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3633 | loss: 0.70206 - acc: 0.5310 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3634  | total loss: \u001b[1m\u001b[32m0.70137\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3634 | loss: 0.70137 - acc: 0.5379 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3635  | total loss: \u001b[1m\u001b[32m0.70075\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3635 | loss: 0.70075 - acc: 0.5441 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3636  | total loss: \u001b[1m\u001b[32m0.70019\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3636 | loss: 0.70019 - acc: 0.5497 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3637  | total loss: \u001b[1m\u001b[32m0.69969\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3637 | loss: 0.69969 - acc: 0.5547 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3638  | total loss: \u001b[1m\u001b[32m0.69924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3638 | loss: 0.69924 - acc: 0.5592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3639  | total loss: \u001b[1m\u001b[32m0.69883\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3639 | loss: 0.69883 - acc: 0.5633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3640  | total loss: \u001b[1m\u001b[32m0.70346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3640 | loss: 0.70346 - acc: 0.5170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3641  | total loss: \u001b[1m\u001b[32m0.70263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3641 | loss: 0.70263 - acc: 0.5253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3642  | total loss: \u001b[1m\u001b[32m0.70188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3642 | loss: 0.70188 - acc: 0.5328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3643  | total loss: \u001b[1m\u001b[32m0.70121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3643 | loss: 0.70121 - acc: 0.5395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3644  | total loss: \u001b[1m\u001b[32m0.70061\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3644 | loss: 0.70061 - acc: 0.5455 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3645  | total loss: \u001b[1m\u001b[32m0.70006\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3645 | loss: 0.70006 - acc: 0.5510 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3646  | total loss: \u001b[1m\u001b[32m0.69957\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3646 | loss: 0.69957 - acc: 0.5559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3647  | total loss: \u001b[1m\u001b[32m0.69913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3647 | loss: 0.69913 - acc: 0.5603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3648  | total loss: \u001b[1m\u001b[32m0.69873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3648 | loss: 0.69873 - acc: 0.5643 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3649  | total loss: \u001b[1m\u001b[32m0.69838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3649 | loss: 0.69838 - acc: 0.5678 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3650  | total loss: \u001b[1m\u001b[32m0.69805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3650 | loss: 0.69805 - acc: 0.5711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3651  | total loss: \u001b[1m\u001b[32m0.69776\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3651 | loss: 0.69776 - acc: 0.5739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3652  | total loss: \u001b[1m\u001b[32m0.69750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3652 | loss: 0.69750 - acc: 0.5766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3653  | total loss: \u001b[1m\u001b[32m0.69727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3653 | loss: 0.69727 - acc: 0.5789 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3654  | total loss: \u001b[1m\u001b[32m0.69706\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3654 | loss: 0.69706 - acc: 0.5810 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3655  | total loss: \u001b[1m\u001b[32m0.69687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3655 | loss: 0.69687 - acc: 0.5829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3656  | total loss: \u001b[1m\u001b[32m0.69670\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3656 | loss: 0.69670 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3657  | total loss: \u001b[1m\u001b[32m0.69654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3657 | loss: 0.69654 - acc: 0.5862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3658  | total loss: \u001b[1m\u001b[32m0.70241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3658 | loss: 0.70241 - acc: 0.5275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3659  | total loss: \u001b[1m\u001b[32m0.70168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3659 | loss: 0.70168 - acc: 0.5348 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3660  | total loss: \u001b[1m\u001b[32m0.70703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3660 | loss: 0.70703 - acc: 0.4813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3661  | total loss: \u001b[1m\u001b[32m0.70584\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3661 | loss: 0.70584 - acc: 0.4932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3662  | total loss: \u001b[1m\u001b[32m0.70477\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3662 | loss: 0.70477 - acc: 0.5039 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3663  | total loss: \u001b[1m\u001b[32m0.70381\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3663 | loss: 0.70381 - acc: 0.5135 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3664  | total loss: \u001b[1m\u001b[32m0.70295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3664 | loss: 0.70295 - acc: 0.5221 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3665  | total loss: \u001b[1m\u001b[32m0.70217\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3665 | loss: 0.70217 - acc: 0.5299 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3666  | total loss: \u001b[1m\u001b[32m0.70147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3666 | loss: 0.70147 - acc: 0.5369 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3667  | total loss: \u001b[1m\u001b[32m0.70084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3667 | loss: 0.70084 - acc: 0.5432 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3668  | total loss: \u001b[1m\u001b[32m0.70627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3668 | loss: 0.70627 - acc: 0.4889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3669  | total loss: \u001b[1m\u001b[32m0.70516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3669 | loss: 0.70516 - acc: 0.5000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3670  | total loss: \u001b[1m\u001b[32m0.70416\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3670 | loss: 0.70416 - acc: 0.5100 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3671  | total loss: \u001b[1m\u001b[32m0.70326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3671 | loss: 0.70326 - acc: 0.5190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3672  | total loss: \u001b[1m\u001b[32m0.70245\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3672 | loss: 0.70245 - acc: 0.5271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3673  | total loss: \u001b[1m\u001b[32m0.70172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3673 | loss: 0.70172 - acc: 0.5344 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3674  | total loss: \u001b[1m\u001b[32m0.70106\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3674 | loss: 0.70106 - acc: 0.5410 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3675  | total loss: \u001b[1m\u001b[32m0.70047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3675 | loss: 0.70047 - acc: 0.5469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3676  | total loss: \u001b[1m\u001b[32m0.69994\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3676 | loss: 0.69994 - acc: 0.5522 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3677  | total loss: \u001b[1m\u001b[32m0.69946\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3677 | loss: 0.69946 - acc: 0.5570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3678  | total loss: \u001b[1m\u001b[32m0.70403\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3678 | loss: 0.70403 - acc: 0.5113 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3679  | total loss: \u001b[1m\u001b[32m0.70315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3679 | loss: 0.70315 - acc: 0.5201 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3680  | total loss: \u001b[1m\u001b[32m0.70735\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3680 | loss: 0.70735 - acc: 0.4781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3681  | total loss: \u001b[1m\u001b[32m0.70613\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3681 | loss: 0.70613 - acc: 0.4903 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3682  | total loss: \u001b[1m\u001b[32m0.71103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3682 | loss: 0.71103 - acc: 0.4413 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3683  | total loss: \u001b[1m\u001b[32m0.70944\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3683 | loss: 0.70944 - acc: 0.4572 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3684  | total loss: \u001b[1m\u001b[32m0.70802\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3684 | loss: 0.70802 - acc: 0.4714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3685  | total loss: \u001b[1m\u001b[32m0.70673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3685 | loss: 0.70673 - acc: 0.4843 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3686  | total loss: \u001b[1m\u001b[32m0.70557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3686 | loss: 0.70557 - acc: 0.4959 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3687  | total loss: \u001b[1m\u001b[32m0.70453\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3687 | loss: 0.70453 - acc: 0.5063 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3688  | total loss: \u001b[1m\u001b[32m0.70359\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3688 | loss: 0.70359 - acc: 0.5156 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3689  | total loss: \u001b[1m\u001b[32m0.70275\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3689 | loss: 0.70275 - acc: 0.5241 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3690  | total loss: \u001b[1m\u001b[32m0.70199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3690 | loss: 0.70199 - acc: 0.5317 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3691  | total loss: \u001b[1m\u001b[32m0.70131\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3691 | loss: 0.70131 - acc: 0.5385 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3692  | total loss: \u001b[1m\u001b[32m0.70569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3692 | loss: 0.70569 - acc: 0.4947 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3693  | total loss: \u001b[1m\u001b[32m0.70464\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3693 | loss: 0.70464 - acc: 0.5052 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3694  | total loss: \u001b[1m\u001b[32m0.70369\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3694 | loss: 0.70369 - acc: 0.5147 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3695  | total loss: \u001b[1m\u001b[32m0.70284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3695 | loss: 0.70284 - acc: 0.5232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3696  | total loss: \u001b[1m\u001b[32m0.70207\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3696 | loss: 0.70207 - acc: 0.5309 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3697  | total loss: \u001b[1m\u001b[32m0.70138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3697 | loss: 0.70138 - acc: 0.5378 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3698  | total loss: \u001b[1m\u001b[32m0.70076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3698 | loss: 0.70076 - acc: 0.5440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3699  | total loss: \u001b[1m\u001b[32m0.70020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3699 | loss: 0.70020 - acc: 0.5496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3700  | total loss: \u001b[1m\u001b[32m0.69969\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3700 | loss: 0.69969 - acc: 0.5547 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3701  | total loss: \u001b[1m\u001b[32m0.69924\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3701 | loss: 0.69924 - acc: 0.5592 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3702  | total loss: \u001b[1m\u001b[32m0.70483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3702 | loss: 0.70483 - acc: 0.5033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3703  | total loss: \u001b[1m\u001b[32m0.70386\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3703 | loss: 0.70386 - acc: 0.5129 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3704  | total loss: \u001b[1m\u001b[32m0.70299\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3704 | loss: 0.70299 - acc: 0.5216 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3705  | total loss: \u001b[1m\u001b[32m0.70221\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3705 | loss: 0.70221 - acc: 0.5295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3706  | total loss: \u001b[1m\u001b[32m0.70151\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3706 | loss: 0.70151 - acc: 0.5365 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3707  | total loss: \u001b[1m\u001b[32m0.70087\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3707 | loss: 0.70087 - acc: 0.5429 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3708  | total loss: \u001b[1m\u001b[32m0.70030\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3708 | loss: 0.70030 - acc: 0.5486 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3709  | total loss: \u001b[1m\u001b[32m0.69979\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3709 | loss: 0.69979 - acc: 0.5537 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3710  | total loss: \u001b[1m\u001b[32m0.69932\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 3710 | loss: 0.69932 - acc: 0.5584 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3711  | total loss: \u001b[1m\u001b[32m0.69891\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3711 | loss: 0.69891 - acc: 0.5625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3712  | total loss: \u001b[1m\u001b[32m0.69853\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3712 | loss: 0.69853 - acc: 0.5663 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3713  | total loss: \u001b[1m\u001b[32m0.69819\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3713 | loss: 0.69819 - acc: 0.5696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3714  | total loss: \u001b[1m\u001b[32m0.69789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3714 | loss: 0.69789 - acc: 0.5727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3715  | total loss: \u001b[1m\u001b[32m0.69762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3715 | loss: 0.69762 - acc: 0.5754 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3716  | total loss: \u001b[1m\u001b[32m0.69737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3716 | loss: 0.69737 - acc: 0.5779 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3717  | total loss: \u001b[1m\u001b[32m0.69715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3717 | loss: 0.69715 - acc: 0.5801 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3718  | total loss: \u001b[1m\u001b[32m0.69695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3718 | loss: 0.69695 - acc: 0.5821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3719  | total loss: \u001b[1m\u001b[32m0.69677\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3719 | loss: 0.69677 - acc: 0.5839 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3720  | total loss: \u001b[1m\u001b[32m0.70161\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3720 | loss: 0.70161 - acc: 0.5355 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3721  | total loss: \u001b[1m\u001b[32m0.70097\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3721 | loss: 0.70097 - acc: 0.5419 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3722  | total loss: \u001b[1m\u001b[32m0.70038\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3722 | loss: 0.70038 - acc: 0.5477 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3723  | total loss: \u001b[1m\u001b[32m0.69986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3723 | loss: 0.69986 - acc: 0.5530 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3724  | total loss: \u001b[1m\u001b[32m0.69939\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3724 | loss: 0.69939 - acc: 0.5577 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3725  | total loss: \u001b[1m\u001b[32m0.69897\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3725 | loss: 0.69897 - acc: 0.5619 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3726  | total loss: \u001b[1m\u001b[32m0.69859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3726 | loss: 0.69859 - acc: 0.5657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3727  | total loss: \u001b[1m\u001b[32m0.69824\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3727 | loss: 0.69824 - acc: 0.5691 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3728  | total loss: \u001b[1m\u001b[32m0.70394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3728 | loss: 0.70394 - acc: 0.5122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3729  | total loss: \u001b[1m\u001b[32m0.70306\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3729 | loss: 0.70306 - acc: 0.5210 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3730  | total loss: \u001b[1m\u001b[32m0.70227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3730 | loss: 0.70227 - acc: 0.5289 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3731  | total loss: \u001b[1m\u001b[32m0.70156\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3731 | loss: 0.70156 - acc: 0.5360 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3732  | total loss: \u001b[1m\u001b[32m0.70292\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3732 | loss: 0.70292 - acc: 0.5224 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3733  | total loss: \u001b[1m\u001b[32m0.70214\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3733 | loss: 0.70214 - acc: 0.5302 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3734  | total loss: \u001b[1m\u001b[32m0.70144\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3734 | loss: 0.70144 - acc: 0.5372 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3735  | total loss: \u001b[1m\u001b[32m0.70082\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3735 | loss: 0.70082 - acc: 0.5434 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3736  | total loss: \u001b[1m\u001b[32m0.70525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3736 | loss: 0.70525 - acc: 0.4991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3737  | total loss: \u001b[1m\u001b[32m0.70424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3737 | loss: 0.70424 - acc: 0.5092 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3738  | total loss: \u001b[1m\u001b[32m0.70333\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3738 | loss: 0.70333 - acc: 0.5183 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3739  | total loss: \u001b[1m\u001b[32m0.70251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3739 | loss: 0.70251 - acc: 0.5264 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3740  | total loss: \u001b[1m\u001b[32m0.70678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3740 | loss: 0.70678 - acc: 0.4838 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3741  | total loss: \u001b[1m\u001b[32m0.70562\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3741 | loss: 0.70562 - acc: 0.4954 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3742  | total loss: \u001b[1m\u001b[32m0.70457\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3742 | loss: 0.70457 - acc: 0.5059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3743  | total loss: \u001b[1m\u001b[32m0.70363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3743 | loss: 0.70363 - acc: 0.5153 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3744  | total loss: \u001b[1m\u001b[32m0.70278\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3744 | loss: 0.70278 - acc: 0.5238 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3745  | total loss: \u001b[1m\u001b[32m0.70202\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3745 | loss: 0.70202 - acc: 0.5314 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3746  | total loss: \u001b[1m\u001b[32m0.70133\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3746 | loss: 0.70133 - acc: 0.5382 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3747  | total loss: \u001b[1m\u001b[32m0.70072\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3747 | loss: 0.70072 - acc: 0.5444 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3748  | total loss: \u001b[1m\u001b[32m0.70016\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3748 | loss: 0.70016 - acc: 0.5500 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3749  | total loss: \u001b[1m\u001b[32m0.69966\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3749 | loss: 0.69966 - acc: 0.5550 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3750  | total loss: \u001b[1m\u001b[32m0.69921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3750 | loss: 0.69921 - acc: 0.5595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3751  | total loss: \u001b[1m\u001b[32m0.69881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3751 | loss: 0.69881 - acc: 0.5635 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3752  | total loss: \u001b[1m\u001b[32m0.69844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3752 | loss: 0.69844 - acc: 0.5672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3753  | total loss: \u001b[1m\u001b[32m0.69811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3753 | loss: 0.69811 - acc: 0.5705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3754  | total loss: \u001b[1m\u001b[32m0.69782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3754 | loss: 0.69782 - acc: 0.5734 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3755  | total loss: \u001b[1m\u001b[32m0.69755\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3755 | loss: 0.69755 - acc: 0.5761 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3756  | total loss: \u001b[1m\u001b[32m0.69731\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3756 | loss: 0.69731 - acc: 0.5785 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3757  | total loss: \u001b[1m\u001b[32m0.69710\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3757 | loss: 0.69710 - acc: 0.5806 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3758  | total loss: \u001b[1m\u001b[32m0.69690\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3758 | loss: 0.69690 - acc: 0.5826 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3759  | total loss: \u001b[1m\u001b[32m0.69673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3759 | loss: 0.69673 - acc: 0.5843 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3760  | total loss: \u001b[1m\u001b[32m0.69657\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3760 | loss: 0.69657 - acc: 0.5859 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3761  | total loss: \u001b[1m\u001b[32m0.69643\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3761 | loss: 0.69643 - acc: 0.5873 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3762  | total loss: \u001b[1m\u001b[32m0.69630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3762 | loss: 0.69630 - acc: 0.5886 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3763  | total loss: \u001b[1m\u001b[32m0.69619\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3763 | loss: 0.69619 - acc: 0.5897 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3764  | total loss: \u001b[1m\u001b[32m0.69609\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3764 | loss: 0.69609 - acc: 0.5907 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3765  | total loss: \u001b[1m\u001b[32m0.69599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3765 | loss: 0.69599 - acc: 0.5917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3766  | total loss: \u001b[1m\u001b[32m0.69591\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3766 | loss: 0.69591 - acc: 0.5925 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3767  | total loss: \u001b[1m\u001b[32m0.69583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3767 | loss: 0.69583 - acc: 0.5932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3768  | total loss: \u001b[1m\u001b[32m0.69577\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3768 | loss: 0.69577 - acc: 0.5939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3769  | total loss: \u001b[1m\u001b[32m0.69571\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3769 | loss: 0.69571 - acc: 0.5945 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3770  | total loss: \u001b[1m\u001b[32m0.69565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3770 | loss: 0.69565 - acc: 0.5951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3771  | total loss: \u001b[1m\u001b[32m0.69560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3771 | loss: 0.69560 - acc: 0.5956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3772  | total loss: \u001b[1m\u001b[32m0.69556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3772 | loss: 0.69556 - acc: 0.5960 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3773  | total loss: \u001b[1m\u001b[32m0.69552\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3773 | loss: 0.69552 - acc: 0.5964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3774  | total loss: \u001b[1m\u001b[32m0.69548\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3774 | loss: 0.69548 - acc: 0.5968 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3775  | total loss: \u001b[1m\u001b[32m0.69545\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3775 | loss: 0.69545 - acc: 0.5971 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3776  | total loss: \u001b[1m\u001b[32m0.69542\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3776 | loss: 0.69542 - acc: 0.5974 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3777  | total loss: \u001b[1m\u001b[32m0.69539\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3777 | loss: 0.69539 - acc: 0.5976 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3778  | total loss: \u001b[1m\u001b[32m0.69537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3778 | loss: 0.69537 - acc: 0.5979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3779  | total loss: \u001b[1m\u001b[32m0.69535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3779 | loss: 0.69535 - acc: 0.5981 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3780  | total loss: \u001b[1m\u001b[32m0.69533\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3780 | loss: 0.69533 - acc: 0.5983 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3781  | total loss: \u001b[1m\u001b[32m0.69531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3781 | loss: 0.69531 - acc: 0.5985 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3782  | total loss: \u001b[1m\u001b[32m0.69530\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3782 | loss: 0.69530 - acc: 0.5986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3783  | total loss: \u001b[1m\u001b[32m0.69528\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3783 | loss: 0.69528 - acc: 0.5987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3784  | total loss: \u001b[1m\u001b[32m0.69527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3784 | loss: 0.69527 - acc: 0.5989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3785  | total loss: \u001b[1m\u001b[32m0.69526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3785 | loss: 0.69526 - acc: 0.5990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3786  | total loss: \u001b[1m\u001b[32m0.69525\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3786 | loss: 0.69525 - acc: 0.5991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3787  | total loss: \u001b[1m\u001b[32m0.69524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3787 | loss: 0.69524 - acc: 0.5992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3788  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3788 | loss: 0.69523 - acc: 0.5993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3789  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.048s\n",
      "| Adam | epoch: 3789 | loss: 0.69523 - acc: 0.5993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3790  | total loss: \u001b[1m\u001b[32m0.69522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3790 | loss: 0.69522 - acc: 0.5994 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3791  | total loss: \u001b[1m\u001b[32m0.69521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3791 | loss: 0.69521 - acc: 0.5995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3792  | total loss: \u001b[1m\u001b[32m0.69921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3792 | loss: 0.69921 - acc: 0.5595 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3793  | total loss: \u001b[1m\u001b[32m0.69880\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3793 | loss: 0.69880 - acc: 0.5636 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3794  | total loss: \u001b[1m\u001b[32m0.69844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3794 | loss: 0.69844 - acc: 0.5672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3795  | total loss: \u001b[1m\u001b[32m0.69811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3795 | loss: 0.69811 - acc: 0.5705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3796  | total loss: \u001b[1m\u001b[32m0.69782\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3796 | loss: 0.69782 - acc: 0.5734 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3797  | total loss: \u001b[1m\u001b[32m0.69755\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3797 | loss: 0.69755 - acc: 0.5761 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3798  | total loss: \u001b[1m\u001b[32m0.69731\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3798 | loss: 0.69731 - acc: 0.5785 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3799  | total loss: \u001b[1m\u001b[32m0.69710\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3799 | loss: 0.69710 - acc: 0.5806 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3800  | total loss: \u001b[1m\u001b[32m0.70190\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3800 | loss: 0.70190 - acc: 0.5326 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3801  | total loss: \u001b[1m\u001b[32m0.70123\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3801 | loss: 0.70123 - acc: 0.5393 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3802  | total loss: \u001b[1m\u001b[32m0.70062\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3802 | loss: 0.70062 - acc: 0.5454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3803  | total loss: \u001b[1m\u001b[32m0.70007\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3803 | loss: 0.70007 - acc: 0.5508 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3804  | total loss: \u001b[1m\u001b[32m0.69958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3804 | loss: 0.69958 - acc: 0.5558 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3805  | total loss: \u001b[1m\u001b[32m0.69914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3805 | loss: 0.69914 - acc: 0.5602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3806  | total loss: \u001b[1m\u001b[32m0.69874\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3806 | loss: 0.69874 - acc: 0.5642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3807  | total loss: \u001b[1m\u001b[32m0.69838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3807 | loss: 0.69838 - acc: 0.5677 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3808  | total loss: \u001b[1m\u001b[32m0.69806\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3808 | loss: 0.69806 - acc: 0.5710 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3809  | total loss: \u001b[1m\u001b[32m0.69777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3809 | loss: 0.69777 - acc: 0.5739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3810  | total loss: \u001b[1m\u001b[32m0.69751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3810 | loss: 0.69751 - acc: 0.5765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3811  | total loss: \u001b[1m\u001b[32m0.69727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3811 | loss: 0.69727 - acc: 0.5788 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3812  | total loss: \u001b[1m\u001b[32m0.69706\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3812 | loss: 0.69706 - acc: 0.5810 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3813  | total loss: \u001b[1m\u001b[32m0.69687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3813 | loss: 0.69687 - acc: 0.5829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3814  | total loss: \u001b[1m\u001b[32m0.69670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3814 | loss: 0.69670 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3815  | total loss: \u001b[1m\u001b[32m0.69655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3815 | loss: 0.69655 - acc: 0.5861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3816  | total loss: \u001b[1m\u001b[32m0.69641\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3816 | loss: 0.69641 - acc: 0.5875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3817  | total loss: \u001b[1m\u001b[32m0.69628\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3817 | loss: 0.69628 - acc: 0.5888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3818  | total loss: \u001b[1m\u001b[32m0.70117\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3818 | loss: 0.70117 - acc: 0.5399 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3819  | total loss: \u001b[1m\u001b[32m0.70057\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3819 | loss: 0.70057 - acc: 0.5459 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3820  | total loss: \u001b[1m\u001b[32m0.70003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3820 | loss: 0.70003 - acc: 0.5513 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3821  | total loss: \u001b[1m\u001b[32m0.69954\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3821 | loss: 0.69954 - acc: 0.5562 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3822  | total loss: \u001b[1m\u001b[32m0.69910\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3822 | loss: 0.69910 - acc: 0.5606 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3823  | total loss: \u001b[1m\u001b[32m0.69871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3823 | loss: 0.69871 - acc: 0.5645 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3824  | total loss: \u001b[1m\u001b[32m0.70335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3824 | loss: 0.70335 - acc: 0.5180 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3825  | total loss: \u001b[1m\u001b[32m0.70253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3825 | loss: 0.70253 - acc: 0.5262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3826  | total loss: \u001b[1m\u001b[32m0.70180\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3826 | loss: 0.70180 - acc: 0.5336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3827  | total loss: \u001b[1m\u001b[32m0.70113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3827 | loss: 0.70113 - acc: 0.5403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3828  | total loss: \u001b[1m\u001b[32m0.70654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3828 | loss: 0.70654 - acc: 0.4862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3829  | total loss: \u001b[1m\u001b[32m0.70540\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3829 | loss: 0.70540 - acc: 0.4976 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3830  | total loss: \u001b[1m\u001b[32m0.71037\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3830 | loss: 0.71037 - acc: 0.4478 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3831  | total loss: \u001b[1m\u001b[32m0.70885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3831 | loss: 0.70885 - acc: 0.4631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3832  | total loss: \u001b[1m\u001b[32m0.70748\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3832 | loss: 0.70748 - acc: 0.4891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3833  | total loss: \u001b[1m\u001b[32m0.70625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3833 | loss: 0.70625 - acc: 0.4891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3834  | total loss: \u001b[1m\u001b[32m0.70514\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3834 | loss: 0.70514 - acc: 0.5002 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3835  | total loss: \u001b[1m\u001b[32m0.70414\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3835 | loss: 0.70414 - acc: 0.5102 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3836  | total loss: \u001b[1m\u001b[32m0.70324\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3836 | loss: 0.70324 - acc: 0.5191 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3837  | total loss: \u001b[1m\u001b[32m0.70244\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3837 | loss: 0.70244 - acc: 0.5272 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3838  | total loss: \u001b[1m\u001b[32m0.70171\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3838 | loss: 0.70171 - acc: 0.5345 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3839  | total loss: \u001b[1m\u001b[32m0.70105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3839 | loss: 0.70105 - acc: 0.5411 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3840  | total loss: \u001b[1m\u001b[32m0.70046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3840 | loss: 0.70046 - acc: 0.5469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3841  | total loss: \u001b[1m\u001b[32m0.69993\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3841 | loss: 0.69993 - acc: 0.5523 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3842  | total loss: \u001b[1m\u001b[32m0.69946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3842 | loss: 0.69946 - acc: 0.5570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3843  | total loss: \u001b[1m\u001b[32m0.69903\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3843 | loss: 0.69903 - acc: 0.5613 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3844  | total loss: \u001b[1m\u001b[32m0.70464\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3844 | loss: 0.70464 - acc: 0.5052 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3845  | total loss: \u001b[1m\u001b[32m0.70369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3845 | loss: 0.70369 - acc: 0.5147 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3846  | total loss: \u001b[1m\u001b[32m0.70284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3846 | loss: 0.70284 - acc: 0.5232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3847  | total loss: \u001b[1m\u001b[32m0.70207\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3847 | loss: 0.70207 - acc: 0.5309 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3848  | total loss: \u001b[1m\u001b[32m0.70138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3848 | loss: 0.70138 - acc: 0.5378 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3849  | total loss: \u001b[1m\u001b[32m0.70076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3849 | loss: 0.70076 - acc: 0.5440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3850  | total loss: \u001b[1m\u001b[32m0.70020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3850 | loss: 0.70020 - acc: 0.5496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3851  | total loss: \u001b[1m\u001b[32m0.69969\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3851 | loss: 0.69969 - acc: 0.5547 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3852  | total loss: \u001b[1m\u001b[32m0.70524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3852 | loss: 0.70524 - acc: 0.4992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3853  | total loss: \u001b[1m\u001b[32m0.70423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3853 | loss: 0.70423 - acc: 0.5093 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3854  | total loss: \u001b[1m\u001b[32m0.70332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3854 | loss: 0.70332 - acc: 0.5183 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3855  | total loss: \u001b[1m\u001b[32m0.70251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3855 | loss: 0.70251 - acc: 0.5265 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3856  | total loss: \u001b[1m\u001b[32m0.70177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3856 | loss: 0.70177 - acc: 0.5339 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3857  | total loss: \u001b[1m\u001b[32m0.70111\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3857 | loss: 0.70111 - acc: 0.5405 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3858  | total loss: \u001b[1m\u001b[32m0.70452\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3858 | loss: 0.70452 - acc: 0.5064 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3859  | total loss: \u001b[1m\u001b[32m0.70358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3859 | loss: 0.70358 - acc: 0.5158 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3860  | total loss: \u001b[1m\u001b[32m0.70274\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3860 | loss: 0.70274 - acc: 0.5242 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3861  | total loss: \u001b[1m\u001b[32m0.70198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3861 | loss: 0.70198 - acc: 0.5318 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3862  | total loss: \u001b[1m\u001b[32m0.70130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3862 | loss: 0.70130 - acc: 0.5386 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3863  | total loss: \u001b[1m\u001b[32m0.70068\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3863 | loss: 0.70068 - acc: 0.5447 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3864  | total loss: \u001b[1m\u001b[32m0.70013\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3864 | loss: 0.70013 - acc: 0.5503 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3865  | total loss: \u001b[1m\u001b[32m0.69963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3865 | loss: 0.69963 - acc: 0.5552 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3866  | total loss: \u001b[1m\u001b[32m0.69919\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3866 | loss: 0.69919 - acc: 0.5597 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3867  | total loss: \u001b[1m\u001b[32m0.69878\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3867 | loss: 0.69878 - acc: 0.5637 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3868  | total loss: \u001b[1m\u001b[32m0.69842\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3868 | loss: 0.69842 - acc: 0.5674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3869  | total loss: \u001b[1m\u001b[32m0.69810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3869 | loss: 0.69810 - acc: 0.5706 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3870  | total loss: \u001b[1m\u001b[32m0.69780\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3870 | loss: 0.69780 - acc: 0.5736 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3871  | total loss: \u001b[1m\u001b[32m0.69754\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3871 | loss: 0.69754 - acc: 0.5762 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3872  | total loss: \u001b[1m\u001b[32m0.69730\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3872 | loss: 0.69730 - acc: 0.5786 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3873  | total loss: \u001b[1m\u001b[32m0.69709\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3873 | loss: 0.69709 - acc: 0.5807 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3874  | total loss: \u001b[1m\u001b[32m0.69689\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3874 | loss: 0.69689 - acc: 0.5827 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3875  | total loss: \u001b[1m\u001b[32m0.69672\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3875 | loss: 0.69672 - acc: 0.5844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3876  | total loss: \u001b[1m\u001b[32m0.69656\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3876 | loss: 0.69656 - acc: 0.5860 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3877  | total loss: \u001b[1m\u001b[32m0.69642\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3877 | loss: 0.69642 - acc: 0.5874 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3878  | total loss: \u001b[1m\u001b[32m0.69630\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3878 | loss: 0.69630 - acc: 0.5886 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3879  | total loss: \u001b[1m\u001b[32m0.69618\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3879 | loss: 0.69618 - acc: 0.5898 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3880  | total loss: \u001b[1m\u001b[32m0.69608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3880 | loss: 0.69608 - acc: 0.5908 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3881  | total loss: \u001b[1m\u001b[32m0.69599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3881 | loss: 0.69599 - acc: 0.5917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3882  | total loss: \u001b[1m\u001b[32m0.69591\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3882 | loss: 0.69591 - acc: 0.5925 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3883  | total loss: \u001b[1m\u001b[32m0.69583\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3883 | loss: 0.69583 - acc: 0.5933 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3884  | total loss: \u001b[1m\u001b[32m0.69576\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3884 | loss: 0.69576 - acc: 0.5940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3885  | total loss: \u001b[1m\u001b[32m0.69570\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3885 | loss: 0.69570 - acc: 0.5946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3886  | total loss: \u001b[1m\u001b[32m0.69565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3886 | loss: 0.69565 - acc: 0.5951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3887  | total loss: \u001b[1m\u001b[32m0.69560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3887 | loss: 0.69560 - acc: 0.5956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3888  | total loss: \u001b[1m\u001b[32m0.69556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3888 | loss: 0.69556 - acc: 0.5960 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3889  | total loss: \u001b[1m\u001b[32m0.69552\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3889 | loss: 0.69552 - acc: 0.5964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3890  | total loss: \u001b[1m\u001b[32m0.69548\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3890 | loss: 0.69548 - acc: 0.5968 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3891  | total loss: \u001b[1m\u001b[32m0.69545\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3891 | loss: 0.69545 - acc: 0.5971 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3892  | total loss: \u001b[1m\u001b[32m0.69942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3892 | loss: 0.69942 - acc: 0.5574 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3893  | total loss: \u001b[1m\u001b[32m0.69899\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3893 | loss: 0.69899 - acc: 0.5617 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3894  | total loss: \u001b[1m\u001b[32m0.69861\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3894 | loss: 0.69861 - acc: 0.5655 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3895  | total loss: \u001b[1m\u001b[32m0.69826\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3895 | loss: 0.69826 - acc: 0.5689 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3896  | total loss: \u001b[1m\u001b[32m0.69795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3896 | loss: 0.69795 - acc: 0.5720 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3897  | total loss: \u001b[1m\u001b[32m0.69767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3897 | loss: 0.69767 - acc: 0.5748 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3898  | total loss: \u001b[1m\u001b[32m0.69742\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3898 | loss: 0.69742 - acc: 0.5774 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3899  | total loss: \u001b[1m\u001b[32m0.69720\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3899 | loss: 0.69720 - acc: 0.5796 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3900  | total loss: \u001b[1m\u001b[32m0.69699\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3900 | loss: 0.69699 - acc: 0.5817 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3901  | total loss: \u001b[1m\u001b[32m0.69681\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3901 | loss: 0.69681 - acc: 0.5835 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3902  | total loss: \u001b[1m\u001b[32m0.69664\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3902 | loss: 0.69664 - acc: 0.5851 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3903  | total loss: \u001b[1m\u001b[32m0.69650\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3903 | loss: 0.69650 - acc: 0.5866 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3904  | total loss: \u001b[1m\u001b[32m0.69636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3904 | loss: 0.69636 - acc: 0.5880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3905  | total loss: \u001b[1m\u001b[32m0.69624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3905 | loss: 0.69624 - acc: 0.5892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3906  | total loss: \u001b[1m\u001b[32m0.69613\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3906 | loss: 0.69613 - acc: 0.5903 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3907  | total loss: \u001b[1m\u001b[32m0.69604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3907 | loss: 0.69604 - acc: 0.5912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3908  | total loss: \u001b[1m\u001b[32m0.69595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3908 | loss: 0.69595 - acc: 0.5921 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3909  | total loss: \u001b[1m\u001b[32m0.69587\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3909 | loss: 0.69587 - acc: 0.5929 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3910  | total loss: \u001b[1m\u001b[32m0.69580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3910 | loss: 0.69580 - acc: 0.5936 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3911  | total loss: \u001b[1m\u001b[32m0.69573\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3911 | loss: 0.69573 - acc: 0.5942 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3912  | total loss: \u001b[1m\u001b[32m0.70068\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3912 | loss: 0.70068 - acc: 0.5448 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3913  | total loss: \u001b[1m\u001b[32m0.70012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3913 | loss: 0.70012 - acc: 0.5503 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3914  | total loss: \u001b[1m\u001b[32m0.69963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3914 | loss: 0.69963 - acc: 0.5553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3915  | total loss: \u001b[1m\u001b[32m0.69918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3915 | loss: 0.69918 - acc: 0.5598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3916  | total loss: \u001b[1m\u001b[32m0.69878\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3916 | loss: 0.69878 - acc: 0.5638 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3917  | total loss: \u001b[1m\u001b[32m0.69842\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3917 | loss: 0.69842 - acc: 0.5674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3918  | total loss: \u001b[1m\u001b[32m0.69809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3918 | loss: 0.69809 - acc: 0.5707 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3919  | total loss: \u001b[1m\u001b[32m0.69780\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3919 | loss: 0.69780 - acc: 0.5736 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3920  | total loss: \u001b[1m\u001b[32m0.70253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3920 | loss: 0.70253 - acc: 0.5262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3921  | total loss: \u001b[1m\u001b[32m0.70180\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3921 | loss: 0.70180 - acc: 0.5336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3922  | total loss: \u001b[1m\u001b[32m0.70113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3922 | loss: 0.70113 - acc: 0.5403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3923  | total loss: \u001b[1m\u001b[32m0.70054\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3923 | loss: 0.70054 - acc: 0.5462 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3924  | total loss: \u001b[1m\u001b[32m0.70000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3924 | loss: 0.70000 - acc: 0.5516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3925  | total loss: \u001b[1m\u001b[32m0.69951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3925 | loss: 0.69951 - acc: 0.5564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3926  | total loss: \u001b[1m\u001b[32m0.69908\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3926 | loss: 0.69908 - acc: 0.5608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3927  | total loss: \u001b[1m\u001b[32m0.69869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3927 | loss: 0.69869 - acc: 0.5647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3928  | total loss: \u001b[1m\u001b[32m0.69833\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3928 | loss: 0.69833 - acc: 0.5683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3929  | total loss: \u001b[1m\u001b[32m0.69802\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3929 | loss: 0.69802 - acc: 0.5714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3930  | total loss: \u001b[1m\u001b[32m0.69773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3930 | loss: 0.69773 - acc: 0.5743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3931  | total loss: \u001b[1m\u001b[32m0.69747\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3931 | loss: 0.69747 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3932  | total loss: \u001b[1m\u001b[32m0.69724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3932 | loss: 0.69724 - acc: 0.5792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3933  | total loss: \u001b[1m\u001b[32m0.69703\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3933 | loss: 0.69703 - acc: 0.5813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3934  | total loss: \u001b[1m\u001b[32m0.70285\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3934 | loss: 0.70285 - acc: 0.5231 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3935  | total loss: \u001b[1m\u001b[32m0.70208\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3935 | loss: 0.70208 - acc: 0.5308 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3936  | total loss: \u001b[1m\u001b[32m0.70139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3936 | loss: 0.70139 - acc: 0.5377 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3937  | total loss: \u001b[1m\u001b[32m0.70076\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3937 | loss: 0.70076 - acc: 0.5440 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3938  | total loss: \u001b[1m\u001b[32m0.70020\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3938 | loss: 0.70020 - acc: 0.5496 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3939  | total loss: \u001b[1m\u001b[32m0.69970\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3939 | loss: 0.69970 - acc: 0.5546 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3940  | total loss: \u001b[1m\u001b[32m0.69924\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3940 | loss: 0.69924 - acc: 0.5591 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3941  | total loss: \u001b[1m\u001b[32m0.69884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3941 | loss: 0.69884 - acc: 0.5632 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3942  | total loss: \u001b[1m\u001b[32m0.69847\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3942 | loss: 0.69847 - acc: 0.5669 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3943  | total loss: \u001b[1m\u001b[32m0.69814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3943 | loss: 0.69814 - acc: 0.5702 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3944  | total loss: \u001b[1m\u001b[32m0.69784\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3944 | loss: 0.69784 - acc: 0.5732 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3945  | total loss: \u001b[1m\u001b[32m0.69757\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3945 | loss: 0.69757 - acc: 0.5759 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3946  | total loss: \u001b[1m\u001b[32m0.69733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3946 | loss: 0.69733 - acc: 0.5783 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3947  | total loss: \u001b[1m\u001b[32m0.69711\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3947 | loss: 0.69711 - acc: 0.5805 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3948  | total loss: \u001b[1m\u001b[32m0.69692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3948 | loss: 0.69692 - acc: 0.5824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3949  | total loss: \u001b[1m\u001b[32m0.69674\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3949 | loss: 0.69674 - acc: 0.5842 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3950  | total loss: \u001b[1m\u001b[32m0.69658\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3950 | loss: 0.69658 - acc: 0.5858 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3951  | total loss: \u001b[1m\u001b[32m0.69644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3951 | loss: 0.69644 - acc: 0.5872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3952  | total loss: \u001b[1m\u001b[32m0.69631\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3952 | loss: 0.69631 - acc: 0.5885 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3953  | total loss: \u001b[1m\u001b[32m0.69620\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3953 | loss: 0.69620 - acc: 0.5896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3954  | total loss: \u001b[1m\u001b[32m0.69609\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3954 | loss: 0.69609 - acc: 0.5907 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3955  | total loss: \u001b[1m\u001b[32m0.69600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3955 | loss: 0.69600 - acc: 0.5916 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3956  | total loss: \u001b[1m\u001b[32m0.70092\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3956 | loss: 0.70092 - acc: 0.5424 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3957  | total loss: \u001b[1m\u001b[32m0.70034\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3957 | loss: 0.70034 - acc: 0.5482 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3958  | total loss: \u001b[1m\u001b[32m0.69982\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3958 | loss: 0.69982 - acc: 0.5534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3959  | total loss: \u001b[1m\u001b[32m0.69936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3959 | loss: 0.69936 - acc: 0.5580 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3960  | total loss: \u001b[1m\u001b[32m0.69894\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3960 | loss: 0.69894 - acc: 0.5622 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3961  | total loss: \u001b[1m\u001b[32m0.69856\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3961 | loss: 0.69856 - acc: 0.5660 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3962  | total loss: \u001b[1m\u001b[32m0.69822\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3962 | loss: 0.69822 - acc: 0.5694 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3963  | total loss: \u001b[1m\u001b[32m0.69791\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3963 | loss: 0.69791 - acc: 0.5725 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3964  | total loss: \u001b[1m\u001b[32m0.70364\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3964 | loss: 0.70364 - acc: 0.5152 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3965  | total loss: \u001b[1m\u001b[32m0.70279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3965 | loss: 0.70279 - acc: 0.5237 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3966  | total loss: \u001b[1m\u001b[32m0.70803\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3966 | loss: 0.70803 - acc: 0.4713 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3967  | total loss: \u001b[1m\u001b[32m0.70674\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3967 | loss: 0.70674 - acc: 0.4842 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3968  | total loss: \u001b[1m\u001b[32m0.71158\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3968 | loss: 0.71158 - acc: 0.4358 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3969  | total loss: \u001b[1m\u001b[32m0.70994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3969 | loss: 0.70994 - acc: 0.4522 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3970  | total loss: \u001b[1m\u001b[32m0.70846\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3970 | loss: 0.70846 - acc: 0.4670 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3971  | total loss: \u001b[1m\u001b[32m0.70713\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3971 | loss: 0.70713 - acc: 0.4803 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3972  | total loss: \u001b[1m\u001b[32m0.70593\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3972 | loss: 0.70593 - acc: 0.4923 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3973  | total loss: \u001b[1m\u001b[32m0.70486\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3973 | loss: 0.70486 - acc: 0.5030 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3974  | total loss: \u001b[1m\u001b[32m0.70989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3974 | loss: 0.70989 - acc: 0.4527 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3975  | total loss: \u001b[1m\u001b[32m0.70841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3975 | loss: 0.70841 - acc: 0.4675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3976  | total loss: \u001b[1m\u001b[32m0.71309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3976 | loss: 0.71309 - acc: 0.4207 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3977  | total loss: \u001b[1m\u001b[32m0.71130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3977 | loss: 0.71130 - acc: 0.4386 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3978  | total loss: \u001b[1m\u001b[32m0.70968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3978 | loss: 0.70968 - acc: 0.4548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3979  | total loss: \u001b[1m\u001b[32m0.70823\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3979 | loss: 0.70823 - acc: 0.4693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3980  | total loss: \u001b[1m\u001b[32m0.70692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3980 | loss: 0.70692 - acc: 0.4824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3981  | total loss: \u001b[1m\u001b[32m0.70575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3981 | loss: 0.70575 - acc: 0.4941 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3982  | total loss: \u001b[1m\u001b[32m0.70469\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3982 | loss: 0.70469 - acc: 0.5047 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3983  | total loss: \u001b[1m\u001b[32m0.70373\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3983 | loss: 0.70373 - acc: 0.5142 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3984  | total loss: \u001b[1m\u001b[32m0.70288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 3984 | loss: 0.70288 - acc: 0.5228 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3985  | total loss: \u001b[1m\u001b[32m0.70211\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3985 | loss: 0.70211 - acc: 0.5305 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3986  | total loss: \u001b[1m\u001b[32m0.70141\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3986 | loss: 0.70141 - acc: 0.5375 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3987  | total loss: \u001b[1m\u001b[32m0.70079\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3987 | loss: 0.70079 - acc: 0.5437 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3988  | total loss: \u001b[1m\u001b[32m0.70022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3988 | loss: 0.70022 - acc: 0.5494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3989  | total loss: \u001b[1m\u001b[32m0.69972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3989 | loss: 0.69972 - acc: 0.5544 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3990  | total loss: \u001b[1m\u001b[32m0.70226\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3990 | loss: 0.70226 - acc: 0.5290 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3991  | total loss: \u001b[1m\u001b[32m0.70155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3991 | loss: 0.70155 - acc: 0.5361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3992  | total loss: \u001b[1m\u001b[32m0.70491\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3992 | loss: 0.70491 - acc: 0.5025 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3993  | total loss: \u001b[1m\u001b[32m0.70394\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3993 | loss: 0.70394 - acc: 0.5122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3994  | total loss: \u001b[1m\u001b[32m0.70906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3994 | loss: 0.70906 - acc: 0.4610 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3995  | total loss: \u001b[1m\u001b[32m0.70767\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 3995 | loss: 0.70767 - acc: 0.4749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3996  | total loss: \u001b[1m\u001b[32m0.71042\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3996 | loss: 0.71042 - acc: 0.4474 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3997  | total loss: \u001b[1m\u001b[32m0.70889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3997 | loss: 0.70889 - acc: 0.4627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3998  | total loss: \u001b[1m\u001b[32m0.70752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3998 | loss: 0.70752 - acc: 0.4764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.70628\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 3999 | loss: 0.70628 - acc: 0.4888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.70517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4000 | loss: 0.70517 - acc: 0.4999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4001  | total loss: \u001b[1m\u001b[32m0.70417\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4001 | loss: 0.70417 - acc: 0.5099 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4002  | total loss: \u001b[1m\u001b[32m0.70327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4002 | loss: 0.70327 - acc: 0.5189 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4003  | total loss: \u001b[1m\u001b[32m0.70246\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4003 | loss: 0.70246 - acc: 0.5270 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4004  | total loss: \u001b[1m\u001b[32m0.70173\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4004 | loss: 0.70173 - acc: 0.5343 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4005  | total loss: \u001b[1m\u001b[32m0.70107\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4005 | loss: 0.70107 - acc: 0.5409 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4006  | total loss: \u001b[1m\u001b[32m0.70048\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4006 | loss: 0.70048 - acc: 0.5468 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4007  | total loss: \u001b[1m\u001b[32m0.69995\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4007 | loss: 0.69995 - acc: 0.5521 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4008  | total loss: \u001b[1m\u001b[32m0.70447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4008 | loss: 0.70447 - acc: 0.5069 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4009  | total loss: \u001b[1m\u001b[32m0.70354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4009 | loss: 0.70354 - acc: 0.5162 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4010  | total loss: \u001b[1m\u001b[32m0.70270\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4010 | loss: 0.70270 - acc: 0.5246 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4011  | total loss: \u001b[1m\u001b[32m0.70195\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4011 | loss: 0.70195 - acc: 0.5321 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4012  | total loss: \u001b[1m\u001b[32m0.70127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4012 | loss: 0.70127 - acc: 0.5389 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4013  | total loss: \u001b[1m\u001b[32m0.70066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4013 | loss: 0.70066 - acc: 0.5450 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4014  | total loss: \u001b[1m\u001b[32m0.70011\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4014 | loss: 0.70011 - acc: 0.5505 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4015  | total loss: \u001b[1m\u001b[32m0.69961\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4015 | loss: 0.69961 - acc: 0.5555 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4016  | total loss: \u001b[1m\u001b[32m0.69917\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4016 | loss: 0.69917 - acc: 0.5599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4017  | total loss: \u001b[1m\u001b[32m0.69877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4017 | loss: 0.69877 - acc: 0.5639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4018  | total loss: \u001b[1m\u001b[32m0.70240\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4018 | loss: 0.70240 - acc: 0.5275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4019  | total loss: \u001b[1m\u001b[32m0.70168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4019 | loss: 0.70168 - acc: 0.5348 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4020  | total loss: \u001b[1m\u001b[32m0.70703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4020 | loss: 0.70703 - acc: 0.4813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4021  | total loss: \u001b[1m\u001b[32m0.70584\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4021 | loss: 0.70584 - acc: 0.4932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4022  | total loss: \u001b[1m\u001b[32m0.70877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4022 | loss: 0.70877 - acc: 0.4639 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4023  | total loss: \u001b[1m\u001b[32m0.70741\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4023 | loss: 0.70741 - acc: 0.4775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4024  | total loss: \u001b[1m\u001b[32m0.70619\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4024 | loss: 0.70619 - acc: 0.4897 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4025  | total loss: \u001b[1m\u001b[32m0.70508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4025 | loss: 0.70508 - acc: 0.5008 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4026  | total loss: \u001b[1m\u001b[32m0.70409\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4026 | loss: 0.70409 - acc: 0.5107 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4027  | total loss: \u001b[1m\u001b[32m0.70320\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4027 | loss: 0.70320 - acc: 0.5196 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4028  | total loss: \u001b[1m\u001b[32m0.70839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4028 | loss: 0.70839 - acc: 0.4676 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4029  | total loss: \u001b[1m\u001b[32m0.70707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4029 | loss: 0.70707 - acc: 0.4809 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4030  | total loss: \u001b[1m\u001b[32m0.71188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4030 | loss: 0.71188 - acc: 0.4328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4031  | total loss: \u001b[1m\u001b[32m0.71021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4031 | loss: 0.71021 - acc: 0.4495 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4032  | total loss: \u001b[1m\u001b[32m0.70870\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4032 | loss: 0.70870 - acc: 0.4646 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4033  | total loss: \u001b[1m\u001b[32m0.70735\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4033 | loss: 0.70735 - acc: 0.4781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4034  | total loss: \u001b[1m\u001b[32m0.70613\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4034 | loss: 0.70613 - acc: 0.4903 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4035  | total loss: \u001b[1m\u001b[32m0.70503\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4035 | loss: 0.70503 - acc: 0.5013 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4036  | total loss: \u001b[1m\u001b[32m0.70404\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4036 | loss: 0.70404 - acc: 0.5111 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4037  | total loss: \u001b[1m\u001b[32m0.70316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4037 | loss: 0.70316 - acc: 0.5200 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4038  | total loss: \u001b[1m\u001b[32m0.70236\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4038 | loss: 0.70236 - acc: 0.5280 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4039  | total loss: \u001b[1m\u001b[32m0.70164\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4039 | loss: 0.70164 - acc: 0.5352 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4040  | total loss: \u001b[1m\u001b[32m0.70099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4040 | loss: 0.70099 - acc: 0.5417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4041  | total loss: \u001b[1m\u001b[32m0.70041\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4041 | loss: 0.70041 - acc: 0.5475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4042  | total loss: \u001b[1m\u001b[32m0.69988\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4042 | loss: 0.69988 - acc: 0.5528 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4043  | total loss: \u001b[1m\u001b[32m0.69941\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4043 | loss: 0.69941 - acc: 0.5575 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4044  | total loss: \u001b[1m\u001b[32m0.69898\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4044 | loss: 0.69898 - acc: 0.5617 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4045  | total loss: \u001b[1m\u001b[32m0.69860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4045 | loss: 0.69860 - acc: 0.5656 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4046  | total loss: \u001b[1m\u001b[32m0.70326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4046 | loss: 0.70326 - acc: 0.5190 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4047  | total loss: \u001b[1m\u001b[32m0.70245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4047 | loss: 0.70245 - acc: 0.5271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4048  | total loss: \u001b[1m\u001b[32m0.70572\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4048 | loss: 0.70572 - acc: 0.4944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4049  | total loss: \u001b[1m\u001b[32m0.70466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4049 | loss: 0.70466 - acc: 0.5050 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4050  | total loss: \u001b[1m\u001b[32m0.70371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4050 | loss: 0.70371 - acc: 0.5145 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4051  | total loss: \u001b[1m\u001b[32m0.70286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4051 | loss: 0.70286 - acc: 0.5230 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4052  | total loss: \u001b[1m\u001b[32m0.70209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4052 | loss: 0.70209 - acc: 0.5307 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4053  | total loss: \u001b[1m\u001b[32m0.70139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4053 | loss: 0.70139 - acc: 0.5376 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4054  | total loss: \u001b[1m\u001b[32m0.70677\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4054 | loss: 0.70677 - acc: 0.4839 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4055  | total loss: \u001b[1m\u001b[32m0.70561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4055 | loss: 0.70561 - acc: 0.4955 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4056  | total loss: \u001b[1m\u001b[32m0.70456\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4056 | loss: 0.70456 - acc: 0.5059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4057  | total loss: \u001b[1m\u001b[32m0.70362\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4057 | loss: 0.70362 - acc: 0.5153 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4058  | total loss: \u001b[1m\u001b[32m0.70778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4058 | loss: 0.70778 - acc: 0.4738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4059  | total loss: \u001b[1m\u001b[32m0.70652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4059 | loss: 0.70652 - acc: 0.4864 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4060  | total loss: \u001b[1m\u001b[32m0.70538\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4060 | loss: 0.70538 - acc: 0.4978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4061  | total loss: \u001b[1m\u001b[32m0.70436\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4061 | loss: 0.70436 - acc: 0.5080 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4062  | total loss: \u001b[1m\u001b[32m0.70344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4062 | loss: 0.70344 - acc: 0.5172 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4063  | total loss: \u001b[1m\u001b[32m0.70261\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4063 | loss: 0.70261 - acc: 0.5255 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4064  | total loss: \u001b[1m\u001b[32m0.70186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4064 | loss: 0.70186 - acc: 0.5329 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4065  | total loss: \u001b[1m\u001b[32m0.70119\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4065 | loss: 0.70119 - acc: 0.5396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4066  | total loss: \u001b[1m\u001b[32m0.70059\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4066 | loss: 0.70059 - acc: 0.5457 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4067  | total loss: \u001b[1m\u001b[32m0.70005\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4067 | loss: 0.70005 - acc: 0.5511 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4068  | total loss: \u001b[1m\u001b[32m0.69956\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4068 | loss: 0.69956 - acc: 0.5560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4069  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4069 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4070  | total loss: \u001b[1m\u001b[32m0.69872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4070 | loss: 0.69872 - acc: 0.5644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4071  | total loss: \u001b[1m\u001b[32m0.69837\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4071 | loss: 0.69837 - acc: 0.5679 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4072  | total loss: \u001b[1m\u001b[32m0.69805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4072 | loss: 0.69805 - acc: 0.5711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4073  | total loss: \u001b[1m\u001b[32m0.69776\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4073 | loss: 0.69776 - acc: 0.5740 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4074  | total loss: \u001b[1m\u001b[32m0.69750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4074 | loss: 0.69750 - acc: 0.5766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4075  | total loss: \u001b[1m\u001b[32m0.69726\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4075 | loss: 0.69726 - acc: 0.5790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4076  | total loss: \u001b[1m\u001b[32m0.69705\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4076 | loss: 0.69705 - acc: 0.5811 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4077  | total loss: \u001b[1m\u001b[32m0.69686\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4077 | loss: 0.69686 - acc: 0.5830 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4078  | total loss: \u001b[1m\u001b[32m0.69669\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4078 | loss: 0.69669 - acc: 0.5847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4079  | total loss: \u001b[1m\u001b[32m0.69654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4079 | loss: 0.69654 - acc: 0.5862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4080  | total loss: \u001b[1m\u001b[32m0.69640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4080 | loss: 0.69640 - acc: 0.5876 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4081  | total loss: \u001b[1m\u001b[32m0.69628\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4081 | loss: 0.69628 - acc: 0.5888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4082  | total loss: \u001b[1m\u001b[32m0.69617\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4082 | loss: 0.69617 - acc: 0.5899 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4083  | total loss: \u001b[1m\u001b[32m0.69606\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4083 | loss: 0.69606 - acc: 0.5909 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4084  | total loss: \u001b[1m\u001b[32m0.69597\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4084 | loss: 0.69597 - acc: 0.5918 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4085  | total loss: \u001b[1m\u001b[32m0.69589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4085 | loss: 0.69589 - acc: 0.5927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4086  | total loss: \u001b[1m\u001b[32m0.69582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4086 | loss: 0.69582 - acc: 0.5934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4087  | total loss: \u001b[1m\u001b[32m0.69575\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4087 | loss: 0.69575 - acc: 0.5941 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4088  | total loss: \u001b[1m\u001b[32m0.69569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4088 | loss: 0.69569 - acc: 0.5947 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4089  | total loss: \u001b[1m\u001b[32m0.69564\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4089 | loss: 0.69564 - acc: 0.5952 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4090  | total loss: \u001b[1m\u001b[32m0.70059\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4090 | loss: 0.70059 - acc: 0.5457 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4091  | total loss: \u001b[1m\u001b[32m0.70005\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4091 | loss: 0.70005 - acc: 0.5511 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4092  | total loss: \u001b[1m\u001b[32m0.69956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4092 | loss: 0.69956 - acc: 0.5560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4093  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4093 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4094  | total loss: \u001b[1m\u001b[32m0.69872\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4094 | loss: 0.69872 - acc: 0.5644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4095  | total loss: \u001b[1m\u001b[32m0.69837\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4095 | loss: 0.69837 - acc: 0.5679 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4096  | total loss: \u001b[1m\u001b[32m0.70205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4096 | loss: 0.70205 - acc: 0.5311 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4097  | total loss: \u001b[1m\u001b[32m0.70136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4097 | loss: 0.70136 - acc: 0.5380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4098  | total loss: \u001b[1m\u001b[32m0.70074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4098 | loss: 0.70074 - acc: 0.5442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4099  | total loss: \u001b[1m\u001b[32m0.70018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4099 | loss: 0.70018 - acc: 0.5498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4100  | total loss: \u001b[1m\u001b[32m0.70468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4100 | loss: 0.70468 - acc: 0.5048 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4101  | total loss: \u001b[1m\u001b[32m0.70373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4101 | loss: 0.70373 - acc: 0.5143 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4102  | total loss: \u001b[1m\u001b[32m0.70287\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4102 | loss: 0.70287 - acc: 0.5229 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4103  | total loss: \u001b[1m\u001b[32m0.70210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4103 | loss: 0.70210 - acc: 0.5306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4104  | total loss: \u001b[1m\u001b[32m0.70640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4104 | loss: 0.70640 - acc: 0.4875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4105  | total loss: \u001b[1m\u001b[32m0.70528\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4105 | loss: 0.70528 - acc: 0.4988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4106  | total loss: \u001b[1m\u001b[32m0.70927\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4106 | loss: 0.70927 - acc: 0.4589 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4107  | total loss: \u001b[1m\u001b[32m0.70786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4107 | loss: 0.70786 - acc: 0.4730 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4108  | total loss: \u001b[1m\u001b[32m0.71159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4108 | loss: 0.71159 - acc: 0.4357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4109  | total loss: \u001b[1m\u001b[32m0.70994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4109 | loss: 0.70994 - acc: 0.4521 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4110  | total loss: \u001b[1m\u001b[32m0.70847\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4110 | loss: 0.70847 - acc: 0.4669 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4111  | total loss: \u001b[1m\u001b[32m0.70713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4111 | loss: 0.70713 - acc: 0.4802 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4112  | total loss: \u001b[1m\u001b[32m0.71194\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4112 | loss: 0.71194 - acc: 0.4322 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4113  | total loss: \u001b[1m\u001b[32m0.71026\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4113 | loss: 0.71026 - acc: 0.4490 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4114  | total loss: \u001b[1m\u001b[32m0.70875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4114 | loss: 0.70875 - acc: 0.4641 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4115  | total loss: \u001b[1m\u001b[32m0.70739\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4115 | loss: 0.70739 - acc: 0.4777 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4116  | total loss: \u001b[1m\u001b[32m0.70917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4116 | loss: 0.70917 - acc: 0.4599 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4117  | total loss: \u001b[1m\u001b[32m0.70777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4117 | loss: 0.70777 - acc: 0.4739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4118  | total loss: \u001b[1m\u001b[32m0.70537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4118 | loss: 0.70537 - acc: 0.4865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4119  | total loss: \u001b[1m\u001b[32m0.70537\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4119 | loss: 0.70537 - acc: 0.4979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4120  | total loss: \u001b[1m\u001b[32m0.70435\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4120 | loss: 0.70435 - acc: 0.5081 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4121  | total loss: \u001b[1m\u001b[32m0.70343\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4121 | loss: 0.70343 - acc: 0.5173 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4122  | total loss: \u001b[1m\u001b[32m0.70760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4122 | loss: 0.70760 - acc: 0.4756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4123  | total loss: \u001b[1m\u001b[32m0.70636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4123 | loss: 0.70636 - acc: 0.4880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4124  | total loss: \u001b[1m\u001b[32m0.70524\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4124 | loss: 0.70524 - acc: 0.4992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4125  | total loss: \u001b[1m\u001b[32m0.70423\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4125 | loss: 0.70423 - acc: 0.5093 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4126  | total loss: \u001b[1m\u001b[32m0.70732\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4126 | loss: 0.70732 - acc: 0.4784 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4127  | total loss: \u001b[1m\u001b[32m0.70611\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4127 | loss: 0.70611 - acc: 0.4905 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4128  | total loss: \u001b[1m\u001b[32m0.70501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4128 | loss: 0.70501 - acc: 0.5015 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4129  | total loss: \u001b[1m\u001b[32m0.70403\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4129 | loss: 0.70403 - acc: 0.5113 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4130  | total loss: \u001b[1m\u001b[32m0.70714\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4130 | loss: 0.70714 - acc: 0.4802 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4131  | total loss: \u001b[1m\u001b[32m0.70594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4131 | loss: 0.70594 - acc: 0.4922 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4132  | total loss: \u001b[1m\u001b[32m0.70486\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4132 | loss: 0.70486 - acc: 0.5030 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4133  | total loss: \u001b[1m\u001b[32m0.70389\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4133 | loss: 0.70389 - acc: 0.5127 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4134  | total loss: \u001b[1m\u001b[32m0.70302\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4134 | loss: 0.70302 - acc: 0.5214 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4135  | total loss: \u001b[1m\u001b[32m0.70223\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4135 | loss: 0.70223 - acc: 0.5293 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4136  | total loss: \u001b[1m\u001b[32m0.70153\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4136 | loss: 0.70153 - acc: 0.5363 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4137  | total loss: \u001b[1m\u001b[32m0.70089\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4137 | loss: 0.70089 - acc: 0.5427 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4138  | total loss: \u001b[1m\u001b[32m0.70032\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4138 | loss: 0.70032 - acc: 0.5484 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4139  | total loss: \u001b[1m\u001b[32m0.69980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4139 | loss: 0.69980 - acc: 0.5536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4140  | total loss: \u001b[1m\u001b[32m0.69934\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4140 | loss: 0.69934 - acc: 0.5582 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4141  | total loss: \u001b[1m\u001b[32m0.69892\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4141 | loss: 0.69892 - acc: 0.5624 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4142  | total loss: \u001b[1m\u001b[32m0.69854\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4142 | loss: 0.69854 - acc: 0.5662 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4143  | total loss: \u001b[1m\u001b[32m0.69820\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4143 | loss: 0.69820 - acc: 0.5695 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4144  | total loss: \u001b[1m\u001b[32m0.69790\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4144 | loss: 0.69790 - acc: 0.5726 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4145  | total loss: \u001b[1m\u001b[32m0.69763\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4145 | loss: 0.69763 - acc: 0.5753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4146  | total loss: \u001b[1m\u001b[32m0.70338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4146 | loss: 0.70338 - acc: 0.5178 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4147  | total loss: \u001b[1m\u001b[32m0.70256\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4147 | loss: 0.70256 - acc: 0.5260 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4148  | total loss: \u001b[1m\u001b[32m0.70682\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4148 | loss: 0.70682 - acc: 0.4834 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4149  | total loss: \u001b[1m\u001b[32m0.70565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4149 | loss: 0.70565 - acc: 0.4951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4150  | total loss: \u001b[1m\u001b[32m0.70460\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4150 | loss: 0.70460 - acc: 0.5056 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4151  | total loss: \u001b[1m\u001b[32m0.70366\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4151 | loss: 0.70366 - acc: 0.5150 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4152  | total loss: \u001b[1m\u001b[32m0.70281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4152 | loss: 0.70281 - acc: 0.5235 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4153  | total loss: \u001b[1m\u001b[32m0.70204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4153 | loss: 0.70204 - acc: 0.5312 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4154  | total loss: \u001b[1m\u001b[32m0.70135\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4154 | loss: 0.70135 - acc: 0.5380 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4155  | total loss: \u001b[1m\u001b[32m0.70073\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4155 | loss: 0.70073 - acc: 0.5442 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4156  | total loss: \u001b[1m\u001b[32m0.70018\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4156 | loss: 0.70018 - acc: 0.5498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4157  | total loss: \u001b[1m\u001b[32m0.69968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4157 | loss: 0.69968 - acc: 0.5548 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4158  | total loss: \u001b[1m\u001b[32m0.70422\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4158 | loss: 0.70422 - acc: 0.5093 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4159  | total loss: \u001b[1m\u001b[32m0.70332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4159 | loss: 0.70332 - acc: 0.5184 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4160  | total loss: \u001b[1m\u001b[32m0.70650\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4160 | loss: 0.70650 - acc: 0.4866 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4161  | total loss: \u001b[1m\u001b[32m0.70537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4161 | loss: 0.70537 - acc: 0.4979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4162  | total loss: \u001b[1m\u001b[32m0.70935\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4162 | loss: 0.70935 - acc: 0.4581 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4163  | total loss: \u001b[1m\u001b[32m0.70793\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4163 | loss: 0.70793 - acc: 0.4723 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4164  | total loss: \u001b[1m\u001b[32m0.71265\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4164 | loss: 0.71265 - acc: 0.4251 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4165  | total loss: \u001b[1m\u001b[32m0.71090\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4165 | loss: 0.71090 - acc: 0.4426 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4166  | total loss: \u001b[1m\u001b[32m0.71433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4166 | loss: 0.71433 - acc: 0.4083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4167  | total loss: \u001b[1m\u001b[32m0.71241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4167 | loss: 0.71241 - acc: 0.4275 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4168  | total loss: \u001b[1m\u001b[32m0.71069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4168 | loss: 0.71069 - acc: 0.4447 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4169  | total loss: \u001b[1m\u001b[32m0.70913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4169 | loss: 0.70913 - acc: 0.4603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4170  | total loss: \u001b[1m\u001b[32m0.70774\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4170 | loss: 0.70774 - acc: 0.4742 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4171  | total loss: \u001b[1m\u001b[32m0.70648\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4171 | loss: 0.70648 - acc: 0.4868 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4172  | total loss: \u001b[1m\u001b[32m0.71035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4172 | loss: 0.71035 - acc: 0.4481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4173  | total loss: \u001b[1m\u001b[32m0.70883\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4173 | loss: 0.70883 - acc: 0.4633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4174  | total loss: \u001b[1m\u001b[32m0.70746\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4174 | loss: 0.70746 - acc: 0.4770 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4175  | total loss: \u001b[1m\u001b[32m0.70623\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4175 | loss: 0.70623 - acc: 0.4893 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4176  | total loss: \u001b[1m\u001b[32m0.70512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4176 | loss: 0.70512 - acc: 0.5004 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4177  | total loss: \u001b[1m\u001b[32m0.70413\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4177 | loss: 0.70413 - acc: 0.5103 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4178  | total loss: \u001b[1m\u001b[32m0.70323\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4178 | loss: 0.70323 - acc: 0.5193 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4179  | total loss: \u001b[1m\u001b[32m0.70242\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4179 | loss: 0.70242 - acc: 0.5274 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4180  | total loss: \u001b[1m\u001b[32m0.70770\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4180 | loss: 0.70770 - acc: 0.4746 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4181  | total loss: \u001b[1m\u001b[32m0.70644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4181 | loss: 0.70644 - acc: 0.4872 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4182  | total loss: \u001b[1m\u001b[32m0.70531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4182 | loss: 0.70531 - acc: 0.4984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4183  | total loss: \u001b[1m\u001b[32m0.70430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4183 | loss: 0.70430 - acc: 0.5086 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4184  | total loss: \u001b[1m\u001b[32m0.70938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4184 | loss: 0.70938 - acc: 0.4577 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4185  | total loss: \u001b[1m\u001b[32m0.70796\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4185 | loss: 0.70796 - acc: 0.4720 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4186  | total loss: \u001b[1m\u001b[32m0.71068\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4186 | loss: 0.71068 - acc: 0.4448 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4187  | total loss: \u001b[1m\u001b[32m0.70913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4187 | loss: 0.70913 - acc: 0.4603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4188  | total loss: \u001b[1m\u001b[32m0.70773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4188 | loss: 0.70773 - acc: 0.4743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4189  | total loss: \u001b[1m\u001b[32m0.70648\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4189 | loss: 0.70648 - acc: 0.4868 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4190  | total loss: \u001b[1m\u001b[32m0.71034\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4190 | loss: 0.71034 - acc: 0.4482 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4191  | total loss: \u001b[1m\u001b[32m0.70882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4191 | loss: 0.70882 - acc: 0.4633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4192  | total loss: \u001b[1m\u001b[32m0.70746\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4192 | loss: 0.70746 - acc: 0.4770 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4193  | total loss: \u001b[1m\u001b[32m0.70623\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4193 | loss: 0.70623 - acc: 0.4893 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4194  | total loss: \u001b[1m\u001b[32m0.70912\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4194 | loss: 0.70912 - acc: 0.4604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4195  | total loss: \u001b[1m\u001b[32m0.70773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4195 | loss: 0.70773 - acc: 0.4743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4196  | total loss: \u001b[1m\u001b[32m0.70647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4196 | loss: 0.70647 - acc: 0.4869 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4197  | total loss: \u001b[1m\u001b[32m0.70534\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4197 | loss: 0.70534 - acc: 0.4982 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4198  | total loss: \u001b[1m\u001b[32m0.70432\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4198 | loss: 0.70432 - acc: 0.5084 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4199  | total loss: \u001b[1m\u001b[32m0.70340\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4199 | loss: 0.70340 - acc: 0.5176 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4200  | total loss: \u001b[1m\u001b[32m0.70258\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4200 | loss: 0.70258 - acc: 0.5258 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4201  | total loss: \u001b[1m\u001b[32m0.70184\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4201 | loss: 0.70184 - acc: 0.5332 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4202  | total loss: \u001b[1m\u001b[32m0.70117\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4202 | loss: 0.70117 - acc: 0.5399 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4203  | total loss: \u001b[1m\u001b[32m0.70057\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4203 | loss: 0.70057 - acc: 0.5459 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4204  | total loss: \u001b[1m\u001b[32m0.70603\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4204 | loss: 0.70603 - acc: 0.4913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4205  | total loss: \u001b[1m\u001b[32m0.70494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4205 | loss: 0.70494 - acc: 0.5022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4206  | total loss: \u001b[1m\u001b[32m0.70396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4206 | loss: 0.70396 - acc: 0.5120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4207  | total loss: \u001b[1m\u001b[32m0.70308\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4207 | loss: 0.70308 - acc: 0.5208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4208  | total loss: \u001b[1m\u001b[32m0.70229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4208 | loss: 0.70229 - acc: 0.5287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4209  | total loss: \u001b[1m\u001b[32m0.70158\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4209 | loss: 0.70158 - acc: 0.5358 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4210  | total loss: \u001b[1m\u001b[32m0.70093\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4210 | loss: 0.70093 - acc: 0.5422 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4211  | total loss: \u001b[1m\u001b[32m0.70036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4211 | loss: 0.70036 - acc: 0.5480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4212  | total loss: \u001b[1m\u001b[32m0.70284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4212 | loss: 0.70284 - acc: 0.5232 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4213  | total loss: \u001b[1m\u001b[32m0.70207\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4213 | loss: 0.70207 - acc: 0.5309 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4214  | total loss: \u001b[1m\u001b[32m0.70638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4214 | loss: 0.70638 - acc: 0.4878 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4215  | total loss: \u001b[1m\u001b[32m0.70526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4215 | loss: 0.70526 - acc: 0.4990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4216  | total loss: \u001b[1m\u001b[32m0.70425\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4216 | loss: 0.70425 - acc: 0.5091 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4217  | total loss: \u001b[1m\u001b[32m0.70334\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4217 | loss: 0.70334 - acc: 0.5182 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4218  | total loss: \u001b[1m\u001b[32m0.70752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4218 | loss: 0.70752 - acc: 0.4764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4219  | total loss: \u001b[1m\u001b[32m0.70628\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4219 | loss: 0.70628 - acc: 0.4887 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4220  | total loss: \u001b[1m\u001b[32m0.70517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4220 | loss: 0.70517 - acc: 0.4999 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4221  | total loss: \u001b[1m\u001b[32m0.70417\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4221 | loss: 0.70417 - acc: 0.5099 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4222  | total loss: \u001b[1m\u001b[32m0.70827\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4222 | loss: 0.70827 - acc: 0.4689 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4223  | total loss: \u001b[1m\u001b[32m0.70696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4223 | loss: 0.70696 - acc: 0.4820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4224  | total loss: \u001b[1m\u001b[32m0.71078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4224 | loss: 0.71078 - acc: 0.4438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4225  | total loss: \u001b[1m\u001b[32m0.70922\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4225 | loss: 0.70922 - acc: 0.4594 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4226  | total loss: \u001b[1m\u001b[32m0.70781\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4226 | loss: 0.70781 - acc: 0.4735 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4227  | total loss: \u001b[1m\u001b[32m0.70655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4227 | loss: 0.70655 - acc: 0.4861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4228  | total loss: \u001b[1m\u001b[32m0.70541\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4228 | loss: 0.70541 - acc: 0.4975 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4229  | total loss: \u001b[1m\u001b[32m0.70438\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4229 | loss: 0.70438 - acc: 0.5078 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4230  | total loss: \u001b[1m\u001b[32m0.70346\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4230 | loss: 0.70346 - acc: 0.5170 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4231  | total loss: \u001b[1m\u001b[32m0.70263\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4231 | loss: 0.70263 - acc: 0.5253 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4232  | total loss: \u001b[1m\u001b[32m0.70188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4232 | loss: 0.70188 - acc: 0.5328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4233  | total loss: \u001b[1m\u001b[32m0.70121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4233 | loss: 0.70121 - acc: 0.5395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4234  | total loss: \u001b[1m\u001b[32m0.70060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4234 | loss: 0.70060 - acc: 0.5455 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4235  | total loss: \u001b[1m\u001b[32m0.70006\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4235 | loss: 0.70006 - acc: 0.5510 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4236  | total loss: \u001b[1m\u001b[32m0.69957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4236 | loss: 0.69957 - acc: 0.5559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4237  | total loss: \u001b[1m\u001b[32m0.69913\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4237 | loss: 0.69913 - acc: 0.5603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4238  | total loss: \u001b[1m\u001b[32m0.70373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4238 | loss: 0.70373 - acc: 0.5143 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4239  | total loss: \u001b[1m\u001b[32m0.70287\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4239 | loss: 0.70287 - acc: 0.5228 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4240  | total loss: \u001b[1m\u001b[32m0.70210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4240 | loss: 0.70210 - acc: 0.5306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4241  | total loss: \u001b[1m\u001b[32m0.70141\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4241 | loss: 0.70141 - acc: 0.5375 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4242  | total loss: \u001b[1m\u001b[32m0.70078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4242 | loss: 0.70078 - acc: 0.5438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4243  | total loss: \u001b[1m\u001b[32m0.70022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4243 | loss: 0.70022 - acc: 0.5494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4244  | total loss: \u001b[1m\u001b[32m0.69972\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4244 | loss: 0.69972 - acc: 0.5544 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4245  | total loss: \u001b[1m\u001b[32m0.69926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4245 | loss: 0.69926 - acc: 0.5590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4246  | total loss: \u001b[1m\u001b[32m0.69885\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4246 | loss: 0.69885 - acc: 0.5631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4247  | total loss: \u001b[1m\u001b[32m0.69848\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4247 | loss: 0.69848 - acc: 0.5668 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4248  | total loss: \u001b[1m\u001b[32m0.69815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4248 | loss: 0.69815 - acc: 0.5701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4249  | total loss: \u001b[1m\u001b[32m0.69785\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4249 | loss: 0.69785 - acc: 0.5731 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4250  | total loss: \u001b[1m\u001b[32m0.69758\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4250 | loss: 0.69758 - acc: 0.5758 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4251  | total loss: \u001b[1m\u001b[32m0.69734\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4251 | loss: 0.69734 - acc: 0.5782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4252  | total loss: \u001b[1m\u001b[32m0.70112\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4252 | loss: 0.70112 - acc: 0.5404 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4253  | total loss: \u001b[1m\u001b[32m0.70052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4253 | loss: 0.70052 - acc: 0.5463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4254  | total loss: \u001b[1m\u001b[32m0.69999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4254 | loss: 0.69999 - acc: 0.5517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4255  | total loss: \u001b[1m\u001b[32m0.69950\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4255 | loss: 0.69950 - acc: 0.5565 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4256  | total loss: \u001b[1m\u001b[32m0.69907\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4256 | loss: 0.69907 - acc: 0.5609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4257  | total loss: \u001b[1m\u001b[32m0.69868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4257 | loss: 0.69868 - acc: 0.5648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4258  | total loss: \u001b[1m\u001b[32m0.69833\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4258 | loss: 0.69833 - acc: 0.5683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4259  | total loss: \u001b[1m\u001b[32m0.69801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4259 | loss: 0.69801 - acc: 0.5715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4260  | total loss: \u001b[1m\u001b[32m0.69772\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4260 | loss: 0.69772 - acc: 0.5743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4261  | total loss: \u001b[1m\u001b[32m0.69747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4261 | loss: 0.69747 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4262  | total loss: \u001b[1m\u001b[32m0.69724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4262 | loss: 0.69724 - acc: 0.5792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4263  | total loss: \u001b[1m\u001b[32m0.69703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4263 | loss: 0.69703 - acc: 0.5813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4264  | total loss: \u001b[1m\u001b[32m0.69684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4264 | loss: 0.69684 - acc: 0.5832 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4265  | total loss: \u001b[1m\u001b[32m0.69667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4265 | loss: 0.69667 - acc: 0.5848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4266  | total loss: \u001b[1m\u001b[32m0.70252\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4266 | loss: 0.70252 - acc: 0.5264 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4267  | total loss: \u001b[1m\u001b[32m0.70179\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4267 | loss: 0.70179 - acc: 0.5337 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4268  | total loss: \u001b[1m\u001b[32m0.70112\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4268 | loss: 0.70112 - acc: 0.5404 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4269  | total loss: \u001b[1m\u001b[32m0.70053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4269 | loss: 0.70053 - acc: 0.5463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4270  | total loss: \u001b[1m\u001b[32m0.69999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4270 | loss: 0.69999 - acc: 0.5517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4271  | total loss: \u001b[1m\u001b[32m0.69951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4271 | loss: 0.69951 - acc: 0.5565 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4272  | total loss: \u001b[1m\u001b[32m0.69907\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4272 | loss: 0.69907 - acc: 0.5609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4273  | total loss: \u001b[1m\u001b[32m0.69868\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4273 | loss: 0.69868 - acc: 0.5648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4274  | total loss: \u001b[1m\u001b[32m0.69833\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4274 | loss: 0.69833 - acc: 0.5683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4275  | total loss: \u001b[1m\u001b[32m0.69801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4275 | loss: 0.69801 - acc: 0.5715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4276  | total loss: \u001b[1m\u001b[32m0.69773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4276 | loss: 0.69773 - acc: 0.5743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4277  | total loss: \u001b[1m\u001b[32m0.69747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4277 | loss: 0.69747 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4278  | total loss: \u001b[1m\u001b[32m0.69724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4278 | loss: 0.69724 - acc: 0.5792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4279  | total loss: \u001b[1m\u001b[32m0.69703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4279 | loss: 0.69703 - acc: 0.5813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4280  | total loss: \u001b[1m\u001b[32m0.69684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4280 | loss: 0.69684 - acc: 0.5832 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4281  | total loss: \u001b[1m\u001b[32m0.69668\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4281 | loss: 0.69668 - acc: 0.5848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4282  | total loss: \u001b[1m\u001b[32m0.69652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4282 | loss: 0.69652 - acc: 0.5864 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4283  | total loss: \u001b[1m\u001b[32m0.69639\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4283 | loss: 0.69639 - acc: 0.5877 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4284  | total loss: \u001b[1m\u001b[32m0.69626\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4284 | loss: 0.69626 - acc: 0.5889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4285  | total loss: \u001b[1m\u001b[32m0.69615\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4285 | loss: 0.69615 - acc: 0.5901 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4286  | total loss: \u001b[1m\u001b[32m0.69605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4286 | loss: 0.69605 - acc: 0.5910 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4287  | total loss: \u001b[1m\u001b[32m0.69596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4287 | loss: 0.69596 - acc: 0.5919 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4288  | total loss: \u001b[1m\u001b[32m0.69588\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4288 | loss: 0.69588 - acc: 0.5927 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4289  | total loss: \u001b[1m\u001b[32m0.69581\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4289 | loss: 0.69581 - acc: 0.5935 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4290  | total loss: \u001b[1m\u001b[32m0.69575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4290 | loss: 0.69575 - acc: 0.5941 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4291  | total loss: \u001b[1m\u001b[32m0.69569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4291 | loss: 0.69569 - acc: 0.5947 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4292  | total loss: \u001b[1m\u001b[32m0.69963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4292 | loss: 0.69963 - acc: 0.5552 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4293  | total loss: \u001b[1m\u001b[32m0.69919\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4293 | loss: 0.69919 - acc: 0.5597 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4294  | total loss: \u001b[1m\u001b[32m0.69878\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4294 | loss: 0.69878 - acc: 0.5637 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4295  | total loss: \u001b[1m\u001b[32m0.69842\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4295 | loss: 0.69842 - acc: 0.5674 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4296  | total loss: \u001b[1m\u001b[32m0.69810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4296 | loss: 0.69810 - acc: 0.5706 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4297  | total loss: \u001b[1m\u001b[32m0.69780\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4297 | loss: 0.69780 - acc: 0.5736 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4298  | total loss: \u001b[1m\u001b[32m0.69754\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4298 | loss: 0.69754 - acc: 0.5762 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4299  | total loss: \u001b[1m\u001b[32m0.69730\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4299 | loss: 0.69730 - acc: 0.5786 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4300  | total loss: \u001b[1m\u001b[32m0.69709\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4300 | loss: 0.69709 - acc: 0.5807 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4301  | total loss: \u001b[1m\u001b[32m0.69689\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4301 | loss: 0.69689 - acc: 0.5827 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4302  | total loss: \u001b[1m\u001b[32m0.69672\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4302 | loss: 0.69672 - acc: 0.5844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4303  | total loss: \u001b[1m\u001b[32m0.69656\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4303 | loss: 0.69656 - acc: 0.5860 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4304  | total loss: \u001b[1m\u001b[32m0.69642\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4304 | loss: 0.69642 - acc: 0.5874 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4305  | total loss: \u001b[1m\u001b[32m0.69630\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4305 | loss: 0.69630 - acc: 0.5886 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4306  | total loss: \u001b[1m\u001b[32m0.69618\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4306 | loss: 0.69618 - acc: 0.5898 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4307  | total loss: \u001b[1m\u001b[32m0.69608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4307 | loss: 0.69608 - acc: 0.5908 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4308  | total loss: \u001b[1m\u001b[32m0.70099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4308 | loss: 0.70099 - acc: 0.5417 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4309  | total loss: \u001b[1m\u001b[32m0.70041\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4309 | loss: 0.70041 - acc: 0.5475 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4310  | total loss: \u001b[1m\u001b[32m0.69988\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4310 | loss: 0.69988 - acc: 0.5528 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4311  | total loss: \u001b[1m\u001b[32m0.69941\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4311 | loss: 0.69941 - acc: 0.5575 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4312  | total loss: \u001b[1m\u001b[32m0.69898\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4312 | loss: 0.69898 - acc: 0.5618 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4313  | total loss: \u001b[1m\u001b[32m0.69860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4313 | loss: 0.69860 - acc: 0.5656 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4314  | total loss: \u001b[1m\u001b[32m0.69826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4314 | loss: 0.69826 - acc: 0.5690 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4315  | total loss: \u001b[1m\u001b[32m0.69795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4315 | loss: 0.69795 - acc: 0.5721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4316  | total loss: \u001b[1m\u001b[32m0.70267\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4316 | loss: 0.70267 - acc: 0.5249 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4317  | total loss: \u001b[1m\u001b[32m0.70192\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4317 | loss: 0.70192 - acc: 0.5324 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4318  | total loss: \u001b[1m\u001b[32m0.70124\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4318 | loss: 0.70124 - acc: 0.5392 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4319  | total loss: \u001b[1m\u001b[32m0.70063\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4319 | loss: 0.70063 - acc: 0.5453 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4320  | total loss: \u001b[1m\u001b[32m0.70009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4320 | loss: 0.70009 - acc: 0.5507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4321  | total loss: \u001b[1m\u001b[32m0.69959\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4321 | loss: 0.69959 - acc: 0.5557 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4322  | total loss: \u001b[1m\u001b[32m0.69915\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4322 | loss: 0.69915 - acc: 0.5601 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4323  | total loss: \u001b[1m\u001b[32m0.69875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4323 | loss: 0.69875 - acc: 0.5641 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4324  | total loss: \u001b[1m\u001b[32m0.69839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4324 | loss: 0.69839 - acc: 0.5677 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4325  | total loss: \u001b[1m\u001b[32m0.69807\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4325 | loss: 0.69807 - acc: 0.5709 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4326  | total loss: \u001b[1m\u001b[32m0.69778\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4326 | loss: 0.69778 - acc: 0.5738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4327  | total loss: \u001b[1m\u001b[32m0.69752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4327 | loss: 0.69752 - acc: 0.5764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4328  | total loss: \u001b[1m\u001b[32m0.69728\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4328 | loss: 0.69728 - acc: 0.5788 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4329  | total loss: \u001b[1m\u001b[32m0.69707\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4329 | loss: 0.69707 - acc: 0.5809 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4330  | total loss: \u001b[1m\u001b[32m0.70188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4330 | loss: 0.70188 - acc: 0.5328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4331  | total loss: \u001b[1m\u001b[32m0.70120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4331 | loss: 0.70120 - acc: 0.5395 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4332  | total loss: \u001b[1m\u001b[32m0.70060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4332 | loss: 0.70060 - acc: 0.5456 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4333  | total loss: \u001b[1m\u001b[32m0.70006\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4333 | loss: 0.70006 - acc: 0.5510 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4334  | total loss: \u001b[1m\u001b[32m0.69957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4334 | loss: 0.69957 - acc: 0.5559 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4335  | total loss: \u001b[1m\u001b[32m0.69913\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4335 | loss: 0.69913 - acc: 0.5603 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4336  | total loss: \u001b[1m\u001b[32m0.70373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4336 | loss: 0.70373 - acc: 0.5143 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4337  | total loss: \u001b[1m\u001b[32m0.70287\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4337 | loss: 0.70287 - acc: 0.5229 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4338  | total loss: \u001b[1m\u001b[32m0.70210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4338 | loss: 0.70210 - acc: 0.5306 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4339  | total loss: \u001b[1m\u001b[32m0.70141\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4339 | loss: 0.70141 - acc: 0.5375 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4340  | total loss: \u001b[1m\u001b[32m0.70078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4340 | loss: 0.70078 - acc: 0.5438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4341  | total loss: \u001b[1m\u001b[32m0.70022\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4341 | loss: 0.70022 - acc: 0.5494 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4342  | total loss: \u001b[1m\u001b[32m0.69971\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4342 | loss: 0.69971 - acc: 0.5545 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4343  | total loss: \u001b[1m\u001b[32m0.69926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4343 | loss: 0.69926 - acc: 0.5590 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4344  | total loss: \u001b[1m\u001b[32m0.69885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4344 | loss: 0.69885 - acc: 0.5631 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4345  | total loss: \u001b[1m\u001b[32m0.69848\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4345 | loss: 0.69848 - acc: 0.5668 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4346  | total loss: \u001b[1m\u001b[32m0.69815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4346 | loss: 0.69815 - acc: 0.5701 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4347  | total loss: \u001b[1m\u001b[32m0.69785\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4347 | loss: 0.69785 - acc: 0.5731 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4348  | total loss: \u001b[1m\u001b[32m0.69758\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4348 | loss: 0.69758 - acc: 0.5758 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4349  | total loss: \u001b[1m\u001b[32m0.69734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4349 | loss: 0.69734 - acc: 0.5782 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4350  | total loss: \u001b[1m\u001b[32m0.69712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4350 | loss: 0.69712 - acc: 0.5804 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4351  | total loss: \u001b[1m\u001b[32m0.69692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4351 | loss: 0.69692 - acc: 0.5824 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4352  | total loss: \u001b[1m\u001b[32m0.69675\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4352 | loss: 0.69675 - acc: 0.5841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4353  | total loss: \u001b[1m\u001b[32m0.69659\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4353 | loss: 0.69659 - acc: 0.5857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4354  | total loss: \u001b[1m\u001b[32m0.70245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4354 | loss: 0.70245 - acc: 0.5271 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4355  | total loss: \u001b[1m\u001b[32m0.70172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4355 | loss: 0.70172 - acc: 0.5344 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4356  | total loss: \u001b[1m\u001b[32m0.70106\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4356 | loss: 0.70106 - acc: 0.5410 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4357  | total loss: \u001b[1m\u001b[32m0.70047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4357 | loss: 0.70047 - acc: 0.5469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4358  | total loss: \u001b[1m\u001b[32m0.70594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4358 | loss: 0.70594 - acc: 0.4922 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4359  | total loss: \u001b[1m\u001b[32m0.70486\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4359 | loss: 0.70486 - acc: 0.5030 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4360  | total loss: \u001b[1m\u001b[32m0.70389\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4360 | loss: 0.70389 - acc: 0.5127 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4361  | total loss: \u001b[1m\u001b[32m0.70302\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4361 | loss: 0.70302 - acc: 0.5214 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4362  | total loss: \u001b[1m\u001b[32m0.70723\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4362 | loss: 0.70723 - acc: 0.4793 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4363  | total loss: \u001b[1m\u001b[32m0.70602\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4363 | loss: 0.70602 - acc: 0.4913 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4364  | total loss: \u001b[1m\u001b[32m0.70494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4364 | loss: 0.70494 - acc: 0.5022 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4365  | total loss: \u001b[1m\u001b[32m0.70396\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4365 | loss: 0.70396 - acc: 0.5120 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4366  | total loss: \u001b[1m\u001b[32m0.70308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4366 | loss: 0.70308 - acc: 0.5208 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4367  | total loss: \u001b[1m\u001b[32m0.70229\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4367 | loss: 0.70229 - acc: 0.5287 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4368  | total loss: \u001b[1m\u001b[32m0.70157\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4368 | loss: 0.70157 - acc: 0.5358 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4369  | total loss: \u001b[1m\u001b[32m0.70093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4369 | loss: 0.70093 - acc: 0.5423 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4370  | total loss: \u001b[1m\u001b[32m0.70036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4370 | loss: 0.70036 - acc: 0.5480 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4371  | total loss: \u001b[1m\u001b[32m0.69984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4371 | loss: 0.69984 - acc: 0.5532 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4372  | total loss: \u001b[1m\u001b[32m0.69937\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4372 | loss: 0.69937 - acc: 0.5579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4373  | total loss: \u001b[1m\u001b[32m0.69895\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4373 | loss: 0.69895 - acc: 0.5621 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4374  | total loss: \u001b[1m\u001b[32m0.70457\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4374 | loss: 0.70457 - acc: 0.5059 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4375  | total loss: \u001b[1m\u001b[32m0.70363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4375 | loss: 0.70363 - acc: 0.5153 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4376  | total loss: \u001b[1m\u001b[32m0.70778\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4376 | loss: 0.70778 - acc: 0.4738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4377  | total loss: \u001b[1m\u001b[32m0.70652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4377 | loss: 0.70652 - acc: 0.4864 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4378  | total loss: \u001b[1m\u001b[32m0.70538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4378 | loss: 0.70538 - acc: 0.4978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4379  | total loss: \u001b[1m\u001b[32m0.70436\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4379 | loss: 0.70436 - acc: 0.5080 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4380  | total loss: \u001b[1m\u001b[32m0.70344\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4380 | loss: 0.70344 - acc: 0.5172 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4381  | total loss: \u001b[1m\u001b[32m0.70261\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4381 | loss: 0.70261 - acc: 0.5255 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4382  | total loss: \u001b[1m\u001b[32m0.70187\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4382 | loss: 0.70187 - acc: 0.5329 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4383  | total loss: \u001b[1m\u001b[32m0.70120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4383 | loss: 0.70120 - acc: 0.5396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4384  | total loss: \u001b[1m\u001b[32m0.70059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4384 | loss: 0.70059 - acc: 0.5457 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4385  | total loss: \u001b[1m\u001b[32m0.70005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4385 | loss: 0.70005 - acc: 0.5511 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4386  | total loss: \u001b[1m\u001b[32m0.69956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4386 | loss: 0.69956 - acc: 0.5560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4387  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4387 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4388  | total loss: \u001b[1m\u001b[32m0.69872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4388 | loss: 0.69872 - acc: 0.5644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4389  | total loss: \u001b[1m\u001b[32m0.69837\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4389 | loss: 0.69837 - acc: 0.5679 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4390  | total loss: \u001b[1m\u001b[32m0.69805\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4390 | loss: 0.69805 - acc: 0.5711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4391  | total loss: \u001b[1m\u001b[32m0.69776\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4391 | loss: 0.69776 - acc: 0.5740 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4392  | total loss: \u001b[1m\u001b[32m0.69750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4392 | loss: 0.69750 - acc: 0.5766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4393  | total loss: \u001b[1m\u001b[32m0.69726\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4393 | loss: 0.69726 - acc: 0.5790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4394  | total loss: \u001b[1m\u001b[32m0.69705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4394 | loss: 0.69705 - acc: 0.5811 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4395  | total loss: \u001b[1m\u001b[32m0.69686\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4395 | loss: 0.69686 - acc: 0.5829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4396  | total loss: \u001b[1m\u001b[32m0.70069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4396 | loss: 0.70069 - acc: 0.5447 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4397  | total loss: \u001b[1m\u001b[32m0.70014\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4397 | loss: 0.70014 - acc: 0.5502 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4398  | total loss: \u001b[1m\u001b[32m0.69964\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4398 | loss: 0.69964 - acc: 0.5552 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4399  | total loss: \u001b[1m\u001b[32m0.69919\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 4399 | loss: 0.69919 - acc: 0.5597 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4400  | total loss: \u001b[1m\u001b[32m0.69879\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4400 | loss: 0.69879 - acc: 0.5637 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4401  | total loss: \u001b[1m\u001b[32m0.69843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4401 | loss: 0.69843 - acc: 0.5673 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4402  | total loss: \u001b[1m\u001b[32m0.70310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4402 | loss: 0.70310 - acc: 0.5206 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4403  | total loss: \u001b[1m\u001b[32m0.70231\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4403 | loss: 0.70231 - acc: 0.5285 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4404  | total loss: \u001b[1m\u001b[32m0.70159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4404 | loss: 0.70159 - acc: 0.5357 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4405  | total loss: \u001b[1m\u001b[32m0.70095\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4405 | loss: 0.70095 - acc: 0.5421 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4406  | total loss: \u001b[1m\u001b[32m0.70637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4406 | loss: 0.70637 - acc: 0.4879 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4407  | total loss: \u001b[1m\u001b[32m0.70525\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4407 | loss: 0.70525 - acc: 0.4991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4408  | total loss: \u001b[1m\u001b[32m0.70424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4408 | loss: 0.70424 - acc: 0.5092 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4409  | total loss: \u001b[1m\u001b[32m0.70333\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4409 | loss: 0.70333 - acc: 0.5183 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4410  | total loss: \u001b[1m\u001b[32m0.70251\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4410 | loss: 0.70251 - acc: 0.5264 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4411  | total loss: \u001b[1m\u001b[32m0.70178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4411 | loss: 0.70178 - acc: 0.5338 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4412  | total loss: \u001b[1m\u001b[32m0.70112\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4412 | loss: 0.70112 - acc: 0.5404 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4413  | total loss: \u001b[1m\u001b[32m0.70052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4413 | loss: 0.70052 - acc: 0.5464 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4414  | total loss: \u001b[1m\u001b[32m0.69998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4414 | loss: 0.69998 - acc: 0.5517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4415  | total loss: \u001b[1m\u001b[32m0.69950\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4415 | loss: 0.69950 - acc: 0.5566 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4416  | total loss: \u001b[1m\u001b[32m0.69907\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4416 | loss: 0.69907 - acc: 0.5609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4417  | total loss: \u001b[1m\u001b[32m0.69868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4417 | loss: 0.69868 - acc: 0.5648 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4418  | total loss: \u001b[1m\u001b[32m0.69832\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4418 | loss: 0.69832 - acc: 0.5683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4419  | total loss: \u001b[1m\u001b[32m0.69801\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4419 | loss: 0.69801 - acc: 0.5715 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4420  | total loss: \u001b[1m\u001b[32m0.69772\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4420 | loss: 0.69772 - acc: 0.5744 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4421  | total loss: \u001b[1m\u001b[32m0.69747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4421 | loss: 0.69747 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4422  | total loss: \u001b[1m\u001b[32m0.69724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4422 | loss: 0.69724 - acc: 0.5792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4423  | total loss: \u001b[1m\u001b[32m0.69703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4423 | loss: 0.69703 - acc: 0.5813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4424  | total loss: \u001b[1m\u001b[32m0.69684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4424 | loss: 0.69684 - acc: 0.5832 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4425  | total loss: \u001b[1m\u001b[32m0.69667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4425 | loss: 0.69667 - acc: 0.5849 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4426  | total loss: \u001b[1m\u001b[32m0.70152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4426 | loss: 0.70152 - acc: 0.5364 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4427  | total loss: \u001b[1m\u001b[32m0.70089\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4427 | loss: 0.70089 - acc: 0.5427 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4428  | total loss: \u001b[1m\u001b[32m0.70031\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4428 | loss: 0.70031 - acc: 0.5485 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4429  | total loss: \u001b[1m\u001b[32m0.69980\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4429 | loss: 0.69980 - acc: 0.5536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4430  | total loss: \u001b[1m\u001b[32m0.69933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4430 | loss: 0.69933 - acc: 0.5583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4431  | total loss: \u001b[1m\u001b[32m0.69892\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4431 | loss: 0.69892 - acc: 0.5624 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4432  | total loss: \u001b[1m\u001b[32m0.69854\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4432 | loss: 0.69854 - acc: 0.5662 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4433  | total loss: \u001b[1m\u001b[32m0.69820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4433 | loss: 0.69820 - acc: 0.5696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4434  | total loss: \u001b[1m\u001b[32m0.69790\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4434 | loss: 0.69790 - acc: 0.5726 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4435  | total loss: \u001b[1m\u001b[32m0.69762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4435 | loss: 0.69762 - acc: 0.5753 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4436  | total loss: \u001b[1m\u001b[32m0.69738\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4436 | loss: 0.69738 - acc: 0.5778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4437  | total loss: \u001b[1m\u001b[32m0.69716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4437 | loss: 0.69716 - acc: 0.5800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4438  | total loss: \u001b[1m\u001b[32m0.69696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4438 | loss: 0.69696 - acc: 0.5820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4439  | total loss: \u001b[1m\u001b[32m0.69678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4439 | loss: 0.69678 - acc: 0.5838 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4440  | total loss: \u001b[1m\u001b[32m0.70161\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4440 | loss: 0.70161 - acc: 0.5354 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4441  | total loss: \u001b[1m\u001b[32m0.70097\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4441 | loss: 0.70097 - acc: 0.5419 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4442  | total loss: \u001b[1m\u001b[32m0.70039\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4442 | loss: 0.70039 - acc: 0.5477 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4443  | total loss: \u001b[1m\u001b[32m0.69986\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4443 | loss: 0.69986 - acc: 0.5529 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4444  | total loss: \u001b[1m\u001b[32m0.69939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4444 | loss: 0.69939 - acc: 0.5576 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4445  | total loss: \u001b[1m\u001b[32m0.69897\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4445 | loss: 0.69897 - acc: 0.5619 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4446  | total loss: \u001b[1m\u001b[32m0.69859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4446 | loss: 0.69859 - acc: 0.5657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4447  | total loss: \u001b[1m\u001b[32m0.69825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4447 | loss: 0.69825 - acc: 0.5691 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4448  | total loss: \u001b[1m\u001b[32m0.69794\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4448 | loss: 0.69794 - acc: 0.5722 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4449  | total loss: \u001b[1m\u001b[32m0.69766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4449 | loss: 0.69766 - acc: 0.5750 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4450  | total loss: \u001b[1m\u001b[32m0.69741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4450 | loss: 0.69741 - acc: 0.5775 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4451  | total loss: \u001b[1m\u001b[32m0.69718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4451 | loss: 0.69718 - acc: 0.5797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4452  | total loss: \u001b[1m\u001b[32m0.69698\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4452 | loss: 0.69698 - acc: 0.5818 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4453  | total loss: \u001b[1m\u001b[32m0.69680\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4453 | loss: 0.69680 - acc: 0.5836 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4454  | total loss: \u001b[1m\u001b[32m0.69664\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4454 | loss: 0.69664 - acc: 0.5852 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4455  | total loss: \u001b[1m\u001b[32m0.69649\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4455 | loss: 0.69649 - acc: 0.5867 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4456  | total loss: \u001b[1m\u001b[32m0.69636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4456 | loss: 0.69636 - acc: 0.5880 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4457  | total loss: \u001b[1m\u001b[32m0.69624\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4457 | loss: 0.69624 - acc: 0.5892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4458  | total loss: \u001b[1m\u001b[32m0.70113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4458 | loss: 0.70113 - acc: 0.5403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4459  | total loss: \u001b[1m\u001b[32m0.70053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4459 | loss: 0.70053 - acc: 0.5463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4460  | total loss: \u001b[1m\u001b[32m0.69999\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4460 | loss: 0.69999 - acc: 0.5517 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4461  | total loss: \u001b[1m\u001b[32m0.69951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4461 | loss: 0.69951 - acc: 0.5565 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4462  | total loss: \u001b[1m\u001b[32m0.69908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4462 | loss: 0.69908 - acc: 0.5608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4463  | total loss: \u001b[1m\u001b[32m0.69868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4463 | loss: 0.69868 - acc: 0.5648 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4464  | total loss: \u001b[1m\u001b[32m0.70433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4464 | loss: 0.70433 - acc: 0.5083 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4465  | total loss: \u001b[1m\u001b[32m0.70341\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4465 | loss: 0.70341 - acc: 0.5175 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4466  | total loss: \u001b[1m\u001b[32m0.70259\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4466 | loss: 0.70259 - acc: 0.5257 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4467  | total loss: \u001b[1m\u001b[32m0.70185\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4467 | loss: 0.70185 - acc: 0.5331 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4468  | total loss: \u001b[1m\u001b[32m0.70118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4468 | loss: 0.70118 - acc: 0.5398 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4469  | total loss: \u001b[1m\u001b[32m0.70057\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4469 | loss: 0.70057 - acc: 0.5458 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4470  | total loss: \u001b[1m\u001b[32m0.70003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4470 | loss: 0.70003 - acc: 0.5513 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4471  | total loss: \u001b[1m\u001b[32m0.69955\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4471 | loss: 0.69955 - acc: 0.5561 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4472  | total loss: \u001b[1m\u001b[32m0.69911\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4472 | loss: 0.69911 - acc: 0.5605 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4473  | total loss: \u001b[1m\u001b[32m0.69871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4473 | loss: 0.69871 - acc: 0.5645 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4474  | total loss: \u001b[1m\u001b[32m0.69836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4474 | loss: 0.69836 - acc: 0.5680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4475  | total loss: \u001b[1m\u001b[32m0.69804\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4475 | loss: 0.69804 - acc: 0.5712 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4476  | total loss: \u001b[1m\u001b[32m0.70375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4476 | loss: 0.70375 - acc: 0.5141 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4477  | total loss: \u001b[1m\u001b[32m0.70289\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4477 | loss: 0.70289 - acc: 0.5227 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4478  | total loss: \u001b[1m\u001b[32m0.70212\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4478 | loss: 0.70212 - acc: 0.5304 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4479  | total loss: \u001b[1m\u001b[32m0.70142\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4479 | loss: 0.70142 - acc: 0.5374 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4480  | total loss: \u001b[1m\u001b[32m0.70580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4480 | loss: 0.70580 - acc: 0.4936 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4481  | total loss: \u001b[1m\u001b[32m0.70473\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4481 | loss: 0.70473 - acc: 0.5043 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4482  | total loss: \u001b[1m\u001b[32m0.70377\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4482 | loss: 0.70377 - acc: 0.5138 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4483  | total loss: \u001b[1m\u001b[32m0.70291\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4483 | loss: 0.70291 - acc: 0.5225 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4484  | total loss: \u001b[1m\u001b[32m0.70214\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4484 | loss: 0.70214 - acc: 0.5302 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4485  | total loss: \u001b[1m\u001b[32m0.70144\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4485 | loss: 0.70144 - acc: 0.5372 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4486  | total loss: \u001b[1m\u001b[32m0.70081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4486 | loss: 0.70081 - acc: 0.5435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4487  | total loss: \u001b[1m\u001b[32m0.70025\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4487 | loss: 0.70025 - acc: 0.5491 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4488  | total loss: \u001b[1m\u001b[32m0.69974\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4488 | loss: 0.69974 - acc: 0.5542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4489  | total loss: \u001b[1m\u001b[32m0.69928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4489 | loss: 0.69928 - acc: 0.5588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4490  | total loss: \u001b[1m\u001b[32m0.69887\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4490 | loss: 0.69887 - acc: 0.5629 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4491  | total loss: \u001b[1m\u001b[32m0.69850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4491 | loss: 0.69850 - acc: 0.5666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4492  | total loss: \u001b[1m\u001b[32m0.69816\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4492 | loss: 0.69816 - acc: 0.5700 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4493  | total loss: \u001b[1m\u001b[32m0.69786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4493 | loss: 0.69786 - acc: 0.5730 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4494  | total loss: \u001b[1m\u001b[32m0.69759\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4494 | loss: 0.69759 - acc: 0.5757 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4495  | total loss: \u001b[1m\u001b[32m0.69735\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4495 | loss: 0.69735 - acc: 0.5781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4496  | total loss: \u001b[1m\u001b[32m0.70113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4496 | loss: 0.70113 - acc: 0.5403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4497  | total loss: \u001b[1m\u001b[32m0.70053\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4497 | loss: 0.70053 - acc: 0.5463 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4498  | total loss: \u001b[1m\u001b[32m0.70000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4498 | loss: 0.70000 - acc: 0.5516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4499  | total loss: \u001b[1m\u001b[32m0.69951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4499 | loss: 0.69951 - acc: 0.5565 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4500  | total loss: \u001b[1m\u001b[32m0.69908\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4500 | loss: 0.69908 - acc: 0.5608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4501  | total loss: \u001b[1m\u001b[32m0.69868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4501 | loss: 0.69868 - acc: 0.5647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4502  | total loss: \u001b[1m\u001b[32m0.69833\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4502 | loss: 0.69833 - acc: 0.5683 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4503  | total loss: \u001b[1m\u001b[32m0.69801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4503 | loss: 0.69801 - acc: 0.5714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4504  | total loss: \u001b[1m\u001b[32m0.69773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4504 | loss: 0.69773 - acc: 0.5743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4505  | total loss: \u001b[1m\u001b[32m0.69747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4505 | loss: 0.69747 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4506  | total loss: \u001b[1m\u001b[32m0.69724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4506 | loss: 0.69724 - acc: 0.5792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4507  | total loss: \u001b[1m\u001b[32m0.69703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4507 | loss: 0.69703 - acc: 0.5813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4508  | total loss: \u001b[1m\u001b[32m0.69685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4508 | loss: 0.69685 - acc: 0.5831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4509  | total loss: \u001b[1m\u001b[32m0.69668\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4509 | loss: 0.69668 - acc: 0.5848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4510  | total loss: \u001b[1m\u001b[32m0.69652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4510 | loss: 0.69652 - acc: 0.5863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4511  | total loss: \u001b[1m\u001b[32m0.69639\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4511 | loss: 0.69639 - acc: 0.5877 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4512  | total loss: \u001b[1m\u001b[32m0.69627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4512 | loss: 0.69627 - acc: 0.5889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4513  | total loss: \u001b[1m\u001b[32m0.69615\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4513 | loss: 0.69615 - acc: 0.5900 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4514  | total loss: \u001b[1m\u001b[32m0.70105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4514 | loss: 0.70105 - acc: 0.5410 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4515  | total loss: \u001b[1m\u001b[32m0.70047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4515 | loss: 0.70047 - acc: 0.5469 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4516  | total loss: \u001b[1m\u001b[32m0.70393\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4516 | loss: 0.70393 - acc: 0.5122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4517  | total loss: \u001b[1m\u001b[32m0.70306\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4517 | loss: 0.70306 - acc: 0.5210 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4518  | total loss: \u001b[1m\u001b[32m0.70227\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4518 | loss: 0.70227 - acc: 0.5289 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4519  | total loss: \u001b[1m\u001b[32m0.70156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4519 | loss: 0.70156 - acc: 0.5360 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4520  | total loss: \u001b[1m\u001b[32m0.70492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4520 | loss: 0.70492 - acc: 0.5024 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4521  | total loss: \u001b[1m\u001b[32m0.70394\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4521 | loss: 0.70394 - acc: 0.5122 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4522  | total loss: \u001b[1m\u001b[32m0.70906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4522 | loss: 0.70906 - acc: 0.4610 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4523  | total loss: \u001b[1m\u001b[32m0.70767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4523 | loss: 0.70767 - acc: 0.4749 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4524  | total loss: \u001b[1m\u001b[32m0.71242\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4524 | loss: 0.71242 - acc: 0.4274 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4525  | total loss: \u001b[1m\u001b[32m0.71069\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4525 | loss: 0.71069 - acc: 0.4446 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4526  | total loss: \u001b[1m\u001b[32m0.70914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4526 | loss: 0.70914 - acc: 0.4602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4527  | total loss: \u001b[1m\u001b[32m0.70774\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4527 | loss: 0.70774 - acc: 0.4742 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4528  | total loss: \u001b[1m\u001b[32m0.70648\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4528 | loss: 0.70648 - acc: 0.4867 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4529  | total loss: \u001b[1m\u001b[32m0.70535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4529 | loss: 0.70535 - acc: 0.4981 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4530  | total loss: \u001b[1m\u001b[32m0.71033\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4530 | loss: 0.71033 - acc: 0.4483 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4531  | total loss: \u001b[1m\u001b[32m0.70882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4531 | loss: 0.70882 - acc: 0.4634 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4532  | total loss: \u001b[1m\u001b[32m0.71145\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4532 | loss: 0.71145 - acc: 0.4371 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4533  | total loss: \u001b[1m\u001b[32m0.70982\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4533 | loss: 0.70982 - acc: 0.4534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4534  | total loss: \u001b[1m\u001b[32m0.70835\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4534 | loss: 0.70835 - acc: 0.4680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4535  | total loss: \u001b[1m\u001b[32m0.70703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4535 | loss: 0.70703 - acc: 0.4812 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4536  | total loss: \u001b[1m\u001b[32m0.70585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4536 | loss: 0.70585 - acc: 0.4931 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4537  | total loss: \u001b[1m\u001b[32m0.70478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4537 | loss: 0.70478 - acc: 0.5038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4538  | total loss: \u001b[1m\u001b[32m0.70982\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4538 | loss: 0.70982 - acc: 0.4534 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4539  | total loss: \u001b[1m\u001b[32m0.70835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4539 | loss: 0.70835 - acc: 0.4681 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4540  | total loss: \u001b[1m\u001b[32m0.71203\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4540 | loss: 0.71203 - acc: 0.4313 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4541  | total loss: \u001b[1m\u001b[32m0.71034\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4541 | loss: 0.71034 - acc: 0.4481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4542  | total loss: \u001b[1m\u001b[32m0.70883\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4542 | loss: 0.70883 - acc: 0.4633 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4543  | total loss: \u001b[1m\u001b[32m0.70746\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4543 | loss: 0.70746 - acc: 0.4770 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4544  | total loss: \u001b[1m\u001b[32m0.71023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4544 | loss: 0.71023 - acc: 0.4493 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4545  | total loss: \u001b[1m\u001b[32m0.70872\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4545 | loss: 0.70872 - acc: 0.4644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4546  | total loss: \u001b[1m\u001b[32m0.71337\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4546 | loss: 0.71337 - acc: 0.4179 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4547  | total loss: \u001b[1m\u001b[32m0.71154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4547 | loss: 0.71154 - acc: 0.4361 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4548  | total loss: \u001b[1m\u001b[32m0.71491\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4548 | loss: 0.71491 - acc: 0.4025 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4549  | total loss: \u001b[1m\u001b[32m0.71293\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4549 | loss: 0.71293 - acc: 0.4223 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4550  | total loss: \u001b[1m\u001b[32m0.71115\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4550 | loss: 0.71115 - acc: 0.4400 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4551  | total loss: \u001b[1m\u001b[32m0.70955\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4551 | loss: 0.70955 - acc: 0.4560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4552  | total loss: \u001b[1m\u001b[32m0.70812\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4552 | loss: 0.70812 - acc: 0.4704 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4553  | total loss: \u001b[1m\u001b[32m0.70682\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4553 | loss: 0.70682 - acc: 0.4834 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4554  | total loss: \u001b[1m\u001b[32m0.70565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4554 | loss: 0.70565 - acc: 0.4951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4555  | total loss: \u001b[1m\u001b[32m0.70460\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4555 | loss: 0.70460 - acc: 0.5055 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4556  | total loss: \u001b[1m\u001b[32m0.70866\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4556 | loss: 0.70866 - acc: 0.4650 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4557  | total loss: \u001b[1m\u001b[32m0.70731\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4557 | loss: 0.70731 - acc: 0.4785 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4558  | total loss: \u001b[1m\u001b[32m0.70609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4558 | loss: 0.70609 - acc: 0.4906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4559  | total loss: \u001b[1m\u001b[32m0.70500\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4559 | loss: 0.70500 - acc: 0.5016 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4560  | total loss: \u001b[1m\u001b[32m0.71002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4560 | loss: 0.71002 - acc: 0.4514 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4561  | total loss: \u001b[1m\u001b[32m0.70853\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4561 | loss: 0.70853 - acc: 0.4663 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4562  | total loss: \u001b[1m\u001b[32m0.70719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4562 | loss: 0.70719 - acc: 0.4797 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4563  | total loss: \u001b[1m\u001b[32m0.70599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4563 | loss: 0.70599 - acc: 0.4917 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4564  | total loss: \u001b[1m\u001b[32m0.70491\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4564 | loss: 0.70491 - acc: 0.5025 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4565  | total loss: \u001b[1m\u001b[32m0.70393\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4565 | loss: 0.70393 - acc: 0.5123 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4566  | total loss: \u001b[1m\u001b[32m0.70305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4566 | loss: 0.70305 - acc: 0.5210 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4567  | total loss: \u001b[1m\u001b[32m0.70227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4567 | loss: 0.70227 - acc: 0.5289 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4568  | total loss: \u001b[1m\u001b[32m0.70155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4568 | loss: 0.70155 - acc: 0.5360 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4569  | total loss: \u001b[1m\u001b[32m0.70092\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4569 | loss: 0.70092 - acc: 0.5424 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4570  | total loss: \u001b[1m\u001b[32m0.70334\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4570 | loss: 0.70334 - acc: 0.5264 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4571  | total loss: \u001b[1m\u001b[32m0.70252\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4571 | loss: 0.70252 - acc: 0.5264 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4572  | total loss: \u001b[1m\u001b[32m0.70579\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4572 | loss: 0.70579 - acc: 0.4937 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4573  | total loss: \u001b[1m\u001b[32m0.70472\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4573 | loss: 0.70472 - acc: 0.5044 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4574  | total loss: \u001b[1m\u001b[32m0.70777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4574 | loss: 0.70777 - acc: 0.4739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4575  | total loss: \u001b[1m\u001b[32m0.70651\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4575 | loss: 0.70651 - acc: 0.4865 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4576  | total loss: \u001b[1m\u001b[32m0.70537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4576 | loss: 0.70537 - acc: 0.4979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4577  | total loss: \u001b[1m\u001b[32m0.70435\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4577 | loss: 0.70435 - acc: 0.5081 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4578  | total loss: \u001b[1m\u001b[32m0.70343\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4578 | loss: 0.70343 - acc: 0.5173 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4579  | total loss: \u001b[1m\u001b[32m0.70260\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4579 | loss: 0.70260 - acc: 0.5256 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4580  | total loss: \u001b[1m\u001b[32m0.70186\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4580 | loss: 0.70186 - acc: 0.5330 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4581  | total loss: \u001b[1m\u001b[32m0.70119\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4581 | loss: 0.70119 - acc: 0.5397 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4582  | total loss: \u001b[1m\u001b[32m0.70059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4582 | loss: 0.70059 - acc: 0.5457 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4583  | total loss: \u001b[1m\u001b[32m0.70004\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4583 | loss: 0.70004 - acc: 0.5512 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4584  | total loss: \u001b[1m\u001b[32m0.69955\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4584 | loss: 0.69955 - acc: 0.5560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4585  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4585 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4586  | total loss: \u001b[1m\u001b[32m0.69872\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4586 | loss: 0.69872 - acc: 0.5644 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4587  | total loss: \u001b[1m\u001b[32m0.69836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4587 | loss: 0.69836 - acc: 0.5680 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4588  | total loss: \u001b[1m\u001b[32m0.69804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4588 | loss: 0.69804 - acc: 0.5712 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4589  | total loss: \u001b[1m\u001b[32m0.69775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4589 | loss: 0.69775 - acc: 0.5740 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4590  | total loss: \u001b[1m\u001b[32m0.69749\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4590 | loss: 0.69749 - acc: 0.5766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4591  | total loss: \u001b[1m\u001b[32m0.69726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4591 | loss: 0.69726 - acc: 0.5790 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4592  | total loss: \u001b[1m\u001b[32m0.69705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4592 | loss: 0.69705 - acc: 0.5811 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4593  | total loss: \u001b[1m\u001b[32m0.69686\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4593 | loss: 0.69686 - acc: 0.5830 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4594  | total loss: \u001b[1m\u001b[32m0.69669\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4594 | loss: 0.69669 - acc: 0.5847 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4595  | total loss: \u001b[1m\u001b[32m0.69654\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4595 | loss: 0.69654 - acc: 0.5862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4596  | total loss: \u001b[1m\u001b[32m0.70240\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4596 | loss: 0.70240 - acc: 0.5276 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4597  | total loss: \u001b[1m\u001b[32m0.70168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4597 | loss: 0.70168 - acc: 0.5348 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4598  | total loss: \u001b[1m\u001b[32m0.70702\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4598 | loss: 0.70702 - acc: 0.4813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4599  | total loss: \u001b[1m\u001b[32m0.70584\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4599 | loss: 0.70584 - acc: 0.4932 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4600  | total loss: \u001b[1m\u001b[32m0.70477\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4600 | loss: 0.70477 - acc: 0.5039 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4601  | total loss: \u001b[1m\u001b[32m0.70381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4601 | loss: 0.70381 - acc: 0.5135 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4602  | total loss: \u001b[1m\u001b[32m0.70794\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4602 | loss: 0.70794 - acc: 0.4721 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4603  | total loss: \u001b[1m\u001b[32m0.70667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4603 | loss: 0.70667 - acc: 0.4849 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4604  | total loss: \u001b[1m\u001b[32m0.70551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4604 | loss: 0.70551 - acc: 0.4964 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4605  | total loss: \u001b[1m\u001b[32m0.70448\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4605 | loss: 0.70448 - acc: 0.5068 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4606  | total loss: \u001b[1m\u001b[32m0.70955\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4606 | loss: 0.70955 - acc: 0.4561 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4607  | total loss: \u001b[1m\u001b[32m0.70811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4607 | loss: 0.70811 - acc: 0.4705 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4608  | total loss: \u001b[1m\u001b[32m0.71181\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4608 | loss: 0.71181 - acc: 0.4335 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4609  | total loss: \u001b[1m\u001b[32m0.71015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4609 | loss: 0.71015 - acc: 0.4501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4610  | total loss: \u001b[1m\u001b[32m0.70865\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4610 | loss: 0.70865 - acc: 0.4651 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4611  | total loss: \u001b[1m\u001b[32m0.70730\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4611 | loss: 0.70730 - acc: 0.4786 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4612  | total loss: \u001b[1m\u001b[32m0.71009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4612 | loss: 0.71009 - acc: 0.4507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4613  | total loss: \u001b[1m\u001b[32m0.70859\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4613 | loss: 0.70859 - acc: 0.4657 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4614  | total loss: \u001b[1m\u001b[32m0.70725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4614 | loss: 0.70725 - acc: 0.4791 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4615  | total loss: \u001b[1m\u001b[32m0.70604\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4615 | loss: 0.70604 - acc: 0.4912 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4616  | total loss: \u001b[1m\u001b[32m0.70495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4616 | loss: 0.70495 - acc: 0.5021 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4617  | total loss: \u001b[1m\u001b[32m0.70397\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4617 | loss: 0.70397 - acc: 0.5119 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4618  | total loss: \u001b[1m\u001b[32m0.70709\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4618 | loss: 0.70709 - acc: 0.4807 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4619  | total loss: \u001b[1m\u001b[32m0.70590\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4619 | loss: 0.70590 - acc: 0.4926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4620  | total loss: \u001b[1m\u001b[32m0.70482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4620 | loss: 0.70482 - acc: 0.5033 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4621  | total loss: \u001b[1m\u001b[32m0.70386\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4621 | loss: 0.70386 - acc: 0.5130 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4622  | total loss: \u001b[1m\u001b[32m0.70299\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4622 | loss: 0.70299 - acc: 0.5217 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4623  | total loss: \u001b[1m\u001b[32m0.70221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4623 | loss: 0.70221 - acc: 0.5295 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4624  | total loss: \u001b[1m\u001b[32m0.70750\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4624 | loss: 0.70750 - acc: 0.4766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4625  | total loss: \u001b[1m\u001b[32m0.70627\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4625 | loss: 0.70627 - acc: 0.4889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4626  | total loss: \u001b[1m\u001b[32m0.70516\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4626 | loss: 0.70516 - acc: 0.5000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4627  | total loss: \u001b[1m\u001b[32m0.70416\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4627 | loss: 0.70416 - acc: 0.5100 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4628  | total loss: \u001b[1m\u001b[32m0.70826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4628 | loss: 0.70826 - acc: 0.4690 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4629  | total loss: \u001b[1m\u001b[32m0.70695\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4629 | loss: 0.70695 - acc: 0.4821 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4630  | total loss: \u001b[1m\u001b[32m0.70577\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4630 | loss: 0.70577 - acc: 0.4939 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4631  | total loss: \u001b[1m\u001b[32m0.70471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4631 | loss: 0.70471 - acc: 0.5045 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4632  | total loss: \u001b[1m\u001b[32m0.70375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4632 | loss: 0.70375 - acc: 0.5141 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4633  | total loss: \u001b[1m\u001b[32m0.70289\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4633 | loss: 0.70289 - acc: 0.5227 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4634  | total loss: \u001b[1m\u001b[32m0.70212\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4634 | loss: 0.70212 - acc: 0.5304 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4635  | total loss: \u001b[1m\u001b[32m0.70142\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4635 | loss: 0.70142 - acc: 0.5374 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4636  | total loss: \u001b[1m\u001b[32m0.70080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4636 | loss: 0.70080 - acc: 0.5436 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4637  | total loss: \u001b[1m\u001b[32m0.70023\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4637 | loss: 0.70023 - acc: 0.5493 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4638  | total loss: \u001b[1m\u001b[32m0.69973\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4638 | loss: 0.69973 - acc: 0.5543 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4639  | total loss: \u001b[1m\u001b[32m0.69927\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4639 | loss: 0.69927 - acc: 0.5589 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4640  | total loss: \u001b[1m\u001b[32m0.69886\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4640 | loss: 0.69886 - acc: 0.5630 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4641  | total loss: \u001b[1m\u001b[32m0.69849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4641 | loss: 0.69849 - acc: 0.5667 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4642  | total loss: \u001b[1m\u001b[32m0.69816\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4642 | loss: 0.69816 - acc: 0.5700 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4643  | total loss: \u001b[1m\u001b[32m0.69786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4643 | loss: 0.69786 - acc: 0.5730 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4644  | total loss: \u001b[1m\u001b[32m0.69759\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4644 | loss: 0.69759 - acc: 0.5757 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4645  | total loss: \u001b[1m\u001b[32m0.69734\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4645 | loss: 0.69734 - acc: 0.5782 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4646  | total loss: \u001b[1m\u001b[32m0.69712\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4646 | loss: 0.69712 - acc: 0.5803 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4647  | total loss: \u001b[1m\u001b[32m0.69693\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4647 | loss: 0.69693 - acc: 0.5823 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4648  | total loss: \u001b[1m\u001b[32m0.69675\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4648 | loss: 0.69675 - acc: 0.5841 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4649  | total loss: \u001b[1m\u001b[32m0.69659\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4649 | loss: 0.69659 - acc: 0.5857 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4650  | total loss: \u001b[1m\u001b[32m0.69645\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4650 | loss: 0.69645 - acc: 0.5871 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4651  | total loss: \u001b[1m\u001b[32m0.69632\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4651 | loss: 0.69632 - acc: 0.5884 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4652  | total loss: \u001b[1m\u001b[32m0.69620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4652 | loss: 0.69620 - acc: 0.5896 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4653  | total loss: \u001b[1m\u001b[32m0.69610\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4653 | loss: 0.69610 - acc: 0.5906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4654  | total loss: \u001b[1m\u001b[32m0.70201\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4654 | loss: 0.70201 - acc: 0.5315 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4655  | total loss: \u001b[1m\u001b[32m0.70132\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4655 | loss: 0.70132 - acc: 0.5384 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4656  | total loss: \u001b[1m\u001b[32m0.70070\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4656 | loss: 0.70070 - acc: 0.5445 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4657  | total loss: \u001b[1m\u001b[32m0.70015\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4657 | loss: 0.70015 - acc: 0.5501 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4658  | total loss: \u001b[1m\u001b[32m0.69965\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4658 | loss: 0.69965 - acc: 0.5551 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4659  | total loss: \u001b[1m\u001b[32m0.69920\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4659 | loss: 0.69920 - acc: 0.5596 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4660  | total loss: \u001b[1m\u001b[32m0.70480\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4660 | loss: 0.70480 - acc: 0.5036 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4661  | total loss: \u001b[1m\u001b[32m0.70383\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4661 | loss: 0.70383 - acc: 0.5133 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4662  | total loss: \u001b[1m\u001b[32m0.70297\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4662 | loss: 0.70297 - acc: 0.5219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4663  | total loss: \u001b[1m\u001b[32m0.70219\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4663 | loss: 0.70219 - acc: 0.5297 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4664  | total loss: \u001b[1m\u001b[32m0.70148\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4664 | loss: 0.70148 - acc: 0.5368 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4665  | total loss: \u001b[1m\u001b[32m0.70085\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4665 | loss: 0.70085 - acc: 0.5431 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4666  | total loss: \u001b[1m\u001b[32m0.70028\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4666 | loss: 0.70028 - acc: 0.5488 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4667  | total loss: \u001b[1m\u001b[32m0.69977\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4667 | loss: 0.69977 - acc: 0.5539 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4668  | total loss: \u001b[1m\u001b[32m0.69931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4668 | loss: 0.69931 - acc: 0.5585 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4669  | total loss: \u001b[1m\u001b[32m0.69889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4669 | loss: 0.69889 - acc: 0.5627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4670  | total loss: \u001b[1m\u001b[32m0.69852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4670 | loss: 0.69852 - acc: 0.5664 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4671  | total loss: \u001b[1m\u001b[32m0.69818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4671 | loss: 0.69818 - acc: 0.5698 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4672  | total loss: \u001b[1m\u001b[32m0.70388\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4672 | loss: 0.70388 - acc: 0.5128 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4673  | total loss: \u001b[1m\u001b[32m0.70301\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4673 | loss: 0.70301 - acc: 0.5215 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4674  | total loss: \u001b[1m\u001b[32m0.70222\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4674 | loss: 0.70222 - acc: 0.5294 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4675  | total loss: \u001b[1m\u001b[32m0.70152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4675 | loss: 0.70152 - acc: 0.5364 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4676  | total loss: \u001b[1m\u001b[32m0.70088\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4676 | loss: 0.70088 - acc: 0.5428 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4677  | total loss: \u001b[1m\u001b[32m0.70031\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4677 | loss: 0.70031 - acc: 0.5485 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4678  | total loss: \u001b[1m\u001b[32m0.69979\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4678 | loss: 0.69979 - acc: 0.5536 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4679  | total loss: \u001b[1m\u001b[32m0.69933\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4679 | loss: 0.69933 - acc: 0.5583 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4680  | total loss: \u001b[1m\u001b[32m0.69891\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4680 | loss: 0.69891 - acc: 0.5625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4681  | total loss: \u001b[1m\u001b[32m0.69854\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4681 | loss: 0.69854 - acc: 0.5662 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4682  | total loss: \u001b[1m\u001b[32m0.69820\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4682 | loss: 0.69820 - acc: 0.5696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4683  | total loss: \u001b[1m\u001b[32m0.69790\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4683 | loss: 0.69790 - acc: 0.5726 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4684  | total loss: \u001b[1m\u001b[32m0.69762\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4684 | loss: 0.69762 - acc: 0.5754 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4685  | total loss: \u001b[1m\u001b[32m0.69738\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4685 | loss: 0.69738 - acc: 0.5778 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4686  | total loss: \u001b[1m\u001b[32m0.69715\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4686 | loss: 0.69715 - acc: 0.5800 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4687  | total loss: \u001b[1m\u001b[32m0.69695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4687 | loss: 0.69695 - acc: 0.5820 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4688  | total loss: \u001b[1m\u001b[32m0.69678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4688 | loss: 0.69678 - acc: 0.5838 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4689  | total loss: \u001b[1m\u001b[32m0.69661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4689 | loss: 0.69661 - acc: 0.5855 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4690  | total loss: \u001b[1m\u001b[32m0.69647\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4690 | loss: 0.69647 - acc: 0.5869 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4691  | total loss: \u001b[1m\u001b[32m0.69634\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4691 | loss: 0.69634 - acc: 0.5882 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4692  | total loss: \u001b[1m\u001b[32m0.69622\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4692 | loss: 0.69622 - acc: 0.5894 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4693  | total loss: \u001b[1m\u001b[32m0.69611\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4693 | loss: 0.69611 - acc: 0.5905 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4694  | total loss: \u001b[1m\u001b[32m0.69602\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4694 | loss: 0.69602 - acc: 0.5923 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4695  | total loss: \u001b[1m\u001b[32m0.69593\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4695 | loss: 0.69593 - acc: 0.5923 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4696  | total loss: \u001b[1m\u001b[32m0.69585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4696 | loss: 0.69585 - acc: 0.5930 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4697  | total loss: \u001b[1m\u001b[32m0.69578\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4697 | loss: 0.69578 - acc: 0.5937 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4698  | total loss: \u001b[1m\u001b[32m0.69572\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4698 | loss: 0.69572 - acc: 0.5944 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4699  | total loss: \u001b[1m\u001b[32m0.69567\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4699 | loss: 0.69567 - acc: 0.5949 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4700  | total loss: \u001b[1m\u001b[32m0.69562\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4700 | loss: 0.69562 - acc: 0.5954 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4701  | total loss: \u001b[1m\u001b[32m0.69557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4701 | loss: 0.69557 - acc: 0.5959 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4702  | total loss: \u001b[1m\u001b[32m0.69553\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4702 | loss: 0.69553 - acc: 0.5963 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4703  | total loss: \u001b[1m\u001b[32m0.69549\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4703 | loss: 0.69549 - acc: 0.5967 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4704  | total loss: \u001b[1m\u001b[32m0.69546\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4704 | loss: 0.69546 - acc: 0.5970 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4705  | total loss: \u001b[1m\u001b[32m0.69543\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4705 | loss: 0.69543 - acc: 0.5973 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4706  | total loss: \u001b[1m\u001b[32m0.69540\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4706 | loss: 0.69540 - acc: 0.5976 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4707  | total loss: \u001b[1m\u001b[32m0.69538\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4707 | loss: 0.69538 - acc: 0.5978 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4708  | total loss: \u001b[1m\u001b[32m0.69536\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4708 | loss: 0.69536 - acc: 0.5980 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4709  | total loss: \u001b[1m\u001b[32m0.69534\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4709 | loss: 0.69534 - acc: 0.5982 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4710  | total loss: \u001b[1m\u001b[32m0.69532\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4710 | loss: 0.69532 - acc: 0.5984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4711  | total loss: \u001b[1m\u001b[32m0.69530\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4711 | loss: 0.69530 - acc: 0.5986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4712  | total loss: \u001b[1m\u001b[32m0.69529\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4712 | loss: 0.69529 - acc: 0.5987 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4713  | total loss: \u001b[1m\u001b[32m0.69527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4713 | loss: 0.69527 - acc: 0.5988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4714  | total loss: \u001b[1m\u001b[32m0.69526\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4714 | loss: 0.69526 - acc: 0.5990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4715  | total loss: \u001b[1m\u001b[32m0.69525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4715 | loss: 0.69525 - acc: 0.5991 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4716  | total loss: \u001b[1m\u001b[32m0.69524\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4716 | loss: 0.69524 - acc: 0.5992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4717  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4717 | loss: 0.69523 - acc: 0.5992 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4718  | total loss: \u001b[1m\u001b[32m0.69523\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4718 | loss: 0.69523 - acc: 0.5993 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4719  | total loss: \u001b[1m\u001b[32m0.69522\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4719 | loss: 0.69522 - acc: 0.5994 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4720  | total loss: \u001b[1m\u001b[32m0.69521\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4720 | loss: 0.69521 - acc: 0.5994 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4721  | total loss: \u001b[1m\u001b[32m0.69521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4721 | loss: 0.69521 - acc: 0.5995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4722  | total loss: \u001b[1m\u001b[32m0.69520\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4722 | loss: 0.69520 - acc: 0.5996 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4723  | total loss: \u001b[1m\u001b[32m0.69520\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4723 | loss: 0.69520 - acc: 0.5996 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4724  | total loss: \u001b[1m\u001b[32m0.69820\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4724 | loss: 0.69820 - acc: 0.5696 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4725  | total loss: \u001b[1m\u001b[32m0.69789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4725 | loss: 0.69789 - acc: 0.5727 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4726  | total loss: \u001b[1m\u001b[32m0.70262\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4726 | loss: 0.70262 - acc: 0.5254 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4727  | total loss: \u001b[1m\u001b[32m0.70187\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4727 | loss: 0.70187 - acc: 0.5329 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4728  | total loss: \u001b[1m\u001b[32m0.70120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4728 | loss: 0.70120 - acc: 0.5396 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4729  | total loss: \u001b[1m\u001b[32m0.70060\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4729 | loss: 0.70060 - acc: 0.5456 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4730  | total loss: \u001b[1m\u001b[32m0.70005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4730 | loss: 0.70005 - acc: 0.5511 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4731  | total loss: \u001b[1m\u001b[32m0.69956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4731 | loss: 0.69956 - acc: 0.5560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4732  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4732 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4733  | total loss: \u001b[1m\u001b[32m0.69873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4733 | loss: 0.69873 - acc: 0.5643 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4734  | total loss: \u001b[1m\u001b[32m0.69837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4734 | loss: 0.69837 - acc: 0.5679 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4735  | total loss: \u001b[1m\u001b[32m0.69805\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4735 | loss: 0.69805 - acc: 0.5711 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4736  | total loss: \u001b[1m\u001b[32m0.69776\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4736 | loss: 0.69776 - acc: 0.5740 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4737  | total loss: \u001b[1m\u001b[32m0.69750\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4737 | loss: 0.69750 - acc: 0.5766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4738  | total loss: \u001b[1m\u001b[32m0.69727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4738 | loss: 0.69727 - acc: 0.5789 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4739  | total loss: \u001b[1m\u001b[32m0.69705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4739 | loss: 0.69705 - acc: 0.5810 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4740  | total loss: \u001b[1m\u001b[32m0.69687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4740 | loss: 0.69687 - acc: 0.5829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4741  | total loss: \u001b[1m\u001b[32m0.69669\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4741 | loss: 0.69669 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4742  | total loss: \u001b[1m\u001b[32m0.69654\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4742 | loss: 0.69654 - acc: 0.5862 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4743  | total loss: \u001b[1m\u001b[32m0.69640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4743 | loss: 0.69640 - acc: 0.5876 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4744  | total loss: \u001b[1m\u001b[32m0.69628\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4744 | loss: 0.69628 - acc: 0.5899 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4745  | total loss: \u001b[1m\u001b[32m0.69617\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4745 | loss: 0.69617 - acc: 0.5899 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4746  | total loss: \u001b[1m\u001b[32m0.69607\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4746 | loss: 0.69607 - acc: 0.5909 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4747  | total loss: \u001b[1m\u001b[32m0.69598\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4747 | loss: 0.69598 - acc: 0.5918 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4748  | total loss: \u001b[1m\u001b[32m0.69589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4748 | loss: 0.69589 - acc: 0.5927 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4749  | total loss: \u001b[1m\u001b[32m0.69582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4749 | loss: 0.69582 - acc: 0.5934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4750  | total loss: \u001b[1m\u001b[32m0.69575\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4750 | loss: 0.69575 - acc: 0.5940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4751  | total loss: \u001b[1m\u001b[32m0.69569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4751 | loss: 0.69569 - acc: 0.5946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4752  | total loss: \u001b[1m\u001b[32m0.69564\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4752 | loss: 0.69564 - acc: 0.5952 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4753  | total loss: \u001b[1m\u001b[32m0.69559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4753 | loss: 0.69559 - acc: 0.5957 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4754  | total loss: \u001b[1m\u001b[32m0.69555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4754 | loss: 0.69555 - acc: 0.5961 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4755  | total loss: \u001b[1m\u001b[32m0.69551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4755 | loss: 0.69551 - acc: 0.5965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4756  | total loss: \u001b[1m\u001b[32m0.69548\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4756 | loss: 0.69548 - acc: 0.5968 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4757  | total loss: \u001b[1m\u001b[32m0.69544\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4757 | loss: 0.69544 - acc: 0.5972 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4758  | total loss: \u001b[1m\u001b[32m0.69541\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4758 | loss: 0.69541 - acc: 0.5974 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4759  | total loss: \u001b[1m\u001b[32m0.69539\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4759 | loss: 0.69539 - acc: 0.5977 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4760  | total loss: \u001b[1m\u001b[32m0.69537\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4760 | loss: 0.69537 - acc: 0.5979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4761  | total loss: \u001b[1m\u001b[32m0.69535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4761 | loss: 0.69535 - acc: 0.5981 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4762  | total loss: \u001b[1m\u001b[32m0.69533\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4762 | loss: 0.69533 - acc: 0.5983 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4763  | total loss: \u001b[1m\u001b[32m0.69531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4763 | loss: 0.69531 - acc: 0.5985 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4764  | total loss: \u001b[1m\u001b[32m0.69529\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4764 | loss: 0.69529 - acc: 0.5986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4765  | total loss: \u001b[1m\u001b[32m0.69528\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4765 | loss: 0.69528 - acc: 0.5988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4766  | total loss: \u001b[1m\u001b[32m0.69527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4766 | loss: 0.69527 - acc: 0.5989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4767  | total loss: \u001b[1m\u001b[32m0.69526\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4767 | loss: 0.69526 - acc: 0.5990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4768  | total loss: \u001b[1m\u001b[32m0.70125\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4768 | loss: 0.70125 - acc: 0.5391 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4769  | total loss: \u001b[1m\u001b[32m0.70064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4769 | loss: 0.70064 - acc: 0.5452 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4770  | total loss: \u001b[1m\u001b[32m0.70009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4770 | loss: 0.70009 - acc: 0.5507 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4771  | total loss: \u001b[1m\u001b[32m0.69960\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4771 | loss: 0.69960 - acc: 0.5556 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4772  | total loss: \u001b[1m\u001b[32m0.69915\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4772 | loss: 0.69915 - acc: 0.5600 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4773  | total loss: \u001b[1m\u001b[32m0.69875\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4773 | loss: 0.69875 - acc: 0.5640 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4774  | total loss: \u001b[1m\u001b[32m0.69839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4774 | loss: 0.69839 - acc: 0.5676 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4775  | total loss: \u001b[1m\u001b[32m0.69807\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4775 | loss: 0.69807 - acc: 0.5709 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4776  | total loss: \u001b[1m\u001b[32m0.69778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4776 | loss: 0.69778 - acc: 0.5738 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4777  | total loss: \u001b[1m\u001b[32m0.69752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4777 | loss: 0.69752 - acc: 0.5764 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4778  | total loss: \u001b[1m\u001b[32m0.70228\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4778 | loss: 0.70228 - acc: 0.5288 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4779  | total loss: \u001b[1m\u001b[32m0.70157\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4779 | loss: 0.70157 - acc: 0.5359 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4780  | total loss: \u001b[1m\u001b[32m0.70093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4780 | loss: 0.70093 - acc: 0.5423 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4781  | total loss: \u001b[1m\u001b[32m0.70035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4781 | loss: 0.70035 - acc: 0.5481 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4782  | total loss: \u001b[1m\u001b[32m0.69983\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4782 | loss: 0.69983 - acc: 0.5533 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4783  | total loss: \u001b[1m\u001b[32m0.69936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4783 | loss: 0.69936 - acc: 0.5579 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4784  | total loss: \u001b[1m\u001b[32m0.69894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4784 | loss: 0.69894 - acc: 0.5621 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4785  | total loss: \u001b[1m\u001b[32m0.69857\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4785 | loss: 0.69857 - acc: 0.5659 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4786  | total loss: \u001b[1m\u001b[32m0.69823\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4786 | loss: 0.69823 - acc: 0.5693 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4787  | total loss: \u001b[1m\u001b[32m0.69792\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4787 | loss: 0.69792 - acc: 0.5724 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4788  | total loss: \u001b[1m\u001b[32m0.70264\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4788 | loss: 0.70264 - acc: 0.5252 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4789  | total loss: \u001b[1m\u001b[32m0.70189\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4789 | loss: 0.70189 - acc: 0.5326 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4790  | total loss: \u001b[1m\u001b[32m0.70122\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4790 | loss: 0.70122 - acc: 0.5394 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4791  | total loss: \u001b[1m\u001b[32m0.70061\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4791 | loss: 0.70061 - acc: 0.5454 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4792  | total loss: \u001b[1m\u001b[32m0.70007\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4792 | loss: 0.70007 - acc: 0.5509 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4793  | total loss: \u001b[1m\u001b[32m0.69958\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4793 | loss: 0.69958 - acc: 0.5558 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4794  | total loss: \u001b[1m\u001b[32m0.69914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4794 | loss: 0.69914 - acc: 0.5602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4795  | total loss: \u001b[1m\u001b[32m0.69874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4795 | loss: 0.69874 - acc: 0.5678 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4796  | total loss: \u001b[1m\u001b[32m0.69838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4796 | loss: 0.69838 - acc: 0.5678 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4797  | total loss: \u001b[1m\u001b[32m0.69806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4797 | loss: 0.69806 - acc: 0.5710 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4798  | total loss: \u001b[1m\u001b[32m0.69777\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4798 | loss: 0.69777 - acc: 0.5739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4799  | total loss: \u001b[1m\u001b[32m0.69751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4799 | loss: 0.69751 - acc: 0.5765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4800  | total loss: \u001b[1m\u001b[32m0.69727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4800 | loss: 0.69727 - acc: 0.5789 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4801  | total loss: \u001b[1m\u001b[32m0.69706\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4801 | loss: 0.69706 - acc: 0.5810 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4802  | total loss: \u001b[1m\u001b[32m0.69687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4802 | loss: 0.69687 - acc: 0.5829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4803  | total loss: \u001b[1m\u001b[32m0.69670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4803 | loss: 0.69670 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4804  | total loss: \u001b[1m\u001b[32m0.69655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4804 | loss: 0.69655 - acc: 0.5861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4805  | total loss: \u001b[1m\u001b[32m0.69641\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4805 | loss: 0.69641 - acc: 0.5875 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4806  | total loss: \u001b[1m\u001b[32m0.70128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4806 | loss: 0.70128 - acc: 0.5388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4807  | total loss: \u001b[1m\u001b[32m0.70067\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4807 | loss: 0.70067 - acc: 0.5449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4808  | total loss: \u001b[1m\u001b[32m0.70012\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4808 | loss: 0.70012 - acc: 0.5504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4809  | total loss: \u001b[1m\u001b[32m0.69962\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4809 | loss: 0.69962 - acc: 0.5554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4810  | total loss: \u001b[1m\u001b[32m0.69918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4810 | loss: 0.69918 - acc: 0.5598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4811  | total loss: \u001b[1m\u001b[32m0.69877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4811 | loss: 0.69877 - acc: 0.5638 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4812  | total loss: \u001b[1m\u001b[32m0.69841\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4812 | loss: 0.69841 - acc: 0.5675 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4813  | total loss: \u001b[1m\u001b[32m0.69809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4813 | loss: 0.69809 - acc: 0.5707 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4814  | total loss: \u001b[1m\u001b[32m0.70379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4814 | loss: 0.70379 - acc: 0.5136 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4815  | total loss: \u001b[1m\u001b[32m0.70293\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4815 | loss: 0.70293 - acc: 0.5223 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4816  | total loss: \u001b[1m\u001b[32m0.70215\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4816 | loss: 0.70215 - acc: 0.5300 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4817  | total loss: \u001b[1m\u001b[32m0.70145\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4817 | loss: 0.70145 - acc: 0.5370 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4818  | total loss: \u001b[1m\u001b[32m0.70082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4818 | loss: 0.70082 - acc: 0.5433 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4819  | total loss: \u001b[1m\u001b[32m0.70026\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4819 | loss: 0.70026 - acc: 0.5490 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4820  | total loss: \u001b[1m\u001b[32m0.69975\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4820 | loss: 0.69975 - acc: 0.5541 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4821  | total loss: \u001b[1m\u001b[32m0.69929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4821 | loss: 0.69929 - acc: 0.5587 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4822  | total loss: \u001b[1m\u001b[32m0.69888\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4822 | loss: 0.69888 - acc: 0.5628 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4823  | total loss: \u001b[1m\u001b[32m0.69850\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4823 | loss: 0.69850 - acc: 0.5665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4824  | total loss: \u001b[1m\u001b[32m0.70217\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4824 | loss: 0.70217 - acc: 0.5299 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4825  | total loss: \u001b[1m\u001b[32m0.70147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4825 | loss: 0.70147 - acc: 0.5369 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4826  | total loss: \u001b[1m\u001b[32m0.70084\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4826 | loss: 0.70084 - acc: 0.5432 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4827  | total loss: \u001b[1m\u001b[32m0.70027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4827 | loss: 0.70027 - acc: 0.5489 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4828  | total loss: \u001b[1m\u001b[32m0.69976\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4828 | loss: 0.69976 - acc: 0.5540 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4829  | total loss: \u001b[1m\u001b[32m0.69930\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4829 | loss: 0.69930 - acc: 0.5586 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4830  | total loss: \u001b[1m\u001b[32m0.69888\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4830 | loss: 0.69888 - acc: 0.5627 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4831  | total loss: \u001b[1m\u001b[32m0.69851\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4831 | loss: 0.69851 - acc: 0.5665 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4832  | total loss: \u001b[1m\u001b[32m0.69818\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4832 | loss: 0.69818 - acc: 0.5698 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4833  | total loss: \u001b[1m\u001b[32m0.69788\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4833 | loss: 0.69788 - acc: 0.5728 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4834  | total loss: \u001b[1m\u001b[32m0.69760\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4834 | loss: 0.69760 - acc: 0.5756 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4835  | total loss: \u001b[1m\u001b[32m0.69736\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4835 | loss: 0.69736 - acc: 0.5780 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4836  | total loss: \u001b[1m\u001b[32m0.69714\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4836 | loss: 0.69714 - acc: 0.5802 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4837  | total loss: \u001b[1m\u001b[32m0.69694\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4837 | loss: 0.69694 - acc: 0.5822 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4838  | total loss: \u001b[1m\u001b[32m0.69676\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4838 | loss: 0.69676 - acc: 0.5840 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4839  | total loss: \u001b[1m\u001b[32m0.69660\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4839 | loss: 0.69660 - acc: 0.5856 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4840  | total loss: \u001b[1m\u001b[32m0.69946\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4840 | loss: 0.69946 - acc: 0.5570 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4841  | total loss: \u001b[1m\u001b[32m0.69903\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4841 | loss: 0.69903 - acc: 0.5613 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4842  | total loss: \u001b[1m\u001b[32m0.70264\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4842 | loss: 0.70264 - acc: 0.5252 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4843  | total loss: \u001b[1m\u001b[32m0.70189\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4843 | loss: 0.70189 - acc: 0.5327 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4844  | total loss: \u001b[1m\u001b[32m0.70122\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4844 | loss: 0.70122 - acc: 0.5394 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4845  | total loss: \u001b[1m\u001b[32m0.70061\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4845 | loss: 0.70061 - acc: 0.5455 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4846  | total loss: \u001b[1m\u001b[32m0.70007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4846 | loss: 0.70007 - acc: 0.5509 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4847  | total loss: \u001b[1m\u001b[32m0.69958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4847 | loss: 0.69958 - acc: 0.5558 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4848  | total loss: \u001b[1m\u001b[32m0.69914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4848 | loss: 0.69914 - acc: 0.5602 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4849  | total loss: \u001b[1m\u001b[32m0.69874\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4849 | loss: 0.69874 - acc: 0.5642 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4850  | total loss: \u001b[1m\u001b[32m0.69838\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4850 | loss: 0.69838 - acc: 0.5678 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4851  | total loss: \u001b[1m\u001b[32m0.69806\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4851 | loss: 0.69806 - acc: 0.5710 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4852  | total loss: \u001b[1m\u001b[32m0.69777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4852 | loss: 0.69777 - acc: 0.5739 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4853  | total loss: \u001b[1m\u001b[32m0.69751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4853 | loss: 0.69751 - acc: 0.5765 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4854  | total loss: \u001b[1m\u001b[32m0.69727\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4854 | loss: 0.69727 - acc: 0.5789 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4855  | total loss: \u001b[1m\u001b[32m0.69706\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4855 | loss: 0.69706 - acc: 0.5810 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4856  | total loss: \u001b[1m\u001b[32m0.69687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4856 | loss: 0.69687 - acc: 0.5829 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4857  | total loss: \u001b[1m\u001b[32m0.69670\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4857 | loss: 0.69670 - acc: 0.5846 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4858  | total loss: \u001b[1m\u001b[32m0.69655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4858 | loss: 0.69655 - acc: 0.5861 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4859  | total loss: \u001b[1m\u001b[32m0.69641\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4859 | loss: 0.69641 - acc: 0.5875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4860  | total loss: \u001b[1m\u001b[32m0.69628\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4860 | loss: 0.69628 - acc: 0.5888 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4861  | total loss: \u001b[1m\u001b[32m0.69617\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4861 | loss: 0.69617 - acc: 0.5899 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4862  | total loss: \u001b[1m\u001b[32m0.69607\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4862 | loss: 0.69607 - acc: 0.5909 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4863  | total loss: \u001b[1m\u001b[32m0.69598\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4863 | loss: 0.69598 - acc: 0.5918 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4864  | total loss: \u001b[1m\u001b[32m0.69590\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4864 | loss: 0.69590 - acc: 0.5926 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4865  | total loss: \u001b[1m\u001b[32m0.69582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4865 | loss: 0.69582 - acc: 0.5934 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4866  | total loss: \u001b[1m\u001b[32m0.69576\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4866 | loss: 0.69576 - acc: 0.5940 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4867  | total loss: \u001b[1m\u001b[32m0.69570\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4867 | loss: 0.69570 - acc: 0.5946 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4868  | total loss: \u001b[1m\u001b[32m0.69564\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4868 | loss: 0.69564 - acc: 0.5952 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4869  | total loss: \u001b[1m\u001b[32m0.69559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4869 | loss: 0.69559 - acc: 0.5956 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4870  | total loss: \u001b[1m\u001b[32m0.69555\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4870 | loss: 0.69555 - acc: 0.5961 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4871  | total loss: \u001b[1m\u001b[32m0.69551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4871 | loss: 0.69551 - acc: 0.5965 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4872  | total loss: \u001b[1m\u001b[32m0.69548\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4872 | loss: 0.69548 - acc: 0.5968 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4873  | total loss: \u001b[1m\u001b[32m0.69544\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4873 | loss: 0.69544 - acc: 0.5971 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4874  | total loss: \u001b[1m\u001b[32m0.69542\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4874 | loss: 0.69542 - acc: 0.5974 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4875  | total loss: \u001b[1m\u001b[32m0.69539\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4875 | loss: 0.69539 - acc: 0.5977 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4876  | total loss: \u001b[1m\u001b[32m0.69537\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4876 | loss: 0.69537 - acc: 0.5979 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4877  | total loss: \u001b[1m\u001b[32m0.69535\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4877 | loss: 0.69535 - acc: 0.5981 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4878  | total loss: \u001b[1m\u001b[32m0.69533\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4878 | loss: 0.69533 - acc: 0.5983 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4879  | total loss: \u001b[1m\u001b[32m0.69531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4879 | loss: 0.69531 - acc: 0.5985 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4880  | total loss: \u001b[1m\u001b[32m0.69530\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4880 | loss: 0.69530 - acc: 0.5986 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4881  | total loss: \u001b[1m\u001b[32m0.69528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4881 | loss: 0.69528 - acc: 0.5988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4882  | total loss: \u001b[1m\u001b[32m0.69527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4882 | loss: 0.69527 - acc: 0.5989 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4883  | total loss: \u001b[1m\u001b[32m0.69526\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4883 | loss: 0.69526 - acc: 0.5990 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4884  | total loss: \u001b[1m\u001b[32m0.70025\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4884 | loss: 0.70025 - acc: 0.5491 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4885  | total loss: \u001b[1m\u001b[32m0.69974\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4885 | loss: 0.69974 - acc: 0.5542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4886  | total loss: \u001b[1m\u001b[32m0.69928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4886 | loss: 0.69928 - acc: 0.5588 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4887  | total loss: \u001b[1m\u001b[32m0.69887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4887 | loss: 0.69887 - acc: 0.5629 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4888  | total loss: \u001b[1m\u001b[32m0.69850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4888 | loss: 0.69850 - acc: 0.5666 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4889  | total loss: \u001b[1m\u001b[32m0.69816\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4889 | loss: 0.69816 - acc: 0.5699 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4890  | total loss: \u001b[1m\u001b[32m0.69786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4890 | loss: 0.69786 - acc: 0.5730 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4891  | total loss: \u001b[1m\u001b[32m0.69759\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4891 | loss: 0.69759 - acc: 0.5757 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4892  | total loss: \u001b[1m\u001b[32m0.69735\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4892 | loss: 0.69735 - acc: 0.5781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4893  | total loss: \u001b[1m\u001b[32m0.69713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4893 | loss: 0.69713 - acc: 0.5803 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4894  | total loss: \u001b[1m\u001b[32m0.69693\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4894 | loss: 0.69693 - acc: 0.5823 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4895  | total loss: \u001b[1m\u001b[32m0.69676\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4895 | loss: 0.69676 - acc: 0.5840 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4896  | total loss: \u001b[1m\u001b[32m0.70060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4896 | loss: 0.70060 - acc: 0.5456 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4897  | total loss: \u001b[1m\u001b[32m0.70005\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4897 | loss: 0.70005 - acc: 0.5511 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4898  | total loss: \u001b[1m\u001b[32m0.69956\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4898 | loss: 0.69956 - acc: 0.5560 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4899  | total loss: \u001b[1m\u001b[32m0.69912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4899 | loss: 0.69912 - acc: 0.5604 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4900  | total loss: \u001b[1m\u001b[32m0.70473\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4900 | loss: 0.70473 - acc: 0.5043 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4901  | total loss: \u001b[1m\u001b[32m0.70377\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4901 | loss: 0.70377 - acc: 0.5139 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4902  | total loss: \u001b[1m\u001b[32m0.70291\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4902 | loss: 0.70291 - acc: 0.5225 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4903  | total loss: \u001b[1m\u001b[32m0.70213\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4903 | loss: 0.70213 - acc: 0.5303 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4904  | total loss: \u001b[1m\u001b[32m0.70144\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4904 | loss: 0.70144 - acc: 0.5372 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4905  | total loss: \u001b[1m\u001b[32m0.70081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4905 | loss: 0.70081 - acc: 0.5435 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4906  | total loss: \u001b[1m\u001b[32m0.70024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4906 | loss: 0.70024 - acc: 0.5492 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4907  | total loss: \u001b[1m\u001b[32m0.69973\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4907 | loss: 0.69973 - acc: 0.5542 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4908  | total loss: \u001b[1m\u001b[32m0.70528\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 4908 | loss: 0.70528 - acc: 0.4988 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4909  | total loss: \u001b[1m\u001b[32m0.70427\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4909 | loss: 0.70427 - acc: 0.5089 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4910  | total loss: \u001b[1m\u001b[32m0.70335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4910 | loss: 0.70335 - acc: 0.5180 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4911  | total loss: \u001b[1m\u001b[32m0.70254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4911 | loss: 0.70254 - acc: 0.5262 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4912  | total loss: \u001b[1m\u001b[32m0.70180\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4912 | loss: 0.70180 - acc: 0.5336 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4913  | total loss: \u001b[1m\u001b[32m0.70113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4913 | loss: 0.70113 - acc: 0.5403 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4914  | total loss: \u001b[1m\u001b[32m0.70054\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4914 | loss: 0.70054 - acc: 0.5462 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4915  | total loss: \u001b[1m\u001b[32m0.70000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4915 | loss: 0.70000 - acc: 0.5516 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4916  | total loss: \u001b[1m\u001b[32m0.69951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4916 | loss: 0.69951 - acc: 0.5564 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4917  | total loss: \u001b[1m\u001b[32m0.69908\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4917 | loss: 0.69908 - acc: 0.5608 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4918  | total loss: \u001b[1m\u001b[32m0.69869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4918 | loss: 0.69869 - acc: 0.5647 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4919  | total loss: \u001b[1m\u001b[32m0.69833\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4919 | loss: 0.69833 - acc: 0.5682 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4920  | total loss: \u001b[1m\u001b[32m0.69802\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4920 | loss: 0.69802 - acc: 0.5714 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4921  | total loss: \u001b[1m\u001b[32m0.69773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4921 | loss: 0.69773 - acc: 0.5743 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4922  | total loss: \u001b[1m\u001b[32m0.69747\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4922 | loss: 0.69747 - acc: 0.5769 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4923  | total loss: \u001b[1m\u001b[32m0.69724\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4923 | loss: 0.69724 - acc: 0.5792 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4924  | total loss: \u001b[1m\u001b[32m0.69703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4924 | loss: 0.69703 - acc: 0.5813 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4925  | total loss: \u001b[1m\u001b[32m0.69685\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4925 | loss: 0.69685 - acc: 0.5831 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4926  | total loss: \u001b[1m\u001b[32m0.69668\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4926 | loss: 0.69668 - acc: 0.5848 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4927  | total loss: \u001b[1m\u001b[32m0.69653\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4927 | loss: 0.69653 - acc: 0.5863 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4928  | total loss: \u001b[1m\u001b[32m0.69639\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4928 | loss: 0.69639 - acc: 0.5877 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4929  | total loss: \u001b[1m\u001b[32m0.69627\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4929 | loss: 0.69627 - acc: 0.5889 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4930  | total loss: \u001b[1m\u001b[32m0.69616\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4930 | loss: 0.69616 - acc: 0.5900 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4931  | total loss: \u001b[1m\u001b[32m0.69606\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4931 | loss: 0.69606 - acc: 0.5910 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4932  | total loss: \u001b[1m\u001b[32m0.70197\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4932 | loss: 0.70197 - acc: 0.5319 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4933  | total loss: \u001b[1m\u001b[32m0.70129\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 4933 | loss: 0.70129 - acc: 0.5387 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4934  | total loss: \u001b[1m\u001b[32m0.70067\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4934 | loss: 0.70067 - acc: 0.5449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4935  | total loss: \u001b[1m\u001b[32m0.70012\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 4935 | loss: 0.70012 - acc: 0.5504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4936  | total loss: \u001b[1m\u001b[32m0.69962\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4936 | loss: 0.69962 - acc: 0.5553 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4937  | total loss: \u001b[1m\u001b[32m0.69918\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4937 | loss: 0.69918 - acc: 0.5598 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4938  | total loss: \u001b[1m\u001b[32m0.70478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4938 | loss: 0.70478 - acc: 0.5038 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4939  | total loss: \u001b[1m\u001b[32m0.70381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4939 | loss: 0.70381 - acc: 0.5134 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4940  | total loss: \u001b[1m\u001b[32m0.70895\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4940 | loss: 0.70895 - acc: 0.4621 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4941  | total loss: \u001b[1m\u001b[32m0.70757\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4941 | loss: 0.70757 - acc: 0.4759 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4942  | total loss: \u001b[1m\u001b[32m0.70633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4942 | loss: 0.70633 - acc: 0.4883 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4943  | total loss: \u001b[1m\u001b[32m0.70521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4943 | loss: 0.70521 - acc: 0.4995 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4944  | total loss: \u001b[1m\u001b[32m0.70421\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4944 | loss: 0.70421 - acc: 0.5095 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4945  | total loss: \u001b[1m\u001b[32m0.70330\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4945 | loss: 0.70330 - acc: 0.5186 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4946  | total loss: \u001b[1m\u001b[32m0.70249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4946 | loss: 0.70249 - acc: 0.5267 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4947  | total loss: \u001b[1m\u001b[32m0.70175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4947 | loss: 0.70175 - acc: 0.5340 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4948  | total loss: \u001b[1m\u001b[32m0.70110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4948 | loss: 0.70110 - acc: 0.5406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4949  | total loss: \u001b[1m\u001b[32m0.70050\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4949 | loss: 0.70050 - acc: 0.5466 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4950  | total loss: \u001b[1m\u001b[32m0.70497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4950 | loss: 0.70497 - acc: 0.5019 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4951  | total loss: \u001b[1m\u001b[32m0.70399\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4951 | loss: 0.70399 - acc: 0.5117 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4952  | total loss: \u001b[1m\u001b[32m0.70810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4952 | loss: 0.70810 - acc: 0.4706 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4953  | total loss: \u001b[1m\u001b[32m0.70681\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4953 | loss: 0.70681 - acc: 0.4835 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4954  | total loss: \u001b[1m\u001b[32m0.70564\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4954 | loss: 0.70564 - acc: 0.4951 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4955  | total loss: \u001b[1m\u001b[32m0.70460\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4955 | loss: 0.70460 - acc: 0.5056 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4956  | total loss: \u001b[1m\u001b[32m0.70365\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4956 | loss: 0.70365 - acc: 0.5151 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4957  | total loss: \u001b[1m\u001b[32m0.70280\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4957 | loss: 0.70280 - acc: 0.5236 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4958  | total loss: \u001b[1m\u001b[32m0.70204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4958 | loss: 0.70204 - acc: 0.5312 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4959  | total loss: \u001b[1m\u001b[32m0.70135\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4959 | loss: 0.70135 - acc: 0.5381 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4960  | total loss: \u001b[1m\u001b[32m0.70073\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4960 | loss: 0.70073 - acc: 0.5443 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4961  | total loss: \u001b[1m\u001b[32m0.70017\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4961 | loss: 0.70017 - acc: 0.5498 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4962  | total loss: \u001b[1m\u001b[32m0.69967\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4962 | loss: 0.69967 - acc: 0.5549 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4963  | total loss: \u001b[1m\u001b[32m0.69922\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4963 | loss: 0.69922 - acc: 0.5594 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4964  | total loss: \u001b[1m\u001b[32m0.69881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4964 | loss: 0.69881 - acc: 0.5634 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4965  | total loss: \u001b[1m\u001b[32m0.69845\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4965 | loss: 0.69845 - acc: 0.5671 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4966  | total loss: \u001b[1m\u001b[32m0.69812\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4966 | loss: 0.69812 - acc: 0.5704 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4967  | total loss: \u001b[1m\u001b[32m0.69782\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4967 | loss: 0.69782 - acc: 0.5733 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4968  | total loss: \u001b[1m\u001b[32m0.69756\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4968 | loss: 0.69756 - acc: 0.5760 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4969  | total loss: \u001b[1m\u001b[32m0.69732\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4969 | loss: 0.69732 - acc: 0.5784 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4970  | total loss: \u001b[1m\u001b[32m0.70110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4970 | loss: 0.70110 - acc: 0.5406 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4971  | total loss: \u001b[1m\u001b[32m0.70051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4971 | loss: 0.70051 - acc: 0.5465 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4972  | total loss: \u001b[1m\u001b[32m0.69997\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4972 | loss: 0.69997 - acc: 0.5519 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4973  | total loss: \u001b[1m\u001b[32m0.69949\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4973 | loss: 0.69949 - acc: 0.5567 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4974  | total loss: \u001b[1m\u001b[32m0.69906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4974 | loss: 0.69906 - acc: 0.5610 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4975  | total loss: \u001b[1m\u001b[32m0.69867\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4975 | loss: 0.69867 - acc: 0.5649 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4976  | total loss: \u001b[1m\u001b[32m0.69832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4976 | loss: 0.69832 - acc: 0.5684 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 4977  | total loss: \u001b[1m\u001b[32m0.69800\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4977 | loss: 0.69800 - acc: 0.5716 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4978  | total loss: \u001b[1m\u001b[32m0.70272\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4978 | loss: 0.70272 - acc: 0.5244 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4979  | total loss: \u001b[1m\u001b[32m0.70128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4979 | loss: 0.70128 - acc: 0.5320 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4980  | total loss: \u001b[1m\u001b[32m0.70128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4980 | loss: 0.70128 - acc: 0.5388 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4981  | total loss: \u001b[1m\u001b[32m0.70067\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4981 | loss: 0.70067 - acc: 0.5449 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4982  | total loss: \u001b[1m\u001b[32m0.70012\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4982 | loss: 0.70012 - acc: 0.5504 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4983  | total loss: \u001b[1m\u001b[32m0.69962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4983 | loss: 0.69962 - acc: 0.5554 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4984  | total loss: \u001b[1m\u001b[32m0.70318\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4984 | loss: 0.70318 - acc: 0.5198 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4985  | total loss: \u001b[1m\u001b[32m0.70237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4985 | loss: 0.70237 - acc: 0.5278 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4986  | total loss: \u001b[1m\u001b[32m0.70165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4986 | loss: 0.70165 - acc: 0.5351 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4987  | total loss: \u001b[1m\u001b[32m0.70100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4987 | loss: 0.70100 - acc: 0.5416 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4988  | total loss: \u001b[1m\u001b[32m0.70442\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4988 | loss: 0.70442 - acc: 0.5074 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4989  | total loss: \u001b[1m\u001b[32m0.70349\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4989 | loss: 0.70349 - acc: 0.5167 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4990  | total loss: \u001b[1m\u001b[32m0.70266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4990 | loss: 0.70266 - acc: 0.5250 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4991  | total loss: \u001b[1m\u001b[32m0.70191\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4991 | loss: 0.70191 - acc: 0.5325 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4992  | total loss: \u001b[1m\u001b[32m0.70623\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4992 | loss: 0.70623 - acc: 0.4892 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4993  | total loss: \u001b[1m\u001b[32m0.70513\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4993 | loss: 0.70513 - acc: 0.5003 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4994  | total loss: \u001b[1m\u001b[32m0.70413\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4994 | loss: 0.70413 - acc: 0.5103 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4995  | total loss: \u001b[1m\u001b[32m0.70323\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 4995 | loss: 0.70323 - acc: 0.5193 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4996  | total loss: \u001b[1m\u001b[32m0.70243\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4996 | loss: 0.70243 - acc: 0.5273 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4997  | total loss: \u001b[1m\u001b[32m0.70170\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4997 | loss: 0.70170 - acc: 0.5346 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4998  | total loss: \u001b[1m\u001b[32m0.70504\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 4998 | loss: 0.70504 - acc: 0.5011 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4999  | total loss: \u001b[1m\u001b[32m0.70406\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 4999 | loss: 0.70406 - acc: 0.5110 -- iter: 10/10\n",
      "--\n",
      "Training Step: 5000  | total loss: \u001b[1m\u001b[32m0.70317\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 5000 | loss: 0.70317 - acc: 0.5199 -- iter: 10/10\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "#Con model.fit(), Tensorflow entrenará a la red 5000 veces con los datos suministrados.\n",
    "model.fit(X, Y, n_epoch=5000, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si durante el entrenamiento, en la salida obtenemos un valor bajo para el parámetro \"loss\" \n",
    "#y un valor alto para el parámetro \"accuracy\" (precisión), entonces la red habrá aprendido correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.99999881e-01   1.30646494e-10   1.97219016e-17   2.13257519e-14\n",
      "   3.63337560e-09   4.62077043e-11   7.26200611e-08   3.01709016e-13\n",
      "   4.54744207e-14   1.35556432e-12]\n",
      "Valores entrenados:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Valores aprendidos:  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  5.29224071e-13   1.00000000e+00   1.20630908e-29   6.69260348e-26\n",
      "   2.06349299e-10   1.78209848e-33   6.08658374e-31   7.22078161e-11\n",
      "   2.80472126e-21   8.09203360e-25]\n",
      "Valores entrenados:  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  6.52298848e-36   0.00000000e+00   1.00000000e+00   4.95722475e-21\n",
      "   2.45150862e-34   0.00000000e+00   7.84975248e-15   3.13253037e-24\n",
      "   1.97604252e-28   4.96499662e-31]\n",
      "Valores entrenados:  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[  1.75777981e-17   0.00000000e+00   1.09329956e-09   1.60628205e-16\n",
      "   2.95879294e-38   4.03512306e-26   1.00000000e+00   5.17013253e-23\n",
      "   7.82757710e-27   1.05200416e-23]\n",
      "Valores entrenados:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[  1.04597007e-14   5.73876207e-07   0.00000000e+00   3.65444721e-35\n",
      "   9.99999404e-01   2.25534232e-35   9.69624437e-24   3.90576470e-21\n",
      "   3.47013738e-21   3.67990824e-27]\n",
      "Valores entrenados:  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[  8.93767110e-34   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "Valores entrenados:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[  2.59781772e-34   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "Valores entrenados:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[  3.11808265e-07   4.00341094e-07   1.18212796e-07   1.56983867e-06\n",
      "   1.78315370e-12   2.72259238e-15   1.53235857e-09   9.99997616e-01\n",
      "   2.40243225e-11   2.07329466e-11]\n",
      "Valores entrenados:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[  8.09257450e-21   0.00000000e+00   1.74687718e-32   4.49118541e-31\n",
      "   2.39689159e-29   1.63459500e-28   1.00000000e+00   0.00000000e+00\n",
      "   6.40654845e-34   2.59116077e-31]\n",
      "Valores entrenados:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "[  5.94150401e-17   0.00000000e+00   6.94729559e-34   5.37260314e-30\n",
      "   5.77419382e-38   4.05523415e-24   1.00000000e+00   0.00000000e+00\n",
      "   6.22798417e-36   4.24905567e-31]\n",
      "Valores entrenados:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "Valores aprendidos:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#Comprobar si el perceptrón ha aprendido todos los casos\n",
    "i=0\n",
    "binaryOutput=[]\n",
    "for x in enumerate(X):\n",
    "    maxValue=max(model.predict(X)[i])\n",
    "    for out in model.predict(X)[i]:\n",
    "        binaryOutput.append(1 if out==maxValue else 0)\n",
    "    print(model.predict(X)[i])\n",
    "    print ('Salida aprendida: ', Y[i])\n",
    "    print ('Salida recordada: ', binaryOutput)\n",
    "    binaryOutput.clear()\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
